<!DOCTYPE html>
<html lang="en" dir="ltr"><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  
                           
     


<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.144.2">
<title>Training Mask R-CNN Models with PyTorch | Shivam Chhetry</title>


<meta property="twitter:site" content="@apreshill">
<meta property="twitter:creator" content="@apreshill">







  
    
  
<meta name="description" content="Social icons may appear on several pages throughout your site. Learn how to set them up, and control where they show up.">


<meta property="og:site_name" content="Shivam Chhetry">
<meta property="og:title" content="Training Mask R-CNN Models with PyTorch | Shivam Chhetry">
<meta property="og:description" content="Social icons may appear on several pages throughout your site. Learn how to set them up, and control where they show up." />
<meta property="og:type" content="page" />
<meta property="og:url" content="http://localhost:1313/blog/training-mask-r-cnn-models-with-pytorch/" />
<meta property="og:locale" content="en">




    
        <meta property="og:image" content="http://localhost:1313/blog/sidebar-listing.jpg" >
        <meta property="twitter:card" content="summary_large_image">
        <meta name="twitter:image" content="http://localhost:1313/blog/sidebar-listing.jpg" >
    
    
  
  <meta itemprop="name" content="Training Mask R-CNN Models with PyTorch">
  <meta itemprop="description" content="Introduction Getting Started with the Code Setting Up Your Python Environment Importing the Required Dependencies Setting Up the Project Loading and Exploring the Dataset Loading the Mask R-CNN Model Preparing the Data Fine-tuning the Model Making Predictions with the Model Conclusion Introduction Welcome to this hands-on guide to training Mask R-CNN models in PyTorch! Mask R-CNN models can identify and locate multiple objects within images and generate segmentation masks for each detected object.">
  <meta itemprop="datePublished" content="2025-02-23T00:00:00+00:00">
  <meta itemprop="dateModified" content="2025-02-23T00:00:00+00:00">
  <meta itemprop="wordCount" content="7570">
  <meta itemprop="keywords" content="Pytorch,Mask-Rcnn,Object-Detection,Instance-Segmentation,Tutorial">
  
  <!--[if IE]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  <link rel="shortcut icon" href="/img/website_icon.png" type="image/x-icon">
  <link rel="icon" href="/img/website_icon.png" type="image/x-icon">
  
  
  <link rel="stylesheet" href="/style.main.min.c2b70bb3d1b31d0b2b25ecf94242cedd9faa232c76e8fb6a1ed73fbc8987835f.css" integrity="sha256-wrcLs9GzHQsrJez5QkLO3Z&#43;qIyx26PtqHtc/vImHg18=" media="screen">
  
  
  <script src="/panelset.min.ed1ac24b6e16f4e2481e3d1d098ae66f5bc77438aef619e6e266d8ac5b00dc72.js" type="text/javascript"></script>
  
  
  <script src="/main.min.c793459ded39f47482c177c2f8967f9ea2b91b4225a969d3909bd20c512c082d.js" type="text/javascript"></script>
</head>
<body>
      <div class="grid-container single">
<header class="site-header pt4 pb2 mb4 bb b--transparent ph5 headroom z-max" role="banner">
  <nav class="site-nav db dt-l w-100" role="navigation">
    <a class="site-brand db dtc-l v-mid link no-underline w-100 w-33-l tc tl-l" href="http://localhost:1313/" title="Home">
      <img src="/img/boy.png" class="dib db-l h2 w-auto" alt="Shivam Chhetry">
    </a>
    <div class="site-links db dtc-l v-mid w-100 w-47-l tc tr-l mt3 mt0-l ttu tracked">
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/about/" title="About Blogophonic">About</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 active" href="/blog/" title="Blog">Blog</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/project/" title="Project Portfolio">Projects</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/talk/" title="Talks">Talks</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/collection/" title="A collection">A collection</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/elements/" title="Element Page">Elements</a>
      
      
    </div>
  </nav>
</header>

<main class="page-main pa4" role="main">
  <section class="page-content mw7 center">
    <article class="post-content pa0 ph4-l">
      <header class="post-header">
        <h1 class="f1 lh-solid measure-narrow mb3 fw4">Training Mask R-CNN Models with PyTorch</h1>
        <h4 class="f4 mt0 mb4 lh-title measure">Social icons may appear on several pages throughout your site. Learn how to set them up, and control where they show up.</h4>
        <p class="f6 measure lh-copy mv1">By Shiavam Chhetry in <a href="http://localhost:1313/categories/pytorch">pytorch</a>  <a href="http://localhost:1313/categories/mask-rcnn">mask-rcnn</a>  <a href="http://localhost:1313/categories/object-detection">object-detection</a>  <a href="http://localhost:1313/categories/instance-segmentation">instance-segmentation</a>  <a href="http://localhost:1313/categories/tutorial">tutorial</a> </p>
        <p class="f7 db mv0 ttu">February 23, 2025</p>

      

      </header>
      <section class="post-body pt5 pb4">
        <ul>
<li>
<a href="#introduction">Introduction</a></li>
<li>
<a href="#getting-started-with-the-code">Getting Started with the Code</a></li>
<li>
<a href="#setting-up-your-python-environment">Setting Up Your Python Environment</a></li>
<li>
<a href="#importing-the-required-dependencies">Importing the Required Dependencies</a></li>
<li>
<a href="#setting-up-the-project">Setting Up the Project</a></li>
<li>
<a href="#loading-and-exploring-the-dataset">Loading and Exploring the Dataset</a></li>
<li>
<a href="#loading-the-mask-r-cnn-model">Loading the Mask R-CNN Model</a></li>
<li>
<a href="#preparing-the-data">Preparing the Data</a></li>
<li>
<a href="#fine-tuning-the-model">Fine-tuning the Model</a></li>
<li>
<a href="#making-predictions-with-the-model">Making Predictions with the Model</a></li>
<li>
<a href="#conclusion">Conclusion</a></li>
</ul>




<h2 id="introduction">Introduction
  <a href="#introduction"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h2>
<p>Welcome to this hands-on guide to training 
<a href="https://arxiv.org/abs/1703.06870" target="_blank" rel="noopener">Mask R-CNN</a> models in PyTorch! Mask R-CNN models can identify and locate multiple objects within images and generate segmentation masks for each detected object.</p>
<p>For this tutorial, we will fine-tune a Mask R-CNN model from the 
<a href="https://pytorch.org/vision/stable/index.html" target="_blank" rel="noopener"><code>torchvision</code></a> library on a small sample dataset of annotated student ID card images.</p>
<p><img src="./images/student-id-sample-annotation.jpg" alt="">{fig-align=&ldquo;center&rdquo;}</p>
<p>This tutorial is suitable for anyone with rudimentary PyTorch experience. If you are new to PyTorch and want to start with a beginner-focused project, check out my tutorial on fine-tuning image classifiers.</p>
<ul>
<li>
<a href="../pytorch-train-image-classifier-timm-hf-tutorial/">Fine-Tuning Image Classifiers with PyTorch and the timm library for Beginners</a></li>
</ul>




<h2 id="getting-started-with-the-code">Getting Started with the Code
  <a href="#getting-started-with-the-code"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h2>
<p>The tutorial code is available as a 
<a href="https://jupyter.org/" target="_blank" rel="noopener">Jupyter Notebook</a>, which you can run locally or in a cloud-based environment like 
<a href="https://colab.research.google.com/" target="_blank" rel="noopener">Google Colab</a>. I have dedicated tutorials for those new to these platforms or who need guidance setting up:</p>
<p>Before diving into the code, we&rsquo;ll cover the steps to create a local Python environment and install the necessary dependencies. The dedicated Colab Notebook includes the code to install the required dependencies in Google Colab.</p>




<h3 id="creating-a-python-environment">Creating a Python Environment
  <a href="#creating-a-python-environment"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h3>
<p>First, we&rsquo;ll create a Python environment using 
<a href="https://docs.conda.io/en/latest/" target="_blank" rel="noopener">Conda</a>/
<a href="https://mamba.readthedocs.io/en/latest/" target="_blank" rel="noopener">Mamba</a>. Open a terminal with Conda/Mamba installed and run the following commands:</p>




<h2 id="conda">Conda
  <a href="#conda"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h2>
<pre tabindex="0"><code># Create a new Python 3.10 environment
conda create --name pytorch-env python=3.10 -y
# Activate the environment
conda activate pytorch-env
</code></pre>



<h2 id="mamba">Mamba
  <a href="#mamba"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h2>
<pre tabindex="0"><code># Create a new Python 3.10 environment
mamba create --name pytorch-env python=3.10 -y
# Activate the environment
mamba activate pytorch-env
</code></pre><p>Next, we&rsquo;ll install PyTorch. Run the appropriate command for your hardware and operating system.</p>




<h2 id="linuxwindows-cuda">Linux/Windows (CUDA)
  <a href="#linuxwindows-cuda"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h2>
<pre tabindex="0"><code># Install PyTorch with CUDA
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
</code></pre>



<h2 id="mac">Mac
  <a href="#mac"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h2>
<pre tabindex="0"><code># MPS (Metal Performance Shaders) acceleration is available on MacOS 12.3+
pip install torch torchvision torchaudio
</code></pre>



<h2 id="linux-cpu">Linux (CPU)
  <a href="#linux-cpu"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h2>
<pre tabindex="0"><code># Install PyTorch for CPU only
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
</code></pre>



<h2 id="windows-cpu">Windows (CPU)
  <a href="#windows-cpu"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h2>
<pre tabindex="0"><code># Install PyTorch for CPU only
pip install torch torchvision torchaudio
</code></pre>



<h3 id="installing-additional-libraries">Installing Additional Libraries
  <a href="#installing-additional-libraries"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h3>
<p>We also need to install some additional libraries for our project.</p>
<p>| <code>jupyter</code>     | An  open-source web application that allows you to create and share  documents that contain live code, equations, visualizations, and  narrative text. (
<a href="https://jupyter.org/" target="_blank" rel="noopener">link</a>) |
| <code>matplotlib</code>  | This package provides a comprehensive collection of visualization tools to  create high-quality plots, charts, and graphs for data exploration and  presentation. (
<a href="https://matplotlib.org/" target="_blank" rel="noopener">link</a>) |
| <code>pandas</code>      | This package provides fast, powerful, and flexible data analysis and manipulation tools. (
<a href="https://pandas.pydata.org/" target="_blank" rel="noopener">link</a>) |
| <code>pillow</code>      | The Python Imaging Library adds image processing capabilities. (
<a href="https://pillow.readthedocs.io/en/stable/" target="_blank" rel="noopener">link</a>) |
| <code>torchtnt</code>    | A library for PyTorch training tools and utilities. (
<a href="https://pytorch.org/tnt/stable/" target="_blank" rel="noopener">link</a>) |
| <code>tqdm</code>        | A Python library that provides fast, extensible progress bars for loops and other iterable objects in Python. (
<a href="https://tqdm.github.io/" target="_blank" rel="noopener">link</a>) |
| <code>tabulate</code>    | Pretty-print tabular data in Python. (
<a href="https://pypi.org/project/tabulate/" target="_blank" rel="noopener">link</a>) |
| <code>distinctipy</code> | A lightweight python package providing functions to generate colours that are visually distinct from one another. (
<a href="https://distinctipy.readthedocs.io/en/latest/" target="_blank" rel="noopener">link</a>) |</p>
<p>Run the following commands to install these additional libraries:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#57606a"># Install additional dependencies</span>
</span></span><span style="display:flex;"><span>pip install distinctipy jupyter matplotlib pandas pillow <span style="color:#953800">torchtnt</span><span style="color:#0550ae">==</span>0.2.0 tqdm tabulate
</span></span></code></pre></div>



<h3 id="installing-utility-packages">Installing Utility Packages
  <a href="#installing-utility-packages"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h3>
<p>We&rsquo;ll also install some utility packages I made to help us handle images, interact with PyTorch, and work with Pandas DataFrames. These utility packages provide shortcuts for routine tasks and keep our code clean and readable.</p>
<table>
  <thead>
      <tr>
          <th>Package</th>
          <th>Description</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><code>cjm_pandas_utils</code></td>
          <td>Some utility functions for working with Pandas. (
<a href="https://cj-mills.github.io/cjm-pandas-utils/" target="_blank" rel="noopener">link</a>)</td>
      </tr>
      <tr>
          <td><code>cjm_pil_utils</code></td>
          <td>Some PIL utility functions I frequently use. (
<a href="https://cj-mills.github.io/cjm-pil-utils/" target="_blank" rel="noopener">link</a>)</td>
      </tr>
      <tr>
          <td><code>cjm_psl_utils</code></td>
          <td>Some utility functions using the Python Standard Library. (
<a href="https://cj-mills.github.io/cjm-psl-utils/" target="_blank" rel="noopener">link</a>)</td>
      </tr>
      <tr>
          <td><code>cjm_pytorch_utils</code></td>
          <td>Some utility functions for working with PyTorch. (
<a href="https://cj-mills.github.io/cjm-pytorch-utils/" target="_blank" rel="noopener">link</a>)</td>
      </tr>
      <tr>
          <td><code>cjm_torchvision_tfms</code></td>
          <td>Some custom Torchvision tranforms. (
<a href="https://cj-mills.github.io/cjm-torchvision-tfms/" target="_blank" rel="noopener">link</a>)</td>
      </tr>
  </tbody>
</table>
<p>Run the following commands to install the utility packages:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#57606a"># Install additional utility packages</span>
</span></span><span style="display:flex;"><span>pip install cjm_pandas_utils cjm_pil_utils cjm_psl_utils cjm_pytorch_utils cjm_torchvision_tfms
</span></span></code></pre></div>



<h2 id="importing-the-required-dependencies">Importing the Required Dependencies
  <a href="#importing-the-required-dependencies"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h2>
<p>With our environment set up, letâ€™s dive into the code. First, we will import the necessary Python packages into our Jupyter Notebook.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#57606a"># Import Python Standard Library dependencies</span>
</span></span><span style="display:flex;"><span><span style="color:#cf222e">import</span> <span style="color:#24292e">datetime</span>
</span></span><span style="display:flex;"><span><span style="color:#cf222e">from</span> <span style="color:#24292e">functools</span> <span style="color:#cf222e">import</span> partial
</span></span><span style="display:flex;"><span><span style="color:#cf222e">from</span> <span style="color:#24292e">glob</span> <span style="color:#cf222e">import</span> glob
</span></span><span style="display:flex;"><span><span style="color:#cf222e">import</span> <span style="color:#24292e">json</span>
</span></span><span style="display:flex;"><span><span style="color:#cf222e">import</span> <span style="color:#24292e">math</span>
</span></span><span style="display:flex;"><span><span style="color:#cf222e">import</span> <span style="color:#24292e">multiprocessing</span>
</span></span><span style="display:flex;"><span><span style="color:#cf222e">import</span> <span style="color:#24292e">os</span>
</span></span><span style="display:flex;"><span><span style="color:#cf222e">from</span> <span style="color:#24292e">pathlib</span> <span style="color:#cf222e">import</span> Path
</span></span><span style="display:flex;"><span><span style="color:#cf222e">import</span> <span style="color:#24292e">random</span>
</span></span><span style="display:flex;"><span><span style="color:#cf222e">from</span> <span style="color:#24292e">typing</span> <span style="color:#cf222e">import</span> Any<span style="color:#1f2328">,</span> Dict<span style="color:#1f2328">,</span> Optional
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Import utility functions</span>
</span></span><span style="display:flex;"><span><span style="color:#cf222e">from</span> <span style="color:#24292e">cjm_psl_utils.core</span> <span style="color:#cf222e">import</span> download_file<span style="color:#1f2328">,</span> file_extract<span style="color:#1f2328">,</span> get_source_code
</span></span><span style="display:flex;"><span><span style="color:#cf222e">from</span> <span style="color:#24292e">cjm_pil_utils.core</span> <span style="color:#cf222e">import</span> resize_img<span style="color:#1f2328">,</span> get_img_files<span style="color:#1f2328">,</span> stack_imgs
</span></span><span style="display:flex;"><span><span style="color:#cf222e">from</span> <span style="color:#24292e">cjm_pytorch_utils.core</span> <span style="color:#cf222e">import</span> pil_to_tensor<span style="color:#1f2328">,</span> tensor_to_pil<span style="color:#1f2328">,</span> get_torch_device<span style="color:#1f2328">,</span> set_seed<span style="color:#1f2328">,</span> denorm_img_tensor<span style="color:#1f2328">,</span> move_data_to_device
</span></span><span style="display:flex;"><span><span style="color:#cf222e">from</span> <span style="color:#24292e">cjm_pandas_utils.core</span> <span style="color:#cf222e">import</span> markdown_to_pandas<span style="color:#1f2328">,</span> convert_to_numeric<span style="color:#1f2328">,</span> convert_to_string
</span></span><span style="display:flex;"><span><span style="color:#cf222e">from</span> <span style="color:#24292e">cjm_torchvision_tfms.core</span> <span style="color:#cf222e">import</span> ResizeMax<span style="color:#1f2328">,</span> PadSquare<span style="color:#1f2328">,</span> CustomRandomIoUCrop
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Import the distinctipy module</span>
</span></span><span style="display:flex;"><span><span style="color:#cf222e">from</span> <span style="color:#24292e">distinctipy</span> <span style="color:#cf222e">import</span> distinctipy
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Import matplotlib for creating plots</span>
</span></span><span style="display:flex;"><span><span style="color:#cf222e">import</span> <span style="color:#24292e">matplotlib.pyplot</span> <span style="color:#cf222e">as</span> <span style="color:#24292e">plt</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Import numpy</span>
</span></span><span style="display:flex;"><span><span style="color:#cf222e">import</span> <span style="color:#24292e">numpy</span> <span style="color:#cf222e">as</span> <span style="color:#24292e">np</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Import the pandas package</span>
</span></span><span style="display:flex;"><span><span style="color:#cf222e">import</span> <span style="color:#24292e">pandas</span> <span style="color:#cf222e">as</span> <span style="color:#24292e">pd</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Set options for Pandas DataFrame display</span>
</span></span><span style="display:flex;"><span>pd<span style="color:#0550ae">.</span>set_option<span style="color:#1f2328">(</span><span style="color:#0a3069">&#39;max_colwidth&#39;</span><span style="color:#1f2328">,</span> <span style="color:#cf222e">None</span><span style="color:#1f2328">)</span>  <span style="color:#57606a"># Do not truncate the contents of cells in the DataFrame</span>
</span></span><span style="display:flex;"><span>pd<span style="color:#0550ae">.</span>set_option<span style="color:#1f2328">(</span><span style="color:#0a3069">&#39;display.max_rows&#39;</span><span style="color:#1f2328">,</span> <span style="color:#cf222e">None</span><span style="color:#1f2328">)</span>  <span style="color:#57606a"># Display all rows in the DataFrame</span>
</span></span><span style="display:flex;"><span>pd<span style="color:#0550ae">.</span>set_option<span style="color:#1f2328">(</span><span style="color:#0a3069">&#39;display.max_columns&#39;</span><span style="color:#1f2328">,</span> <span style="color:#cf222e">None</span><span style="color:#1f2328">)</span>  <span style="color:#57606a"># Display all columns in the DataFrame</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Import PIL for image manipulation</span>
</span></span><span style="display:flex;"><span><span style="color:#cf222e">from</span> <span style="color:#24292e">PIL</span> <span style="color:#cf222e">import</span> Image<span style="color:#1f2328">,</span> ImageDraw
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Import PyTorch dependencies</span>
</span></span><span style="display:flex;"><span><span style="color:#cf222e">import</span> <span style="color:#24292e">torch</span>
</span></span><span style="display:flex;"><span><span style="color:#cf222e">from</span> <span style="color:#24292e">torch.amp</span> <span style="color:#cf222e">import</span> autocast
</span></span><span style="display:flex;"><span><span style="color:#cf222e">from</span> <span style="color:#24292e">torch.cuda.amp</span> <span style="color:#cf222e">import</span> GradScaler
</span></span><span style="display:flex;"><span><span style="color:#cf222e">import</span> <span style="color:#24292e">torch.nn</span> <span style="color:#cf222e">as</span> <span style="color:#24292e">nn</span>
</span></span><span style="display:flex;"><span><span style="color:#cf222e">import</span> <span style="color:#24292e">torch.nn.functional</span> <span style="color:#cf222e">as</span> <span style="color:#24292e">F</span>
</span></span><span style="display:flex;"><span><span style="color:#cf222e">from</span> <span style="color:#24292e">torch.utils.data</span> <span style="color:#cf222e">import</span> Dataset<span style="color:#1f2328">,</span> DataLoader
</span></span><span style="display:flex;"><span><span style="color:#cf222e">from</span> <span style="color:#24292e">torchtnt.utils</span> <span style="color:#cf222e">import</span> get_module_summary
</span></span><span style="display:flex;"><span><span style="color:#cf222e">import</span> <span style="color:#24292e">torchvision</span>
</span></span><span style="display:flex;"><span>torchvision<span style="color:#0550ae">.</span>disable_beta_transforms_warning<span style="color:#1f2328">()</span>
</span></span><span style="display:flex;"><span><span style="color:#cf222e">from</span> <span style="color:#24292e">torchvision.tv_tensors</span> <span style="color:#cf222e">import</span> BoundingBoxes<span style="color:#1f2328">,</span> Mask
</span></span><span style="display:flex;"><span><span style="color:#cf222e">from</span> <span style="color:#24292e">torchvision.utils</span> <span style="color:#cf222e">import</span> draw_bounding_boxes<span style="color:#1f2328">,</span> draw_segmentation_masks
</span></span><span style="display:flex;"><span><span style="color:#cf222e">import</span> <span style="color:#24292e">torchvision.transforms.v2</span>  <span style="color:#cf222e">as</span> <span style="color:#24292e">transforms</span>
</span></span><span style="display:flex;"><span><span style="color:#cf222e">from</span> <span style="color:#24292e">torchvision.transforms.v2</span> <span style="color:#cf222e">import</span> functional <span style="color:#cf222e">as</span> TF
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Import Mask R-CNN</span>
</span></span><span style="display:flex;"><span><span style="color:#cf222e">from</span> <span style="color:#24292e">torchvision.models.detection</span> <span style="color:#cf222e">import</span> maskrcnn_resnet50_fpn_v2<span style="color:#1f2328">,</span> MaskRCNN
</span></span><span style="display:flex;"><span><span style="color:#cf222e">from</span> <span style="color:#24292e">torchvision.models.detection</span> <span style="color:#cf222e">import</span> MaskRCNN_ResNet50_FPN_V2_Weights
</span></span><span style="display:flex;"><span><span style="color:#cf222e">from</span> <span style="color:#24292e">torchvision.models.detection.faster_rcnn</span> <span style="color:#cf222e">import</span> FastRCNNPredictor
</span></span><span style="display:flex;"><span><span style="color:#cf222e">from</span> <span style="color:#24292e">torchvision.models.detection.mask_rcnn</span> <span style="color:#cf222e">import</span> MaskRCNNPredictor
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Import tqdm for progress bar</span>
</span></span><span style="display:flex;"><span><span style="color:#cf222e">from</span> <span style="color:#24292e">tqdm.auto</span> <span style="color:#cf222e">import</span> tqdm
</span></span></code></pre></div>



<h2 id="setting-up-the-project">Setting Up the Project
  <a href="#setting-up-the-project"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h2>
<p>In this section, we set up some basics for our project, such as initializing random number generators, setting the PyTorch device to run the model, and preparing the folders for our project and datasets.</p>




<h3 id="setting-a-random-number-seed">Setting a Random Number Seed
  <a href="#setting-a-random-number-seed"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h3>
<p>First, we set the seed for generating random numbers using the 
<a href="https://cj-mills.github.io/cjm-pytorch-utils/core.html#set_seed" target="_blank" rel="noopener">set_seed</a> function from the <code>cjm_pytorch_utils</code> package.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#57606a"># Set the seed for generating random numbers in PyTorch, NumPy, and Python&#39;s random module.</span>
</span></span><span style="display:flex;"><span>seed <span style="color:#0550ae">=</span> <span style="color:#0550ae">1234</span>
</span></span><span style="display:flex;"><span>set_seed<span style="color:#1f2328">(</span>seed<span style="color:#1f2328">)</span>
</span></span></code></pre></div>



<h3 id="setting-the-device-and-data-type">Setting the Device and Data Type
  <a href="#setting-the-device-and-data-type"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h3>
<p>Next, we determine the device to use for training using the 
<a href="https://cj-mills.github.io/cjm-pytorch-utils/core.html#get_torch_device" target="_blank" rel="noopener">get_torch_device</a> function from the <code>cjm_pytorch_utils</code> package and set the data type of our tensors.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>device <span style="color:#0550ae">=</span> get_torch_device<span style="color:#1f2328">()</span>
</span></span><span style="display:flex;"><span>dtype <span style="color:#0550ae">=</span> torch<span style="color:#0550ae">.</span>float32
</span></span><span style="display:flex;"><span>device<span style="color:#1f2328">,</span> dtype
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>(&#39;cuda&#39;, torch.float32)
</span></span></code></pre></div>



<h3 id="setting-the-directory-paths">Setting the Directory Paths
  <a href="#setting-the-directory-paths"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h3>
<p>We can then set up a directory for our project to store our results  and other related files. The following code creates the folder in the  current directory (<code>./</code>). Update the path if that is not suitable for you.</p>
<p>We also need a place to store our dataset. Readers following the tutorial on their local machine should select a location with read-and-write access to store datasets. For a cloud service like Google Colab, you can set it to the current directory.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#57606a"># The name for the project</span>
</span></span><span style="display:flex;"><span>project_name <span style="color:#0550ae">=</span> <span style="color:#0a3069">f</span><span style="color:#0a3069">&#34;pytorch-mask-r-cnn-instance-segmentation&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># The path for the project folder</span>
</span></span><span style="display:flex;"><span>project_dir <span style="color:#0550ae">=</span> Path<span style="color:#1f2328">(</span><span style="color:#0a3069">f</span><span style="color:#0a3069">&#34;./</span><span style="color:#0a3069">{</span>project_name<span style="color:#0a3069">}</span><span style="color:#0a3069">/&#34;</span><span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Create the project directory if it does not already exist</span>
</span></span><span style="display:flex;"><span>project_dir<span style="color:#0550ae">.</span>mkdir<span style="color:#1f2328">(</span>parents<span style="color:#0550ae">=</span><span style="color:#cf222e">True</span><span style="color:#1f2328">,</span> exist_ok<span style="color:#0550ae">=</span><span style="color:#cf222e">True</span><span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Define path to store datasets</span>
</span></span><span style="display:flex;"><span>dataset_dir <span style="color:#0550ae">=</span> Path<span style="color:#1f2328">(</span><span style="color:#0a3069">&#34;./Datasets/&#34;</span><span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Create the dataset directory if it does not exist</span>
</span></span><span style="display:flex;"><span>dataset_dir<span style="color:#0550ae">.</span>mkdir<span style="color:#1f2328">(</span>parents<span style="color:#0550ae">=</span><span style="color:#cf222e">True</span><span style="color:#1f2328">,</span> exist_ok<span style="color:#0550ae">=</span><span style="color:#cf222e">True</span><span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>pd<span style="color:#0550ae">.</span>Series<span style="color:#1f2328">({</span>
</span></span><span style="display:flex;"><span>    <span style="color:#0a3069">&#34;Project Directory:&#34;</span><span style="color:#1f2328">:</span> project_dir<span style="color:#1f2328">,</span> 
</span></span><span style="display:flex;"><span>    <span style="color:#0a3069">&#34;Dataset Directory:&#34;</span><span style="color:#1f2328">:</span> dataset_dir
</span></span><span style="display:flex;"><span><span style="color:#1f2328">})</span><span style="color:#0550ae">.</span>to_frame<span style="color:#1f2328">()</span><span style="color:#0550ae">.</span>style<span style="color:#0550ae">.</span>hide<span style="color:#1f2328">(</span>axis<span style="color:#0550ae">=</span><span style="color:#0a3069">&#39;columns&#39;</span><span style="color:#1f2328">)</span>
</span></span></code></pre></div><div style="overflow-x:auto; max-height:500px">
<table id="T_68713">
  <thead>
  </thead>
  <tbody>
    <tr>
      <th id="T_68713_level0_row0" class="row_heading level0 row0" >Project Directory:</th>
      <td id="T_68713_row0_col0" class="data row0 col0" >pytorch-mask-r-cnn-instance-segmentation</td>
    </tr>
    <tr>
      <th id="T_68713_level0_row1" class="row_heading level0 row1" >Dataset Directory:</th>
      <td id="T_68713_row1_col0" class="data row1 col0" >Datasets</td>
    </tr>
  </tbody>
</table>
</div>
Double-check the project and dataset directories exist in the specified paths and that you can add files to them before continuing. At this  point, our project is set up and ready to go. In the next section, we  will download and explore the dataset.




<h2 id="loading-and-exploring-the-dataset">Loading and Exploring the Dataset
  <a href="#loading-and-exploring-the-dataset"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h2>
<p>Now that we set up the project, we can start working with our dataset. The dataset is originally from the following GitHub repository:</p>
<ul>
<li>
<a href="https://github.com/MbassiJaphet/pytorch-for-information-extraction" target="_blank" rel="noopener">pytorch-for-information-extraction</a></li>
</ul>
<p>I made a fork of the original repository with only the files needed for this tutorial, which takes up approximately 77 MB.</p>
<p>The segmentation masks for this dataset uses the 
<a href="https://github.com/labelmeai/labelme" target="_blank" rel="noopener">LabelMe</a> annotation format. You can learn more about this format and how to work with such annotations in the tutorial linked below:</p>
<ul>
<li>
<a href="/posts/torchvision-labelme-annotation-tutorials/segmentation-polygons/">Working with LabelMe Segmentation Annotations in Torchvision</a></li>
</ul>




<h3 id="setting-the-dataset-path">Setting the Dataset Path
  <a href="#setting-the-dataset-path"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h3>
<p>We first need to construct the name for the GitHub repository and define the path to the subfolder with the dataset.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#57606a"># Set the name of the dataset</span>
</span></span><span style="display:flex;"><span>dataset_name <span style="color:#0550ae">=</span> <span style="color:#0a3069">&#39;pytorch-for-information-extraction&#39;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Construct the GitHub repository name </span>
</span></span><span style="display:flex;"><span>gh_repo <span style="color:#0550ae">=</span> <span style="color:#0a3069">f</span><span style="color:#0a3069">&#39;cj-mills/</span><span style="color:#0a3069">{</span>dataset_name<span style="color:#0a3069">}</span><span style="color:#0a3069">&#39;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Create the path to the directory where the dataset will be extracted</span>
</span></span><span style="display:flex;"><span>dataset_path <span style="color:#0550ae">=</span> Path<span style="color:#1f2328">(</span><span style="color:#0a3069">f</span><span style="color:#0a3069">&#39;</span><span style="color:#0a3069">{</span>dataset_dir<span style="color:#0a3069">}</span><span style="color:#0a3069">/</span><span style="color:#0a3069">{</span>dataset_name<span style="color:#0a3069">}</span><span style="color:#0a3069">/code/datasets/detection/student-id/&#39;</span><span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>pd<span style="color:#0550ae">.</span>Series<span style="color:#1f2328">({</span>
</span></span><span style="display:flex;"><span>    <span style="color:#0a3069">&#34;GitHub Repository:&#34;</span><span style="color:#1f2328">:</span> gh_repo<span style="color:#1f2328">,</span> 
</span></span><span style="display:flex;"><span>    <span style="color:#0a3069">&#34;Dataset Path:&#34;</span><span style="color:#1f2328">:</span> dataset_path
</span></span><span style="display:flex;"><span><span style="color:#1f2328">})</span><span style="color:#0550ae">.</span>to_frame<span style="color:#1f2328">()</span><span style="color:#0550ae">.</span>style<span style="color:#0550ae">.</span>hide<span style="color:#1f2328">(</span>axis<span style="color:#0550ae">=</span><span style="color:#0a3069">&#39;columns&#39;</span><span style="color:#1f2328">)</span>
</span></span></code></pre></div><div style="overflow-x:auto; max-height:500px">
<table id="T_e4eb3">
  <thead>
  </thead>
  <tbody>
    <tr>
      <th id="T_e4eb3_level0_row0" class="row_heading level0 row0" >GitHub Repository:</th>
      <td id="T_e4eb3_row0_col0" class="data row0 col0" >cj-mills/pytorch-for-information-extraction</td>
    </tr>
    <tr>
      <th id="T_e4eb3_level0_row1" class="row_heading level0 row1" >Dataset Path:</th>
      <td id="T_e4eb3_row1_col0" class="data row1 col0" >Datasets/pytorch-for-information-extraction/code/datasets/detection/student-id</td>
    </tr>
  </tbody>
</table>
</div>




<h3 id="downloading-the-dataset">Downloading the Dataset
  <a href="#downloading-the-dataset"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h3>
<p>We can now clone the repository to the dataset directory we defined earlier.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#57606a"># Clone the dataset repository from GitHub</span>
</span></span><span style="display:flex;"><span><span style="color:#f6f8fa;background-color:#82071e">!</span>git clone <span style="color:#1f2328">{</span><span style="color:#0a3069">f</span><span style="color:#0a3069">&#39;https://github.com/</span><span style="color:#0a3069">{</span>gh_repo<span style="color:#0a3069">}</span><span style="color:#0a3069">.git&#39;</span><span style="color:#1f2328">}</span> <span style="color:#1f2328">{</span>dataset_dir<span style="color:#0550ae">/</span>dataset_name<span style="color:#1f2328">}</span>
</span></span></code></pre></div>



<h3 id="getting-the-image-and-annotation-files">Getting the Image and Annotation Files
  <a href="#getting-the-image-and-annotation-files"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h3>
<p>The dataset folder contains sample images and annotation files. Each sample image has its own JSON annotation file.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#57606a"># Get a list of image files in the dataset</span>
</span></span><span style="display:flex;"><span>img_file_paths <span style="color:#0550ae">=</span> get_img_files<span style="color:#1f2328">(</span>dataset_path<span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Get a list of JSON files in the dataset</span>
</span></span><span style="display:flex;"><span>annotation_file_paths <span style="color:#0550ae">=</span> <span style="color:#6639ba">list</span><span style="color:#1f2328">(</span>dataset_path<span style="color:#0550ae">.</span>glob<span style="color:#1f2328">(</span><span style="color:#0a3069">&#39;*.json&#39;</span><span style="color:#1f2328">))</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Display the names of the folders using a Pandas DataFrame</span>
</span></span><span style="display:flex;"><span>pd<span style="color:#0550ae">.</span>DataFrame<span style="color:#1f2328">({</span><span style="color:#0a3069">&#34;Image File&#34;</span><span style="color:#1f2328">:</span> <span style="color:#1f2328">[</span>file<span style="color:#0550ae">.</span>name <span style="color:#cf222e">for</span> file <span style="color:#0550ae">in</span> img_file_paths<span style="color:#1f2328">],</span> 
</span></span><span style="display:flex;"><span>              <span style="color:#0a3069">&#34;Annotation File&#34;</span><span style="color:#1f2328">:[</span>file<span style="color:#0550ae">.</span>name <span style="color:#cf222e">for</span> file <span style="color:#0550ae">in</span> annotation_file_paths<span style="color:#1f2328">]})</span><span style="color:#0550ae">.</span>head<span style="color:#1f2328">()</span>
</span></span></code></pre></div><div style="overflow-x:auto; max-height:500px">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Image File</th>
      <th>Annotation File</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>10134.jpg</td>
      <td>10134.json</td>
    </tr>
    <tr>
      <th>1</th>
      <td>10135.jpg</td>
      <td>10135.json</td>
    </tr>
    <tr>
      <th>2</th>
      <td>10136.jpg</td>
      <td>10136.json</td>
    </tr>
    <tr>
      <th>3</th>
      <td>10137.jpg</td>
      <td>10137.json</td>
    </tr>
    <tr>
      <th>4</th>
      <td>10138.jpg</td>
      <td>10138.json</td>
    </tr>
  </tbody>
</table>
</div>




<h3 id="get-image-file-paths">Get Image File Paths
  <a href="#get-image-file-paths"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h3>
<p>Each image file has a unique name that we can use to locate the corresponding annotation data. Letâ€™s make a dictionary that maps image names to file paths. The dictionary will allow us to retrieve the file path for a given image more efficiently.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#57606a"># Create a dictionary that maps file names to file paths</span>
</span></span><span style="display:flex;"><span>img_dict <span style="color:#0550ae">=</span> <span style="color:#1f2328">{</span>file<span style="color:#0550ae">.</span>stem <span style="color:#1f2328">:</span> file <span style="color:#cf222e">for</span> file <span style="color:#0550ae">in</span> img_file_paths<span style="color:#1f2328">}</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Print the number of image files</span>
</span></span><span style="display:flex;"><span><span style="color:#6639ba">print</span><span style="color:#1f2328">(</span><span style="color:#0a3069">f</span><span style="color:#0a3069">&#34;Number of Images: </span><span style="color:#0a3069">{</span><span style="color:#6639ba">len</span><span style="color:#1f2328">(</span>img_dict<span style="color:#1f2328">)</span><span style="color:#0a3069">}</span><span style="color:#0a3069">&#34;</span><span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Display the first five entries from the dictionary using a Pandas DataFrame</span>
</span></span><span style="display:flex;"><span>pd<span style="color:#0550ae">.</span>DataFrame<span style="color:#0550ae">.</span>from_dict<span style="color:#1f2328">(</span>img_dict<span style="color:#1f2328">,</span> orient<span style="color:#0550ae">=</span><span style="color:#0a3069">&#39;index&#39;</span><span style="color:#1f2328">)</span><span style="color:#0550ae">.</span>head<span style="color:#1f2328">()</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>Number of Images: 150
</span></span></code></pre></div><div style="overflow-x:auto; max-height:500px">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>10134</th>
      <td>Datasets/pytorch-for-information-extraction/code/datasets/detection/student-id/10134.jpg</td>
    </tr>
    <tr>
      <th>10135</th>
      <td>Datasets/pytorch-for-information-extraction/code/datasets/detection/student-id/10135.jpg</td>
    </tr>
    <tr>
      <th>10136</th>
      <td>Datasets/pytorch-for-information-extraction/code/datasets/detection/student-id/10136.jpg</td>
    </tr>
    <tr>
      <th>10137</th>
      <td>Datasets/pytorch-for-information-extraction/code/datasets/detection/student-id/10137.jpg</td>
    </tr>
    <tr>
      <th>10138</th>
      <td>Datasets/pytorch-for-information-extraction/code/datasets/detection/student-id/10138.jpg</td>
    </tr>
  </tbody>
</table>
</div>




<h3 id="get-image-annotations">Get Image Annotations
  <a href="#get-image-annotations"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h3>
<p>Next, we read the contents of the JSON annotation files into a Pandas DataFrame so we can easily query the annotations.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#57606a"># Create a generator that yields Pandas DataFrames containing the data from each JSON file</span>
</span></span><span style="display:flex;"><span>cls_dataframes <span style="color:#0550ae">=</span> <span style="color:#1f2328">(</span>pd<span style="color:#0550ae">.</span>read_json<span style="color:#1f2328">(</span>f<span style="color:#1f2328">,</span> orient<span style="color:#0550ae">=</span><span style="color:#0a3069">&#39;index&#39;</span><span style="color:#1f2328">)</span><span style="color:#0550ae">.</span>transpose<span style="color:#1f2328">()</span> <span style="color:#cf222e">for</span> f <span style="color:#0550ae">in</span> tqdm<span style="color:#1f2328">(</span>annotation_file_paths<span style="color:#1f2328">))</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Concatenate the DataFrames into a single DataFrame</span>
</span></span><span style="display:flex;"><span>annotation_df <span style="color:#0550ae">=</span> pd<span style="color:#0550ae">.</span>concat<span style="color:#1f2328">(</span>cls_dataframes<span style="color:#1f2328">,</span> ignore_index<span style="color:#0550ae">=</span><span style="color:#cf222e">False</span><span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Assign the image file name as the index for each row</span>
</span></span><span style="display:flex;"><span>annotation_df<span style="color:#1f2328">[</span><span style="color:#0a3069">&#39;index&#39;</span><span style="color:#1f2328">]</span> <span style="color:#0550ae">=</span> annotation_df<span style="color:#0550ae">.</span>apply<span style="color:#1f2328">(</span><span style="color:#cf222e">lambda</span> row<span style="color:#1f2328">:</span> row<span style="color:#1f2328">[</span><span style="color:#0a3069">&#39;imagePath&#39;</span><span style="color:#1f2328">]</span><span style="color:#0550ae">.</span>split<span style="color:#1f2328">(</span><span style="color:#0a3069">&#39;.&#39;</span><span style="color:#1f2328">)[</span><span style="color:#0550ae">0</span><span style="color:#1f2328">],</span> axis<span style="color:#0550ae">=</span><span style="color:#0550ae">1</span><span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>annotation_df <span style="color:#0550ae">=</span> annotation_df<span style="color:#0550ae">.</span>set_index<span style="color:#1f2328">(</span><span style="color:#0a3069">&#39;index&#39;</span><span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Keep only the rows that correspond to the filenames in the &#39;img_dict&#39; dictionary</span>
</span></span><span style="display:flex;"><span>annotation_df <span style="color:#0550ae">=</span> annotation_df<span style="color:#0550ae">.</span>loc<span style="color:#1f2328">[</span><span style="color:#6639ba">list</span><span style="color:#1f2328">(</span>img_dict<span style="color:#0550ae">.</span>keys<span style="color:#1f2328">())]</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Print the first 5 rows of the DataFrame</span>
</span></span><span style="display:flex;"><span>annotation_df<span style="color:#0550ae">.</span>head<span style="color:#1f2328">()</span>
</span></span></code></pre></div><div style="overflow-x:auto; max-height:500px">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>version</th>
      <th>flags</th>
      <th>shapes</th>
      <th>lineColor</th>
      <th>fillColor</th>
      <th>imagePath</th>
      <th>imageData</th>
      <th>imageHeight</th>
      <th>imageWidth</th>
    </tr>
    <tr>
      <th>index</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>10134</th>
      <td>3.21.1</td>
      <td>{}</td>
      <td>[{'label': 'student_id', 'line_color': None, 'fill_color': None, 'points': [[83.7142857142857, 133.57142857142856], [86.57142857142856, 123.57142857142856], [95.14285714285714, 117.14285714285714], [595.1428571428571, 125.71428571428571], [604.4285714285713, 127.85714285714285], [607.2857142857142, 138.57142857142856], [619.4285714285713, 443.57142857142856], [612.2857142857142, 449.2857142857142], [97.99999999999997, 469.2857142857142], [85.14285714285714, 465.71428571428567], [78.0, 457.1428571428571]], 'shape_type': 'polygon', 'flags': {}}]</td>
      <td>[0, 255, 0, 128]</td>
      <td>[255, 0, 0, 128]</td>
      <td>10134.jpg</td>
      <td></td>
      <td>480</td>
      <td>640</td>
    </tr>
    <tr>
      <th>10135</th>
      <td>3.21.1</td>
      <td>{}</td>
      <td>[{'label': 'student_id', 'line_color': None, 'fill_color': None, 'points': [[125.85714285714283, 288.57142857142856], [391.57142857142856, 24.285714285714285], [459.4285714285714, 7.857142857142857], [612.2857142857142, 166.42857142857142], [612.2857142857142, 174.28571428571428], [334.4285714285714, 477.85714285714283], [321.57142857142856, 478.5714285714285], [127.99999999999997, 297.1428571428571]], 'shape_type': 'polygon', 'flags': {}}]</td>
      <td>[0, 255, 0, 128]</td>
      <td>[255, 0, 0, 128]</td>
      <td>10135.jpg</td>
      <td></td>
      <td>480</td>
      <td>640</td>
    </tr>
    <tr>
      <th>10136</th>
      <td>3.21.1</td>
      <td>{}</td>
      <td>[{'label': 'student_id', 'line_color': None, 'fill_color': None, 'points': [[62.28571428571428, 44.285714285714285], [70.85714285714285, 39.99999999999999], [571.5714285714286, 81.42857142857142], [582.9999999999999, 90.71428571428571], [634.4285714285713, 374.99999999999994], [634.4285714285713, 389.2857142857142], [622.9999999999999, 394.2857142857142], [46.571428571428555, 427.1428571428571], [35.85714285714285, 424.99999999999994], [30.857142857142847, 414.99999999999994]], 'shape_type': 'polygon', 'flags': {}}]</td>
      <td>[0, 255, 0, 128]</td>
      <td>[255, 0, 0, 128]</td>
      <td>10136.jpg</td>
      <td></td>
      <td>480</td>
      <td>640</td>
    </tr>
    <tr>
      <th>10137</th>
      <td>3.21.1</td>
      <td>{}</td>
      <td>[{'label': 'student_id', 'line_color': None, 'fill_color': None, 'points': [[81.57142857142856, 137.85714285714283], [84.42857142857142, 129.28571428571428], [273.71428571428567, 29.999999999999996], [284.4285714285714, 29.999999999999996], [549.4285714285713, 277.85714285714283], [550.8571428571428, 288.57142857142856], [362.2857142857142, 472.85714285714283], [354.4285714285714, 472.85714285714283], [345.1428571428571, 467.1428571428571]], 'shape_type': 'polygon', 'flags': {}}, {'label': 'student_id', 'line_color': None, 'fill_color': None, 'points': [[324.4285714285714, 69.28571428571428], [340.1428571428571, 0.7142857142857141], [525.8571428571428, 27.857142857142854], [529.4285714285713, 177.14285714285714], [395.1428571428571, 135.0]], 'shape_type': 'polygon', 'flags': {}}]</td>
      <td>[0, 255, 0, 128]</td>
      <td>[255, 0, 0, 128]</td>
      <td>10137.jpg</td>
      <td></td>
      <td>480</td>
      <td>640</td>
    </tr>
    <tr>
      <th>10138</th>
      <td>3.21.1</td>
      <td>{}</td>
      <td>[{'label': 'student_id', 'line_color': None, 'fill_color': None, 'points': [[202.28571428571425, 12.142857142857142], [208.71428571428567, 2.857142857142856], [434.4285714285714, 70.0], [411.57142857142856, 392.1428571428571], [407.2857142857142, 445.71428571428567], [402.99999999999994, 453.57142857142856], [392.2857142857142, 454.99999999999994], [165.85714285714283, 470.71428571428567], [155.85714285714283, 467.85714285714283], [152.28571428571428, 459.2857142857142]], 'shape_type': 'polygon', 'flags': {}}]</td>
      <td>[0, 255, 0, 128]</td>
      <td>[255, 0, 0, 128]</td>
      <td>10138.jpg</td>
      <td></td>
      <td>480</td>
      <td>640</td>
    </tr>
  </tbody>
</table>
</div>
---
<p>The <code>shapes</code> column contains the point coordinates to draw the segmentation masks. We will also use this information to generate the associated bounding box annotations.</p>




<h3 id="inspecting-the-class-distribution">Inspecting the Class Distribution
  <a href="#inspecting-the-class-distribution"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h3>
<p>Now that we have the annotation data, we can extract the unique class names and inspect the class distribution. This small sample dataset only has one object class, but reviewing the class distribution is still good practice for other datasets.</p>




<h4 id="get-image-classes">Get image classes
  <a href="#get-image-classes"></a>
</h4>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#57606a"># Explode the &#39;shapes&#39; column in the annotation_df dataframe</span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Convert the resulting series to a dataframe and rename the &#39;shapes&#39; column to &#39;shapes&#39;</span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Apply the pandas Series function to the &#39;shapes&#39; column of the dataframe</span>
</span></span><span style="display:flex;"><span>shapes_df <span style="color:#0550ae">=</span> annotation_df<span style="color:#1f2328">[</span><span style="color:#0a3069">&#39;shapes&#39;</span><span style="color:#1f2328">]</span><span style="color:#0550ae">.</span>explode<span style="color:#1f2328">()</span><span style="color:#0550ae">.</span>to_frame<span style="color:#1f2328">()</span><span style="color:#0550ae">.</span>shapes<span style="color:#0550ae">.</span>apply<span style="color:#1f2328">(</span>pd<span style="color:#0550ae">.</span>Series<span style="color:#1f2328">)</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#57606a"># Get a list of unique labels in the &#39;annotation_df&#39; DataFrame</span>
</span></span><span style="display:flex;"><span>class_names <span style="color:#0550ae">=</span> shapes_df<span style="color:#1f2328">[</span><span style="color:#0a3069">&#39;label&#39;</span><span style="color:#1f2328">]</span><span style="color:#0550ae">.</span>unique<span style="color:#1f2328">()</span><span style="color:#0550ae">.</span>tolist<span style="color:#1f2328">()</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Display labels using a Pandas DataFrame</span>
</span></span><span style="display:flex;"><span>pd<span style="color:#0550ae">.</span>DataFrame<span style="color:#1f2328">(</span>class_names<span style="color:#1f2328">)</span>
</span></span></code></pre></div><div style="overflow-x:auto; max-height:500px">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>student_id</td>
    </tr>
  </tbody>
</table>
</div>




<h4 id="visualize-the-class-distribution">Visualize the class distribution
  <a href="#visualize-the-class-distribution"></a>
</h4>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#57606a"># Get the number of samples for each object class</span>
</span></span><span style="display:flex;"><span>class_counts <span style="color:#0550ae">=</span> shapes_df<span style="color:#1f2328">[</span><span style="color:#0a3069">&#39;label&#39;</span><span style="color:#1f2328">]</span><span style="color:#0550ae">.</span>value_counts<span style="color:#1f2328">()</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Plot the distribution</span>
</span></span><span style="display:flex;"><span>class_counts<span style="color:#0550ae">.</span>plot<span style="color:#1f2328">(</span>kind<span style="color:#0550ae">=</span><span style="color:#0a3069">&#39;bar&#39;</span><span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#0550ae">.</span>title<span style="color:#1f2328">(</span><span style="color:#0a3069">&#39;Class distribution&#39;</span><span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#0550ae">.</span>ylabel<span style="color:#1f2328">(</span><span style="color:#0a3069">&#39;Count&#39;</span><span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#0550ae">.</span>xlabel<span style="color:#1f2328">(</span><span style="color:#0a3069">&#39;Classes&#39;</span><span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#0550ae">.</span>xticks<span style="color:#1f2328">(</span><span style="color:#6639ba">range</span><span style="color:#1f2328">(</span><span style="color:#6639ba">len</span><span style="color:#1f2328">(</span>class_counts<span style="color:#0550ae">.</span>index<span style="color:#1f2328">)),</span> class_names<span style="color:#1f2328">,</span> rotation<span style="color:#0550ae">=</span><span style="color:#0550ae">75</span><span style="color:#1f2328">)</span>  <span style="color:#57606a"># Set the x-axis tick labels</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#0550ae">.</span>show<span style="color:#1f2328">()</span>
</span></span></code></pre></div><p><img src="./images/output_27_0.png" alt="">{fig-align=&ldquo;center&rdquo;}</p>




<h4 id="add-a-background-class">Add a background class
  <a href="#add-a-background-class"></a>
</h4>
<p>The Mask R-CNN model provided with the torchvision library expects datasets to have a <code>background</code> class. We can prepend one to the list of class names.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#57606a"># Prepend a `background` class to the list of class names</span>
</span></span><span style="display:flex;"><span>class_names <span style="color:#0550ae">=</span> <span style="color:#1f2328">[</span><span style="color:#0a3069">&#39;background&#39;</span><span style="color:#1f2328">]</span><span style="color:#0550ae">+</span>class_names
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Display labels using a Pandas DataFrame</span>
</span></span><span style="display:flex;"><span>pd<span style="color:#0550ae">.</span>DataFrame<span style="color:#1f2328">(</span>class_names<span style="color:#1f2328">)</span>
</span></span></code></pre></div><div style="overflow-x:auto; max-height:500px">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>background</td>
    </tr>
    <tr>
      <th>1</th>
      <td>student_id</td>
    </tr>
  </tbody>
</table>
</div>




<h3 id="visualizing-image-annotations">Visualizing Image Annotations
  <a href="#visualizing-image-annotations"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h3>
<p>Lastly, we will visualize the segmentation masks and bounding boxes for one of the sample images to demonstrate how to interpret the annotations.</p>




<h4 id="generate-a-color-map">Generate a color map
  <a href="#generate-a-color-map"></a>
</h4>
<p>While not required, assigning a unique color to segmentation masks and bounding boxes for each object class enhances visual distinction, allowing for easier identification of different objects in the scene. We can use the 
<a href="https://distinctipy.readthedocs.io/en/latest/" target="_blank" rel="noopener"><code>distinctipy</code></a> package to generate a visually distinct colormap.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#57606a"># Generate a list of colors with a length equal to the number of labels</span>
</span></span><span style="display:flex;"><span>colors <span style="color:#0550ae">=</span> distinctipy<span style="color:#0550ae">.</span>get_colors<span style="color:#1f2328">(</span><span style="color:#6639ba">len</span><span style="color:#1f2328">(</span>class_names<span style="color:#1f2328">))</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Make a copy of the color map in integer format</span>
</span></span><span style="display:flex;"><span>int_colors <span style="color:#0550ae">=</span> <span style="color:#1f2328">[</span><span style="color:#6639ba">tuple</span><span style="color:#1f2328">(</span><span style="color:#6639ba">int</span><span style="color:#1f2328">(</span>c<span style="color:#0550ae">*</span><span style="color:#0550ae">255</span><span style="color:#1f2328">)</span> <span style="color:#cf222e">for</span> c <span style="color:#0550ae">in</span> color<span style="color:#1f2328">)</span> <span style="color:#cf222e">for</span> color <span style="color:#0550ae">in</span> colors<span style="color:#1f2328">]</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Generate a color swatch to visualize the color map</span>
</span></span><span style="display:flex;"><span>distinctipy<span style="color:#0550ae">.</span>color_swatch<span style="color:#1f2328">(</span>colors<span style="color:#1f2328">)</span>
</span></span></code></pre></div><p><img src="./images/output_32_0.png" alt="">{fig-align=&ldquo;center&rdquo;}</p>




<h4 id="download-a-font-file">Download a font file
  <a href="#download-a-font-file"></a>
</h4>
<p>The 
<a href="https://pytorch.org/vision/stable/generated/torchvision.utils.draw_bounding_boxes.html" target="_blank" rel="noopener"><code>draw_bounding_boxes</code></a> function included with torchvision uses a pretty small font size. We  can increase the font size if we use a custom font. Font files are  available on sites like 
<a href="https://fonts.google.com/" target="_blank" rel="noopener">Google Fonts</a>, or we can use one included with the operating system.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#57606a"># Set the name of the font file</span>
</span></span><span style="display:flex;"><span>font_file <span style="color:#0550ae">=</span> <span style="color:#0a3069">&#39;KFOlCnqEu92Fr1MmEU9vAw.ttf&#39;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Download the font file</span>
</span></span><span style="display:flex;"><span>download_file<span style="color:#1f2328">(</span><span style="color:#0a3069">f</span><span style="color:#0a3069">&#34;https://fonts.gstatic.com/s/roboto/v30/</span><span style="color:#0a3069">{</span>font_file<span style="color:#0a3069">}</span><span style="color:#0a3069">&#34;</span><span style="color:#1f2328">,</span> <span style="color:#0a3069">&#34;./&#34;</span><span style="color:#1f2328">)</span>
</span></span></code></pre></div>



<h4 id="define-the-bounding-box-annotation-function">Define the bounding box annotation function
  <a href="#define-the-bounding-box-annotation-function"></a>
</h4>
<p>Letâ€™s make a partial function using <code>draw_bounding_boxes</code> since weâ€™ll use the same box thickness and font each time we visualize bounding boxes.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>draw_bboxes <span style="color:#0550ae">=</span> partial<span style="color:#1f2328">(</span>draw_bounding_boxes<span style="color:#1f2328">,</span> fill<span style="color:#0550ae">=</span><span style="color:#cf222e">False</span><span style="color:#1f2328">,</span> width<span style="color:#0550ae">=</span><span style="color:#0550ae">2</span><span style="color:#1f2328">,</span> font<span style="color:#0550ae">=</span>font_file<span style="color:#1f2328">,</span> font_size<span style="color:#0550ae">=</span><span style="color:#0550ae">25</span><span style="color:#1f2328">)</span>
</span></span></code></pre></div>



<h3 id="selecting-a-sample-image">Selecting a Sample Image
  <a href="#selecting-a-sample-image"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h3>
<p>We can use the unique ID for an image in the image dictionary to get the imageâ€™s file path and the associated annotations from the annotation DataFrame.</p>




<h4 id="load-the-sample-image">Load the sample image
  <a href="#load-the-sample-image"></a>
</h4>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#57606a"># Get the file ID of the first image file</span>
</span></span><span style="display:flex;"><span>file_id <span style="color:#0550ae">=</span> <span style="color:#6639ba">list</span><span style="color:#1f2328">(</span>img_dict<span style="color:#0550ae">.</span>keys<span style="color:#1f2328">())[</span><span style="color:#0550ae">56</span><span style="color:#1f2328">]</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Open the associated image file as a RGB image</span>
</span></span><span style="display:flex;"><span>sample_img <span style="color:#0550ae">=</span> Image<span style="color:#0550ae">.</span>open<span style="color:#1f2328">(</span>img_dict<span style="color:#1f2328">[</span>file_id<span style="color:#1f2328">])</span><span style="color:#0550ae">.</span>convert<span style="color:#1f2328">(</span><span style="color:#0a3069">&#39;RGB&#39;</span><span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Print the dimensions of the image</span>
</span></span><span style="display:flex;"><span><span style="color:#6639ba">print</span><span style="color:#1f2328">(</span><span style="color:#0a3069">f</span><span style="color:#0a3069">&#34;Image Dims: </span><span style="color:#0a3069">{</span>sample_img<span style="color:#0550ae">.</span>size<span style="color:#0a3069">}</span><span style="color:#0a3069">&#34;</span><span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Show the image</span>
</span></span><span style="display:flex;"><span>sample_img
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>Image Dims: (640, 480)
</span></span></code></pre></div><p><img src="./images/output_39_1.png" alt="">{fig-align=&ldquo;center&rdquo;}</p>




<h4 id="inspect-the-corresponding-annotation-data">Inspect the corresponding annotation data
  <a href="#inspect-the-corresponding-annotation-data"></a>
</h4>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#57606a"># Get the row from the &#39;annotation_df&#39; DataFrame corresponding to the &#39;file_id&#39;</span>
</span></span><span style="display:flex;"><span>annotation_df<span style="color:#0550ae">.</span>loc<span style="color:#1f2328">[</span>file_id<span style="color:#1f2328">]</span><span style="color:#0550ae">.</span>to_frame<span style="color:#1f2328">()</span>
</span></span></code></pre></div><div style="overflow-x:auto; max-height:500px">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>10067</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>version</th>
      <td>3.21.1</td>
    </tr>
    <tr>
      <th>flags</th>
      <td>{}</td>
    </tr>
    <tr>
      <th>shapes</th>
      <td>[{'label': 'student_id', 'line_color': None, 'fill_color': None, 'points': [[90.85714285714283, 22.142857142857142], [414.4285714285714, 17.857142857142854], [427.2857142857142, 19.285714285714285], [430.1428571428571, 24.999999999999996], [437.99999999999994, 222.85714285714283], [432.99999999999994, 227.1428571428571], [270.1428571428571, 231.42857142857142], [101.57142857142856, 234.28571428571428], [92.28571428571428, 232.85714285714283], [88.0, 227.85714285714283], [89.42857142857142, 44.99999999999999], [88.0, 31.428571428571427]], 'shape_type': 'polygon', 'flags': {}}, {'label': 'student_id', 'line_color': None, 'fill_color': None, 'points': [[0.14285714285713802, 226.42857142857142], [85.85714285714283, 107.14285714285714], [87.28571428571428, 234.99999999999997], [101.57142857142856, 235.7142857142857], [266.57142857142856, 231.42857142857142], [91, 479], [38, 479], [0, 453]], 'shape_type': 'polygon', 'flags': {}}, {'label': 'student_id', 'line_color': None, 'fill_color': None, 'points': [[287.99999999999994, 257.1428571428571], [293.71428571428567, 246.42857142857142], [314.4285714285714, 246.42857142857142], [615.1428571428571, 247.1428571428571], [621.5714285714286, 253.57142857142856], [626.5714285714286, 456.4285714285714], [617.9999999999999, 461.4285714285714], [311.57142857142856, 471.4285714285714], [297.2857142857143, 472.1428571428571], [290.1428571428571, 469.99999999999994], [285.1428571428571, 464.99999999999994]], 'shape_type': 'polygon', 'flags': {}}]</td>
    </tr>
    <tr>
      <th>lineColor</th>
      <td>[0, 255, 0, 128]</td>
    </tr>
    <tr>
      <th>fillColor</th>
      <td>[255, 0, 0, 128]</td>
    </tr>
    <tr>
      <th>imagePath</th>
      <td>10067.jpg</td>
    </tr>
    <tr>
      <th>imageData</th>
      <td></td>
    </tr>
    <tr>
      <th>imageHeight</th>
      <td>480</td>
    </tr>
    <tr>
      <th>imageWidth</th>
      <td>640</td>
    </tr>
  </tbody>
</table>
</div>
---
<p>The lists of point coordinates in the shapes column are the vertices of a polygon for the individual segmentation masks. We can use these to generate images for each segmentation mask.</p>




<h4 id="define-a-function-to-convert-segmentation-polygons-to-images">Define a function to convert segmentation polygons to images
  <a href="#define-a-function-to-convert-segmentation-polygons-to-images"></a>
</h4>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#cf222e">def</span> <span style="color:#6639ba">create_polygon_mask</span><span style="color:#1f2328">(</span>image_size<span style="color:#1f2328">,</span> vertices<span style="color:#1f2328">):</span>
</span></span><span style="display:flex;"><span>    <span style="color:#0a3069">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">    Create a grayscale image with a white polygonal area on a black background.
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">    Parameters:
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">    - image_size (tuple): A tuple representing the dimensions (width, height) of the image.
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">    - vertices (list): A list of tuples, each containing the x, y coordinates of a vertex
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">                        of the polygon. Vertices should be in clockwise or counter-clockwise order.
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">    Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">    - PIL.Image.Image: A PIL Image object containing the polygonal mask.
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#57606a"># Create a new black image with the given dimensions</span>
</span></span><span style="display:flex;"><span>    mask_img <span style="color:#0550ae">=</span> Image<span style="color:#0550ae">.</span>new<span style="color:#1f2328">(</span><span style="color:#0a3069">&#39;L&#39;</span><span style="color:#1f2328">,</span> image_size<span style="color:#1f2328">,</span> <span style="color:#0550ae">0</span><span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#57606a"># Draw the polygon on the image. The area inside the polygon will be white (255).</span>
</span></span><span style="display:flex;"><span>    ImageDraw<span style="color:#0550ae">.</span>Draw<span style="color:#1f2328">(</span>mask_img<span style="color:#1f2328">,</span> <span style="color:#0a3069">&#39;L&#39;</span><span style="color:#1f2328">)</span><span style="color:#0550ae">.</span>polygon<span style="color:#1f2328">(</span>vertices<span style="color:#1f2328">,</span> fill<span style="color:#0550ae">=</span><span style="color:#1f2328">(</span><span style="color:#0550ae">255</span><span style="color:#1f2328">))</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#57606a"># Return the image with the drawn polygon</span>
</span></span><span style="display:flex;"><span>    <span style="color:#cf222e">return</span> mask_img
</span></span></code></pre></div>



<h4 id="annotate-sample-image">Annotate sample image
  <a href="#annotate-sample-image"></a>
</h4>
<p>The torchvision library provides a 
<a href="https://pytorch.org/vision/stable/generated/torchvision.utils.draw_segmentation_masks.html#torchvision.utils.draw_segmentation_masks" target="_blank" rel="noopener"><code>draw_segmentation_masks</code></a> function to annotate images with segmentation masks. We can use the 
<a href="https://pytorch.org/vision/stable/generated/torchvision.ops.masks_to_boxes.html#torchvision.ops.masks_to_boxes" target="_blank" rel="noopener"><code>masks_to_boxes</code></a> function included with torchvision to generate bounding box annotations in the <code>[top-left X, top-left Y, bottom-right X, bottom-right Y]</code> format from the segmentation masks. That is the same format the <code>draw_bounding_boxes</code> function expects so we can use the output directly.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#57606a"># Extract the labels for the sample</span>
</span></span><span style="display:flex;"><span>labels <span style="color:#0550ae">=</span> <span style="color:#1f2328">[</span>shape<span style="color:#1f2328">[</span><span style="color:#0a3069">&#39;label&#39;</span><span style="color:#1f2328">]</span> <span style="color:#cf222e">for</span> shape <span style="color:#0550ae">in</span> annotation_df<span style="color:#0550ae">.</span>loc<span style="color:#1f2328">[</span>file_id<span style="color:#1f2328">][</span><span style="color:#0a3069">&#39;shapes&#39;</span><span style="color:#1f2328">]]</span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Extract the polygon points for segmentation mask</span>
</span></span><span style="display:flex;"><span>shape_points <span style="color:#0550ae">=</span> <span style="color:#1f2328">[</span>shape<span style="color:#1f2328">[</span><span style="color:#0a3069">&#39;points&#39;</span><span style="color:#1f2328">]</span> <span style="color:#cf222e">for</span> shape <span style="color:#0550ae">in</span> annotation_df<span style="color:#0550ae">.</span>loc<span style="color:#1f2328">[</span>file_id<span style="color:#1f2328">][</span><span style="color:#0a3069">&#39;shapes&#39;</span><span style="color:#1f2328">]]</span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Format polygon points for PIL</span>
</span></span><span style="display:flex;"><span>xy_coords <span style="color:#0550ae">=</span> <span style="color:#1f2328">[[</span><span style="color:#6639ba">tuple</span><span style="color:#1f2328">(</span>p<span style="color:#1f2328">)</span> <span style="color:#cf222e">for</span> p <span style="color:#0550ae">in</span> points<span style="color:#1f2328">]</span> <span style="color:#cf222e">for</span> points <span style="color:#0550ae">in</span> shape_points<span style="color:#1f2328">]</span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Generate mask images from polygons</span>
</span></span><span style="display:flex;"><span>mask_imgs <span style="color:#0550ae">=</span> <span style="color:#1f2328">[</span>create_polygon_mask<span style="color:#1f2328">(</span>sample_img<span style="color:#0550ae">.</span>size<span style="color:#1f2328">,</span> xy<span style="color:#1f2328">)</span> <span style="color:#cf222e">for</span> xy <span style="color:#0550ae">in</span> xy_coords<span style="color:#1f2328">]</span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Convert mask images to tensors</span>
</span></span><span style="display:flex;"><span>masks <span style="color:#0550ae">=</span> torch<span style="color:#0550ae">.</span>concat<span style="color:#1f2328">([</span>Mask<span style="color:#1f2328">(</span>transforms<span style="color:#0550ae">.</span>PILToTensor<span style="color:#1f2328">()(</span>mask_img<span style="color:#1f2328">),</span> dtype<span style="color:#0550ae">=</span>torch<span style="color:#0550ae">.</span>bool<span style="color:#1f2328">)</span> <span style="color:#cf222e">for</span> mask_img <span style="color:#0550ae">in</span> mask_imgs<span style="color:#1f2328">])</span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Generate bounding box annotations from segmentation masks</span>
</span></span><span style="display:flex;"><span>bboxes <span style="color:#0550ae">=</span> torchvision<span style="color:#0550ae">.</span>ops<span style="color:#0550ae">.</span>masks_to_boxes<span style="color:#1f2328">(</span>masks<span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Annotate the sample image with segmentation masks</span>
</span></span><span style="display:flex;"><span>annotated_tensor <span style="color:#0550ae">=</span> draw_segmentation_masks<span style="color:#1f2328">(</span>
</span></span><span style="display:flex;"><span>    image<span style="color:#0550ae">=</span>transforms<span style="color:#0550ae">.</span>PILToTensor<span style="color:#1f2328">()(</span>sample_img<span style="color:#1f2328">),</span> 
</span></span><span style="display:flex;"><span>    masks<span style="color:#0550ae">=</span>masks<span style="color:#1f2328">,</span> 
</span></span><span style="display:flex;"><span>    alpha<span style="color:#0550ae">=</span><span style="color:#0550ae">0.3</span><span style="color:#1f2328">,</span> 
</span></span><span style="display:flex;"><span>    colors<span style="color:#0550ae">=</span><span style="color:#1f2328">[</span>int_colors<span style="color:#1f2328">[</span>i<span style="color:#1f2328">]</span> <span style="color:#cf222e">for</span> i <span style="color:#0550ae">in</span> <span style="color:#1f2328">[</span>class_names<span style="color:#0550ae">.</span>index<span style="color:#1f2328">(</span>label<span style="color:#1f2328">)</span> <span style="color:#cf222e">for</span> label <span style="color:#0550ae">in</span> labels<span style="color:#1f2328">]]</span>
</span></span><span style="display:flex;"><span><span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Annotate the sample image with labels and bounding boxes</span>
</span></span><span style="display:flex;"><span>annotated_tensor <span style="color:#0550ae">=</span> draw_bboxes<span style="color:#1f2328">(</span>
</span></span><span style="display:flex;"><span>    image<span style="color:#0550ae">=</span>annotated_tensor<span style="color:#1f2328">,</span> 
</span></span><span style="display:flex;"><span>    boxes<span style="color:#0550ae">=</span>bboxes<span style="color:#1f2328">,</span> 
</span></span><span style="display:flex;"><span>    labels<span style="color:#0550ae">=</span>labels<span style="color:#1f2328">,</span> 
</span></span><span style="display:flex;"><span>    colors<span style="color:#0550ae">=</span><span style="color:#1f2328">[</span>int_colors<span style="color:#1f2328">[</span>i<span style="color:#1f2328">]</span> <span style="color:#cf222e">for</span> i <span style="color:#0550ae">in</span> <span style="color:#1f2328">[</span>class_names<span style="color:#0550ae">.</span>index<span style="color:#1f2328">(</span>label<span style="color:#1f2328">)</span> <span style="color:#cf222e">for</span> label <span style="color:#0550ae">in</span> labels<span style="color:#1f2328">]]</span>
</span></span><span style="display:flex;"><span><span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>tensor_to_pil<span style="color:#1f2328">(</span>annotated_tensor<span style="color:#1f2328">)</span>
</span></span></code></pre></div><p><img src="./images/output_45_0.png" alt="">{fig-align=&ldquo;center&rdquo;}</p>
<p>We have explored the dataset and visualized the annotations for a sample image. In the next section, we will load and prepare our model.</p>




<h2 id="loading-the-mask-r-cnn-model">Loading the Mask R-CNN Model
  <a href="#loading-the-mask-r-cnn-model"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h2>
<p>TorchVision provides 
<a href="https://pytorch.org/vision/stable/models/generated/torchvision.models.detection.maskrcnn_resnet50_fpn_v2.html#torchvision.models.detection.MaskRCNN_ResNet50_FPN_V2_Weights" target="_blank" rel="noopener">checkpoints</a> for the Mask R-CNN model trained on the 
<a href="https://cocodataset.org/" target="_blank" rel="noopener">COCO</a> (Common Objects in Context) dataset. We can initialize a model with these pretrained weights using the 
<a href="https://pytorch.org/vision/stable/models/generated/torchvision.models.detection.maskrcnn_resnet50_fpn_v2.html#torchvision.models.detection.maskrcnn_resnet50_fpn_v2" target="_blank" rel="noopener"><code>maskrcnn_resnet50_fpn_v2</code></a> function. We must then replace the bounding box and segmentation mask predictors for the pretrained model with new ones for our dataset.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#57606a"># Initialize a Mask R-CNN model with pretrained weights</span>
</span></span><span style="display:flex;"><span>model <span style="color:#0550ae">=</span> maskrcnn_resnet50_fpn_v2<span style="color:#1f2328">(</span>weights<span style="color:#0550ae">=</span><span style="color:#0a3069">&#39;DEFAULT&#39;</span><span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Get the number of input features for the classifier</span>
</span></span><span style="display:flex;"><span>in_features_box <span style="color:#0550ae">=</span> model<span style="color:#0550ae">.</span>roi_heads<span style="color:#0550ae">.</span>box_predictor<span style="color:#0550ae">.</span>cls_score<span style="color:#0550ae">.</span>in_features
</span></span><span style="display:flex;"><span>in_features_mask <span style="color:#0550ae">=</span> model<span style="color:#0550ae">.</span>roi_heads<span style="color:#0550ae">.</span>mask_predictor<span style="color:#0550ae">.</span>conv5_mask<span style="color:#0550ae">.</span>in_channels
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Get the numbner of output channels for the Mask Predictor</span>
</span></span><span style="display:flex;"><span>dim_reduced <span style="color:#0550ae">=</span> model<span style="color:#0550ae">.</span>roi_heads<span style="color:#0550ae">.</span>mask_predictor<span style="color:#0550ae">.</span>conv5_mask<span style="color:#0550ae">.</span>out_channels
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Replace the box predictor</span>
</span></span><span style="display:flex;"><span>model<span style="color:#0550ae">.</span>roi_heads<span style="color:#0550ae">.</span>box_predictor <span style="color:#0550ae">=</span> FastRCNNPredictor<span style="color:#1f2328">(</span>in_channels<span style="color:#0550ae">=</span>in_features_box<span style="color:#1f2328">,</span> num_classes<span style="color:#0550ae">=</span><span style="color:#6639ba">len</span><span style="color:#1f2328">(</span>class_names<span style="color:#1f2328">))</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Replace the mask predictor</span>
</span></span><span style="display:flex;"><span>model<span style="color:#0550ae">.</span>roi_heads<span style="color:#0550ae">.</span>mask_predictor <span style="color:#0550ae">=</span> MaskRCNNPredictor<span style="color:#1f2328">(</span>in_channels<span style="color:#0550ae">=</span>in_features_mask<span style="color:#1f2328">,</span> dim_reduced<span style="color:#0550ae">=</span>dim_reduced<span style="color:#1f2328">,</span> num_classes<span style="color:#0550ae">=</span><span style="color:#6639ba">len</span><span style="color:#1f2328">(</span>class_names<span style="color:#1f2328">))</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Set the model&#39;s device and data type</span>
</span></span><span style="display:flex;"><span>model<span style="color:#0550ae">.</span>to<span style="color:#1f2328">(</span>device<span style="color:#0550ae">=</span>device<span style="color:#1f2328">,</span> dtype<span style="color:#0550ae">=</span>dtype<span style="color:#1f2328">);</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Add attributes to store the device and model name for later reference</span>
</span></span><span style="display:flex;"><span>model<span style="color:#0550ae">.</span>device <span style="color:#0550ae">=</span> device
</span></span><span style="display:flex;"><span>model<span style="color:#0550ae">.</span>name <span style="color:#0550ae">=</span> <span style="color:#0a3069">&#39;maskrcnn_resnet50_fpn_v2&#39;</span>
</span></span></code></pre></div><p>The model internally normalizes input using the mean and standard deviation values used during the pretraining process, so we do not need to keep track of them separately.</p>




<h3 id="summarizing-the-model">Summarizing the Model
  <a href="#summarizing-the-model"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h3>
<p>Before moving on, letâ€™s generate a summary of our model to get an overview of its performance characteristics. We can use this to gauge the computational requirements for deploying the model.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>test_inp <span style="color:#0550ae">=</span> torch<span style="color:#0550ae">.</span>randn<span style="color:#1f2328">(</span><span style="color:#0550ae">1</span><span style="color:#1f2328">,</span> <span style="color:#0550ae">3</span><span style="color:#1f2328">,</span> <span style="color:#0550ae">256</span><span style="color:#1f2328">,</span> <span style="color:#0550ae">256</span><span style="color:#1f2328">)</span><span style="color:#0550ae">.</span>to<span style="color:#1f2328">(</span>device<span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>summary_df <span style="color:#0550ae">=</span> markdown_to_pandas<span style="color:#1f2328">(</span><span style="color:#0a3069">f</span><span style="color:#0a3069">&#34;</span><span style="color:#0a3069">{</span>get_module_summary<span style="color:#1f2328">(</span>model<span style="color:#0550ae">.</span>eval<span style="color:#1f2328">(),</span> <span style="color:#1f2328">[</span>test_inp<span style="color:#1f2328">])</span><span style="color:#0a3069">}</span><span style="color:#0a3069">&#34;</span><span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># # Filter the summary to only contain Conv2d layers and the model</span>
</span></span><span style="display:flex;"><span>summary_df <span style="color:#0550ae">=</span> summary_df<span style="color:#1f2328">[</span>summary_df<span style="color:#0550ae">.</span>index <span style="color:#0550ae">==</span> <span style="color:#0550ae">0</span><span style="color:#1f2328">]</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>summary_df<span style="color:#0550ae">.</span>drop<span style="color:#1f2328">([</span><span style="color:#0a3069">&#39;In size&#39;</span><span style="color:#1f2328">,</span> <span style="color:#0a3069">&#39;Out size&#39;</span><span style="color:#1f2328">,</span> <span style="color:#0a3069">&#39;Contains Uninitialized Parameters?&#39;</span><span style="color:#1f2328">],</span> axis<span style="color:#0550ae">=</span><span style="color:#0550ae">1</span><span style="color:#1f2328">)</span>
</span></span></code></pre></div><div style="overflow-x:auto; max-height:500px">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Type</th>
      <th># Parameters</th>
      <th># Trainable Parameters</th>
      <th>Size (bytes)</th>
      <th>Forward FLOPs</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>MaskRCNN</td>
      <td>45.9 M</td>
      <td>45.7 M</td>
      <td>183 M</td>
      <td>331 G</td>
    </tr>
  </tbody>
</table>
</div>
The above table shows the model has approximately `45.7` million trainable parameters. It takes up `183` Megabytes and performs around `331` billion floating point operations for a single `256x256` RGB image. This model internally resizes input images and executes the same number of floating point operations for different input resolutions.
<p>That completes the model setup. In the next section, we will prepare our dataset for training.</p>




<h2 id="preparing-the-data">Preparing the Data
  <a href="#preparing-the-data"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h2>
<p>The data preparation involves several steps, such as applying data augmentation techniques, setting up the train-validation split for the dataset, resizing and padding the images, defining the training dataset class, and initializing DataLoaders to feed data to the model.</p>




<h3 id="training-validation-split">Training-Validation Split
  <a href="#training-validation-split"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h3>
<p>Letâ€™s begin by defining the training-validation split. Weâ€™ll randomly select 80% of the available samples for the training set and use the remaining 20% for the validation set.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#57606a"># Get the list of image IDs</span>
</span></span><span style="display:flex;"><span>img_keys <span style="color:#0550ae">=</span> <span style="color:#6639ba">list</span><span style="color:#1f2328">(</span>img_dict<span style="color:#0550ae">.</span>keys<span style="color:#1f2328">())</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Shuffle the image IDs</span>
</span></span><span style="display:flex;"><span>random<span style="color:#0550ae">.</span>shuffle<span style="color:#1f2328">(</span>img_keys<span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Define the percentage of the images that should be used for training</span>
</span></span><span style="display:flex;"><span>train_pct <span style="color:#0550ae">=</span> <span style="color:#0550ae">0.8</span>
</span></span><span style="display:flex;"><span>val_pct <span style="color:#0550ae">=</span> <span style="color:#0550ae">0.2</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Calculate the index at which to split the subset of image paths into training and validation sets</span>
</span></span><span style="display:flex;"><span>train_split <span style="color:#0550ae">=</span> <span style="color:#6639ba">int</span><span style="color:#1f2328">(</span><span style="color:#6639ba">len</span><span style="color:#1f2328">(</span>img_keys<span style="color:#1f2328">)</span><span style="color:#0550ae">*</span>train_pct<span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>val_split <span style="color:#0550ae">=</span> <span style="color:#6639ba">int</span><span style="color:#1f2328">(</span><span style="color:#6639ba">len</span><span style="color:#1f2328">(</span>img_keys<span style="color:#1f2328">)</span><span style="color:#0550ae">*</span><span style="color:#1f2328">(</span>train_pct<span style="color:#0550ae">+</span>val_pct<span style="color:#1f2328">))</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Split the subset of image paths into training and validation sets</span>
</span></span><span style="display:flex;"><span>train_keys <span style="color:#0550ae">=</span> img_keys<span style="color:#1f2328">[:</span>train_split<span style="color:#1f2328">]</span>
</span></span><span style="display:flex;"><span>val_keys <span style="color:#0550ae">=</span> img_keys<span style="color:#1f2328">[</span>train_split<span style="color:#1f2328">:]</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Print the number of images in the training and validation sets</span>
</span></span><span style="display:flex;"><span>pd<span style="color:#0550ae">.</span>Series<span style="color:#1f2328">({</span>
</span></span><span style="display:flex;"><span>    <span style="color:#0a3069">&#34;Training Samples:&#34;</span><span style="color:#1f2328">:</span> <span style="color:#6639ba">len</span><span style="color:#1f2328">(</span>train_keys<span style="color:#1f2328">),</span>
</span></span><span style="display:flex;"><span>    <span style="color:#0a3069">&#34;Validation Samples:&#34;</span><span style="color:#1f2328">:</span> <span style="color:#6639ba">len</span><span style="color:#1f2328">(</span>val_keys<span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span><span style="color:#1f2328">})</span><span style="color:#0550ae">.</span>to_frame<span style="color:#1f2328">()</span><span style="color:#0550ae">.</span>style<span style="color:#0550ae">.</span>hide<span style="color:#1f2328">(</span>axis<span style="color:#0550ae">=</span><span style="color:#0a3069">&#39;columns&#39;</span><span style="color:#1f2328">)</span>
</span></span></code></pre></div><div style="overflow-x:auto; max-height:500px">
<table id="T_ebe49">
  <thead>
  </thead>
  <tbody>
    <tr>
      <th id="T_ebe49_level0_row0" class="row_heading level0 row0" >Training Samples:</th>
      <td id="T_ebe49_row0_col0" class="data row0 col0" >120</td>
    </tr>
    <tr>
      <th id="T_ebe49_level0_row1" class="row_heading level0 row1" >Validation Samples:</th>
      <td id="T_ebe49_row1_col0" class="data row1 col0" >30</td>
    </tr>
  </tbody>
</table>
</div>




<h3 id="data-augmentation">Data Augmentation
  <a href="#data-augmentation"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h3>
<p>Next, we can define what data augmentations to apply to images during training. I created a few custom image transforms to help streamline the code.</p>
<p>The 
<a href="https://cj-mills.github.io/cjm-torchvision-tfms/core.html#customrandomioucrop" target="_blank" rel="noopener">first</a> extends torchvisionâ€™s 
<a href="https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.RandomIoUCrop.html#torchvision.transforms.v2.RandomIoUCrop" target="_blank" rel="noopener"><code>RandomIoUCrop</code></a> transform to give the user more control over how much it crops into bounding box areas. The 
<a href="https://cj-mills.github.io/cjm-torchvision-tfms/core.html#resizemax" target="_blank" rel="noopener">second</a> resizes images based on their largest dimension rather than their smallest. The 
<a href="https://cj-mills.github.io/cjm-torchvision-tfms/core.html#padsquare" target="_blank" rel="noopener">third</a> applies square padding and allows the padding to be applied equally on both sides or randomly split between the two sides.</p>
<p>All three are available through the 
<a href="https://cj-mills.github.io/cjm-torchvision-tfms/" target="_blank" rel="noopener"><code>cjm-torchvision-tfms</code></a> package.</p>




<h4 id="set-training-image-size">Set training image size
  <a href="#set-training-image-size"></a>
</h4>
<p>First, weâ€™ll set the size to use for training. The 
<a href="https://cj-mills.github.io/cjm-torchvision-tfms/core.html#resizemax" target="_blank" rel="noopener"><code>ResizeMax</code></a> transform will resize images so that the longest dimension equals this value while preserving the aspect ratio. The 
<a href="https://cj-mills.github.io/cjm-torchvision-tfms/core.html#padsquare" target="_blank" rel="noopener"><code>PadSquare</code></a> transform will then pad the other side to make all the input squares.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#57606a"># Set training image size</span>
</span></span><span style="display:flex;"><span>train_sz <span style="color:#0550ae">=</span> <span style="color:#0550ae">512</span>
</span></span></code></pre></div>



<h4 id="initialize-the-transforms">Initialize the transforms
  <a href="#initialize-the-transforms"></a>
</h4>
<p>Now, we can initialize the transform objects. The <code>jitter_factor</code> parameter for the 
<a href="https://cj-mills.github.io/cjm-torchvision-tfms/core.html#customrandomioucrop" target="_blank" rel="noopener"><code>CustomRandomIoUCrop</code></a> transform controls how much the center coordinates for the crop area  can deviate from the center of a bounding box. Setting this to a value  greater than zero allows the transform to crop into the bounding box  area. Weâ€™ll keep this value small as cutting into the hand gestures too  much will change their meaning.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#57606a"># Create a RandomIoUCrop object</span>
</span></span><span style="display:flex;"><span>iou_crop <span style="color:#0550ae">=</span> CustomRandomIoUCrop<span style="color:#1f2328">(</span>min_scale<span style="color:#0550ae">=</span><span style="color:#0550ae">0.3</span><span style="color:#1f2328">,</span> 
</span></span><span style="display:flex;"><span>                               max_scale<span style="color:#0550ae">=</span><span style="color:#0550ae">1.0</span><span style="color:#1f2328">,</span> 
</span></span><span style="display:flex;"><span>                               min_aspect_ratio<span style="color:#0550ae">=</span><span style="color:#0550ae">0.5</span><span style="color:#1f2328">,</span> 
</span></span><span style="display:flex;"><span>                               max_aspect_ratio<span style="color:#0550ae">=</span><span style="color:#0550ae">2.0</span><span style="color:#1f2328">,</span> 
</span></span><span style="display:flex;"><span>                               sampler_options<span style="color:#0550ae">=</span><span style="color:#1f2328">[</span><span style="color:#0550ae">0.0</span><span style="color:#1f2328">,</span> <span style="color:#0550ae">0.1</span><span style="color:#1f2328">,</span> <span style="color:#0550ae">0.3</span><span style="color:#1f2328">,</span> <span style="color:#0550ae">0.5</span><span style="color:#1f2328">,</span> <span style="color:#0550ae">0.7</span><span style="color:#1f2328">,</span> <span style="color:#0550ae">0.9</span><span style="color:#1f2328">,</span> <span style="color:#0550ae">1.0</span><span style="color:#1f2328">],</span>
</span></span><span style="display:flex;"><span>                               trials<span style="color:#0550ae">=</span><span style="color:#0550ae">400</span><span style="color:#1f2328">,</span> 
</span></span><span style="display:flex;"><span>                               jitter_factor<span style="color:#0550ae">=</span><span style="color:#0550ae">0.25</span><span style="color:#1f2328">)</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#57606a"># Create a `ResizeMax` object</span>
</span></span><span style="display:flex;"><span>resize_max <span style="color:#0550ae">=</span> ResizeMax<span style="color:#1f2328">(</span>max_sz<span style="color:#0550ae">=</span>train_sz<span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Create a `PadSquare` object</span>
</span></span><span style="display:flex;"><span>pad_square <span style="color:#0550ae">=</span> PadSquare<span style="color:#1f2328">(</span>shift<span style="color:#0550ae">=</span><span style="color:#cf222e">True</span><span style="color:#1f2328">,</span> fill<span style="color:#0550ae">=</span><span style="color:#0550ae">0</span><span style="color:#1f2328">)</span>
</span></span></code></pre></div>



<h4 id="test-the-transforms">Test the transforms
  <a href="#test-the-transforms"></a>
</h4>
<p>Weâ€™ll pass input through the <code>CustomRandomIoUCrop</code> transform first and then through <code>ResizeMax</code> and <code>PadSquare</code>. We can pass the result through a final resize operation to ensure both sides match the <code>train_sz</code> value.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#57606a"># Extract the labels for the sample</span>
</span></span><span style="display:flex;"><span>labels <span style="color:#0550ae">=</span> <span style="color:#1f2328">[</span>shape<span style="color:#1f2328">[</span><span style="color:#0a3069">&#39;label&#39;</span><span style="color:#1f2328">]</span> <span style="color:#cf222e">for</span> shape <span style="color:#0550ae">in</span> annotation_df<span style="color:#0550ae">.</span>loc<span style="color:#1f2328">[</span>file_id<span style="color:#1f2328">][</span><span style="color:#0a3069">&#39;shapes&#39;</span><span style="color:#1f2328">]]</span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Extract the polygon points for segmentation mask</span>
</span></span><span style="display:flex;"><span>shape_points <span style="color:#0550ae">=</span> <span style="color:#1f2328">[</span>shape<span style="color:#1f2328">[</span><span style="color:#0a3069">&#39;points&#39;</span><span style="color:#1f2328">]</span> <span style="color:#cf222e">for</span> shape <span style="color:#0550ae">in</span> annotation_df<span style="color:#0550ae">.</span>loc<span style="color:#1f2328">[</span>file_id<span style="color:#1f2328">][</span><span style="color:#0a3069">&#39;shapes&#39;</span><span style="color:#1f2328">]]</span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Format polygon points for PIL</span>
</span></span><span style="display:flex;"><span>xy_coords <span style="color:#0550ae">=</span> <span style="color:#1f2328">[[</span><span style="color:#6639ba">tuple</span><span style="color:#1f2328">(</span>p<span style="color:#1f2328">)</span> <span style="color:#cf222e">for</span> p <span style="color:#0550ae">in</span> points<span style="color:#1f2328">]</span> <span style="color:#cf222e">for</span> points <span style="color:#0550ae">in</span> shape_points<span style="color:#1f2328">]</span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Generate mask images from polygons</span>
</span></span><span style="display:flex;"><span>mask_imgs <span style="color:#0550ae">=</span> <span style="color:#1f2328">[</span>create_polygon_mask<span style="color:#1f2328">(</span>sample_img<span style="color:#0550ae">.</span>size<span style="color:#1f2328">,</span> xy<span style="color:#1f2328">)</span> <span style="color:#cf222e">for</span> xy <span style="color:#0550ae">in</span> xy_coords<span style="color:#1f2328">]</span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Convert mask images to tensors</span>
</span></span><span style="display:flex;"><span>masks <span style="color:#0550ae">=</span> torch<span style="color:#0550ae">.</span>concat<span style="color:#1f2328">([</span>Mask<span style="color:#1f2328">(</span>transforms<span style="color:#0550ae">.</span>PILToTensor<span style="color:#1f2328">()(</span>mask_img<span style="color:#1f2328">),</span> dtype<span style="color:#0550ae">=</span>torch<span style="color:#0550ae">.</span>bool<span style="color:#1f2328">)</span> <span style="color:#cf222e">for</span> mask_img <span style="color:#0550ae">in</span> mask_imgs<span style="color:#1f2328">])</span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Generate bounding box annotations from segmentation masks</span>
</span></span><span style="display:flex;"><span>bboxes <span style="color:#0550ae">=</span> BoundingBoxes<span style="color:#1f2328">(</span>data<span style="color:#0550ae">=</span>torchvision<span style="color:#0550ae">.</span>ops<span style="color:#0550ae">.</span>masks_to_boxes<span style="color:#1f2328">(</span>masks<span style="color:#1f2328">),</span> <span style="color:#6639ba">format</span><span style="color:#0550ae">=</span><span style="color:#0a3069">&#39;xyxy&#39;</span><span style="color:#1f2328">,</span> canvas_size<span style="color:#0550ae">=</span>sample_img<span style="color:#0550ae">.</span>size<span style="color:#1f2328">[::</span><span style="color:#0550ae">-</span><span style="color:#0550ae">1</span><span style="color:#1f2328">])</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Get colors for dataset sample</span>
</span></span><span style="display:flex;"><span>sample_colors <span style="color:#0550ae">=</span> <span style="color:#1f2328">[</span>int_colors<span style="color:#1f2328">[</span>i<span style="color:#1f2328">]</span> <span style="color:#cf222e">for</span> i <span style="color:#0550ae">in</span> <span style="color:#1f2328">[</span>class_names<span style="color:#0550ae">.</span>index<span style="color:#1f2328">(</span>label<span style="color:#1f2328">)</span> <span style="color:#cf222e">for</span> label <span style="color:#0550ae">in</span> labels<span style="color:#1f2328">]]</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Prepare mask and bounding box targets</span>
</span></span><span style="display:flex;"><span>targets <span style="color:#0550ae">=</span> <span style="color:#1f2328">{</span>
</span></span><span style="display:flex;"><span>    <span style="color:#0a3069">&#39;masks&#39;</span><span style="color:#1f2328">:</span> Mask<span style="color:#1f2328">(</span>masks<span style="color:#1f2328">),</span> 
</span></span><span style="display:flex;"><span>    <span style="color:#0a3069">&#39;boxes&#39;</span><span style="color:#1f2328">:</span> bboxes<span style="color:#1f2328">,</span> 
</span></span><span style="display:flex;"><span>    <span style="color:#0a3069">&#39;labels&#39;</span><span style="color:#1f2328">:</span> torch<span style="color:#0550ae">.</span>Tensor<span style="color:#1f2328">([</span>class_names<span style="color:#0550ae">.</span>index<span style="color:#1f2328">(</span>label<span style="color:#1f2328">)</span> <span style="color:#cf222e">for</span> label <span style="color:#0550ae">in</span> labels<span style="color:#1f2328">])</span>
</span></span><span style="display:flex;"><span><span style="color:#1f2328">}</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Crop the image</span>
</span></span><span style="display:flex;"><span>cropped_img<span style="color:#1f2328">,</span> targets <span style="color:#0550ae">=</span> iou_crop<span style="color:#1f2328">(</span>sample_img<span style="color:#1f2328">,</span> targets<span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Resize the image</span>
</span></span><span style="display:flex;"><span>resized_img<span style="color:#1f2328">,</span> targets <span style="color:#0550ae">=</span> resize_max<span style="color:#1f2328">(</span>cropped_img<span style="color:#1f2328">,</span> targets<span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Pad the image</span>
</span></span><span style="display:flex;"><span>padded_img<span style="color:#1f2328">,</span> targets <span style="color:#0550ae">=</span> pad_square<span style="color:#1f2328">(</span>resized_img<span style="color:#1f2328">,</span> targets<span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Ensure the padded image is the target size</span>
</span></span><span style="display:flex;"><span>resize <span style="color:#0550ae">=</span> transforms<span style="color:#0550ae">.</span>Resize<span style="color:#1f2328">([</span>train_sz<span style="color:#1f2328">]</span> <span style="color:#0550ae">*</span> <span style="color:#0550ae">2</span><span style="color:#1f2328">,</span> antialias<span style="color:#0550ae">=</span><span style="color:#cf222e">True</span><span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>resized_padded_img<span style="color:#1f2328">,</span> targets <span style="color:#0550ae">=</span> resize<span style="color:#1f2328">(</span>padded_img<span style="color:#1f2328">,</span> targets<span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>sanitized_img<span style="color:#1f2328">,</span> targets <span style="color:#0550ae">=</span> transforms<span style="color:#0550ae">.</span>SanitizeBoundingBoxes<span style="color:#1f2328">()(</span>resized_padded_img<span style="color:#1f2328">,</span> targets<span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Annotate the sample image with segmentation masks</span>
</span></span><span style="display:flex;"><span>annotated_tensor <span style="color:#0550ae">=</span> draw_segmentation_masks<span style="color:#1f2328">(</span>
</span></span><span style="display:flex;"><span>    image<span style="color:#0550ae">=</span>transforms<span style="color:#0550ae">.</span>PILToTensor<span style="color:#1f2328">()(</span>sanitized_img<span style="color:#1f2328">),</span> 
</span></span><span style="display:flex;"><span>    masks<span style="color:#0550ae">=</span>targets<span style="color:#1f2328">[</span><span style="color:#0a3069">&#39;masks&#39;</span><span style="color:#1f2328">],</span> 
</span></span><span style="display:flex;"><span>    alpha<span style="color:#0550ae">=</span><span style="color:#0550ae">0.3</span><span style="color:#1f2328">,</span> 
</span></span><span style="display:flex;"><span>    colors<span style="color:#0550ae">=</span>sample_colors
</span></span><span style="display:flex;"><span><span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Annotate the sample image with labels and bounding boxes</span>
</span></span><span style="display:flex;"><span>annotated_tensor <span style="color:#0550ae">=</span> draw_bboxes<span style="color:#1f2328">(</span>
</span></span><span style="display:flex;"><span>    image<span style="color:#0550ae">=</span>annotated_tensor<span style="color:#1f2328">,</span> 
</span></span><span style="display:flex;"><span>    boxes<span style="color:#0550ae">=</span>targets<span style="color:#1f2328">[</span><span style="color:#0a3069">&#39;boxes&#39;</span><span style="color:#1f2328">],</span> 
</span></span><span style="display:flex;"><span>    labels<span style="color:#0550ae">=</span><span style="color:#1f2328">[</span>class_names<span style="color:#1f2328">[</span><span style="color:#6639ba">int</span><span style="color:#1f2328">(</span>label<span style="color:#0550ae">.</span>item<span style="color:#1f2328">())]</span> <span style="color:#cf222e">for</span> label <span style="color:#0550ae">in</span> targets<span style="color:#1f2328">[</span><span style="color:#0a3069">&#39;labels&#39;</span><span style="color:#1f2328">]],</span> 
</span></span><span style="display:flex;"><span>    colors<span style="color:#0550ae">=</span>sample_colors
</span></span><span style="display:flex;"><span><span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># # Display the annotated image</span>
</span></span><span style="display:flex;"><span>display<span style="color:#1f2328">(</span>tensor_to_pil<span style="color:#1f2328">(</span>annotated_tensor<span style="color:#1f2328">))</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>pd<span style="color:#0550ae">.</span>Series<span style="color:#1f2328">({</span>
</span></span><span style="display:flex;"><span>    <span style="color:#0a3069">&#34;Source Image:&#34;</span><span style="color:#1f2328">:</span> sample_img<span style="color:#0550ae">.</span>size<span style="color:#1f2328">,</span>
</span></span><span style="display:flex;"><span>    <span style="color:#0a3069">&#34;Cropped Image:&#34;</span><span style="color:#1f2328">:</span> cropped_img<span style="color:#0550ae">.</span>size<span style="color:#1f2328">,</span>
</span></span><span style="display:flex;"><span>    <span style="color:#0a3069">&#34;Resized Image:&#34;</span><span style="color:#1f2328">:</span> resized_img<span style="color:#0550ae">.</span>size<span style="color:#1f2328">,</span>
</span></span><span style="display:flex;"><span>    <span style="color:#0a3069">&#34;Padded Image:&#34;</span><span style="color:#1f2328">:</span> padded_img<span style="color:#0550ae">.</span>size<span style="color:#1f2328">,</span>
</span></span><span style="display:flex;"><span>    <span style="color:#0a3069">&#34;Resized Padded Image:&#34;</span><span style="color:#1f2328">:</span> resized_padded_img<span style="color:#0550ae">.</span>size<span style="color:#1f2328">,</span>
</span></span><span style="display:flex;"><span><span style="color:#1f2328">})</span><span style="color:#0550ae">.</span>to_frame<span style="color:#1f2328">()</span><span style="color:#0550ae">.</span>style<span style="color:#0550ae">.</span>hide<span style="color:#1f2328">(</span>axis<span style="color:#0550ae">=</span><span style="color:#0a3069">&#39;columns&#39;</span><span style="color:#1f2328">)</span>
</span></span></code></pre></div><p><img src="./images/output_60_0.png" alt="">{fig-align=&ldquo;center&rdquo;}</p>
<div style="overflow-x:auto; max-height:500px">
<table id="T_10683">
  <thead>
  </thead>
  <tbody>
    <tr>
      <th id="T_10683_level0_row0" class="row_heading level0 row0" >Source Image:</th>
      <td id="T_10683_row0_col0" class="data row0 col0" >(640, 480)</td>
    </tr>
    <tr>
      <th id="T_10683_level0_row1" class="row_heading level0 row1" >Cropped Image:</th>
      <td id="T_10683_row1_col0" class="data row1 col0" >(286, 387)</td>
    </tr>
    <tr>
      <th id="T_10683_level0_row2" class="row_heading level0 row2" >Resized Image:</th>
      <td id="T_10683_row2_col0" class="data row2 col0" >(378, 511)</td>
    </tr>
    <tr>
      <th id="T_10683_level0_row3" class="row_heading level0 row3" >Padded Image:</th>
      <td id="T_10683_row3_col0" class="data row3 col0" >(511, 511)</td>
    </tr>
    <tr>
      <th id="T_10683_level0_row4" class="row_heading level0 row4" >Resized Padded Image:</th>
      <td id="T_10683_row4_col0" class="data row4 col0" >(512, 512)</td>
    </tr>
  </tbody>
</table>
</div>




<h3 id="training-dataset-class">Training Dataset Class
  <a href="#training-dataset-class"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h3>
<p>Now, we can define a custom dataset class to load images, extract the segmentation masks, generate the bounding box annotations, and apply the image transforms during training.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#cf222e">class</span> <span style="color:#1f2328">StudentIDDataset</span><span style="color:#1f2328">(</span>Dataset<span style="color:#1f2328">):</span>
</span></span><span style="display:flex;"><span>    <span style="color:#0a3069">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">    This class represents a PyTorch Dataset for a collection of images and their annotations.
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">    The class is designed to load images along with their corresponding segmentation masks, bounding box annotations, and labels.
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#cf222e">def</span> __init__<span style="color:#1f2328">(</span><span style="color:#6a737d">self</span><span style="color:#1f2328">,</span> img_keys<span style="color:#1f2328">,</span> annotation_df<span style="color:#1f2328">,</span> img_dict<span style="color:#1f2328">,</span> class_to_idx<span style="color:#1f2328">,</span> transforms<span style="color:#0550ae">=</span><span style="color:#cf222e">None</span><span style="color:#1f2328">):</span>
</span></span><span style="display:flex;"><span>        <span style="color:#0a3069">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">        Constructor for the HagridDataset class.
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">        Parameters:
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">        img_keys (list): List of unique identifiers for images.
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">        annotation_df (DataFrame): DataFrame containing the image annotations.
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">        img_dict (dict): Dictionary mapping image identifiers to image file paths.
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">        class_to_idx (dict): Dictionary mapping class labels to indices.
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">        transforms (callable, optional): Optional transform to be applied on a sample.
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#6639ba">super</span><span style="color:#1f2328">(</span>Dataset<span style="color:#1f2328">,</span> <span style="color:#6a737d">self</span><span style="color:#1f2328">)</span><span style="color:#0550ae">.</span>__init__<span style="color:#1f2328">()</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#6a737d">self</span><span style="color:#0550ae">.</span>_img_keys <span style="color:#0550ae">=</span> img_keys  <span style="color:#57606a"># List of image keys</span>
</span></span><span style="display:flex;"><span>        <span style="color:#6a737d">self</span><span style="color:#0550ae">.</span>_annotation_df <span style="color:#0550ae">=</span> annotation_df  <span style="color:#57606a"># DataFrame containing annotations</span>
</span></span><span style="display:flex;"><span>        <span style="color:#6a737d">self</span><span style="color:#0550ae">.</span>_img_dict <span style="color:#0550ae">=</span> img_dict  <span style="color:#57606a"># Dictionary mapping image keys to image paths</span>
</span></span><span style="display:flex;"><span>        <span style="color:#6a737d">self</span><span style="color:#0550ae">.</span>_class_to_idx <span style="color:#0550ae">=</span> class_to_idx  <span style="color:#57606a"># Dictionary mapping class names to class indices</span>
</span></span><span style="display:flex;"><span>        <span style="color:#6a737d">self</span><span style="color:#0550ae">.</span>_transforms <span style="color:#0550ae">=</span> transforms  <span style="color:#57606a"># Image transforms to be applied</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>    <span style="color:#cf222e">def</span> __len__<span style="color:#1f2328">(</span><span style="color:#6a737d">self</span><span style="color:#1f2328">):</span>
</span></span><span style="display:flex;"><span>        <span style="color:#0a3069">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">        Returns the length of the dataset.
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">        Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">        int: The number of items in the dataset.
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#cf222e">return</span> <span style="color:#6639ba">len</span><span style="color:#1f2328">(</span><span style="color:#6a737d">self</span><span style="color:#0550ae">.</span>_img_keys<span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>    <span style="color:#cf222e">def</span> __getitem__<span style="color:#1f2328">(</span><span style="color:#6a737d">self</span><span style="color:#1f2328">,</span> index<span style="color:#1f2328">):</span>
</span></span><span style="display:flex;"><span>        <span style="color:#0a3069">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">        Fetch an item from the dataset at the specified index.
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">        Parameters:
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">        index (int): Index of the item to fetch from the dataset.
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">        Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">        tuple: A tuple containing the image and its associated target (annotations).
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#57606a"># Retrieve the key for the image at the specified index</span>
</span></span><span style="display:flex;"><span>        img_key <span style="color:#0550ae">=</span> <span style="color:#6a737d">self</span><span style="color:#0550ae">.</span>_img_keys<span style="color:#1f2328">[</span>index<span style="color:#1f2328">]</span>
</span></span><span style="display:flex;"><span>        <span style="color:#57606a"># Get the annotations for this image</span>
</span></span><span style="display:flex;"><span>        annotation <span style="color:#0550ae">=</span> <span style="color:#6a737d">self</span><span style="color:#0550ae">.</span>_annotation_df<span style="color:#0550ae">.</span>loc<span style="color:#1f2328">[</span>img_key<span style="color:#1f2328">]</span>
</span></span><span style="display:flex;"><span>        <span style="color:#57606a"># Load the image and its target (segmentation masks, bounding boxes and labels)</span>
</span></span><span style="display:flex;"><span>        image<span style="color:#1f2328">,</span> target <span style="color:#0550ae">=</span> <span style="color:#6a737d">self</span><span style="color:#0550ae">.</span>_load_image_and_target<span style="color:#1f2328">(</span>annotation<span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#57606a"># Apply the transformations, if any</span>
</span></span><span style="display:flex;"><span>        <span style="color:#cf222e">if</span> <span style="color:#6a737d">self</span><span style="color:#0550ae">.</span>_transforms<span style="color:#1f2328">:</span>
</span></span><span style="display:flex;"><span>            image<span style="color:#1f2328">,</span> target <span style="color:#0550ae">=</span> <span style="color:#6a737d">self</span><span style="color:#0550ae">.</span>_transforms<span style="color:#1f2328">(</span>image<span style="color:#1f2328">,</span> target<span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#cf222e">return</span> image<span style="color:#1f2328">,</span> target
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#cf222e">def</span> <span style="color:#6639ba">_load_image_and_target</span><span style="color:#1f2328">(</span><span style="color:#6a737d">self</span><span style="color:#1f2328">,</span> annotation<span style="color:#1f2328">):</span>
</span></span><span style="display:flex;"><span>        <span style="color:#0a3069">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">        Load an image and its target (bounding boxes and labels).
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">        Parameters:
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">        annotation (pandas.Series): The annotations for an image.
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">        Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">        tuple: A tuple containing the image and a dictionary with &#39;boxes&#39; and &#39;labels&#39; keys.
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#57606a"># Retrieve the file path of the image</span>
</span></span><span style="display:flex;"><span>        filepath <span style="color:#0550ae">=</span> <span style="color:#6a737d">self</span><span style="color:#0550ae">.</span>_img_dict<span style="color:#1f2328">[</span>annotation<span style="color:#0550ae">.</span>name<span style="color:#1f2328">]</span>
</span></span><span style="display:flex;"><span>        <span style="color:#57606a"># Open the image file and convert it to RGB</span>
</span></span><span style="display:flex;"><span>        image <span style="color:#0550ae">=</span> Image<span style="color:#0550ae">.</span>open<span style="color:#1f2328">(</span>filepath<span style="color:#1f2328">)</span><span style="color:#0550ae">.</span>convert<span style="color:#1f2328">(</span><span style="color:#0a3069">&#39;RGB&#39;</span><span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#57606a"># Convert the class labels to indices</span>
</span></span><span style="display:flex;"><span>        labels <span style="color:#0550ae">=</span> <span style="color:#1f2328">[</span>shape<span style="color:#1f2328">[</span><span style="color:#0a3069">&#39;label&#39;</span><span style="color:#1f2328">]</span> <span style="color:#cf222e">for</span> shape <span style="color:#0550ae">in</span> annotation<span style="color:#1f2328">[</span><span style="color:#0a3069">&#39;shapes&#39;</span><span style="color:#1f2328">]]</span>
</span></span><span style="display:flex;"><span>        labels <span style="color:#0550ae">=</span> torch<span style="color:#0550ae">.</span>Tensor<span style="color:#1f2328">([</span><span style="color:#6a737d">self</span><span style="color:#0550ae">.</span>_class_to_idx<span style="color:#1f2328">[</span>label<span style="color:#1f2328">]</span> <span style="color:#cf222e">for</span> label <span style="color:#0550ae">in</span> labels<span style="color:#1f2328">])</span>
</span></span><span style="display:flex;"><span>        labels <span style="color:#0550ae">=</span> labels<span style="color:#0550ae">.</span>to<span style="color:#1f2328">(</span>dtype<span style="color:#0550ae">=</span>torch<span style="color:#0550ae">.</span>int64<span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#57606a"># Convert polygons to mask images</span>
</span></span><span style="display:flex;"><span>        shape_points <span style="color:#0550ae">=</span> <span style="color:#1f2328">[</span>shape<span style="color:#1f2328">[</span><span style="color:#0a3069">&#39;points&#39;</span><span style="color:#1f2328">]</span> <span style="color:#cf222e">for</span> shape <span style="color:#0550ae">in</span> annotation<span style="color:#1f2328">[</span><span style="color:#0a3069">&#39;shapes&#39;</span><span style="color:#1f2328">]]</span>
</span></span><span style="display:flex;"><span>        xy_coords <span style="color:#0550ae">=</span> <span style="color:#1f2328">[[</span><span style="color:#6639ba">tuple</span><span style="color:#1f2328">(</span>p<span style="color:#1f2328">)</span> <span style="color:#cf222e">for</span> p <span style="color:#0550ae">in</span> points<span style="color:#1f2328">]</span> <span style="color:#cf222e">for</span> points <span style="color:#0550ae">in</span> shape_points<span style="color:#1f2328">]</span>
</span></span><span style="display:flex;"><span>        mask_imgs <span style="color:#0550ae">=</span> <span style="color:#1f2328">[</span>create_polygon_mask<span style="color:#1f2328">(</span>image<span style="color:#0550ae">.</span>size<span style="color:#1f2328">,</span> xy<span style="color:#1f2328">)</span> <span style="color:#cf222e">for</span> xy <span style="color:#0550ae">in</span> xy_coords<span style="color:#1f2328">]</span>
</span></span><span style="display:flex;"><span>        masks <span style="color:#0550ae">=</span> Mask<span style="color:#1f2328">(</span>torch<span style="color:#0550ae">.</span>concat<span style="color:#1f2328">([</span>Mask<span style="color:#1f2328">(</span>transforms<span style="color:#0550ae">.</span>PILToTensor<span style="color:#1f2328">()(</span>mask_img<span style="color:#1f2328">),</span> dtype<span style="color:#0550ae">=</span>torch<span style="color:#0550ae">.</span>bool<span style="color:#1f2328">)</span> <span style="color:#cf222e">for</span> mask_img <span style="color:#0550ae">in</span> mask_imgs<span style="color:#1f2328">]))</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#57606a"># Generate bounding box annotations from segmentation masks</span>
</span></span><span style="display:flex;"><span>        bboxes <span style="color:#0550ae">=</span> BoundingBoxes<span style="color:#1f2328">(</span>data<span style="color:#0550ae">=</span>torchvision<span style="color:#0550ae">.</span>ops<span style="color:#0550ae">.</span>masks_to_boxes<span style="color:#1f2328">(</span>masks<span style="color:#1f2328">),</span> <span style="color:#6639ba">format</span><span style="color:#0550ae">=</span><span style="color:#0a3069">&#39;xyxy&#39;</span><span style="color:#1f2328">,</span> canvas_size<span style="color:#0550ae">=</span>image<span style="color:#0550ae">.</span>size<span style="color:#1f2328">[::</span><span style="color:#0550ae">-</span><span style="color:#0550ae">1</span><span style="color:#1f2328">])</span>
</span></span><span style="display:flex;"><span>                
</span></span><span style="display:flex;"><span>        <span style="color:#cf222e">return</span> image<span style="color:#1f2328">,</span> <span style="color:#1f2328">{</span><span style="color:#0a3069">&#39;masks&#39;</span><span style="color:#1f2328">:</span> masks<span style="color:#1f2328">,</span><span style="color:#0a3069">&#39;boxes&#39;</span><span style="color:#1f2328">:</span> bboxes<span style="color:#1f2328">,</span> <span style="color:#0a3069">&#39;labels&#39;</span><span style="color:#1f2328">:</span> labels<span style="color:#1f2328">}</span>
</span></span></code></pre></div>



<h3 id="image-transforms">Image Transforms
  <a href="#image-transforms"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h3>
<p>We&rsquo;ll add additional data augmentations with the IoU crop transform to help the model generalize.</p>
<table>
  <thead>
      <tr>
          <th>Transform</th>
          <th>Description</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><code>ColorJitter</code></td>
          <td>Randomly change the brightness, contrast, saturation and hue of an image or video. (
<a href="https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.ColorJitter.html#torchvision.transforms.v2.ColorJitter" target="_blank" rel="noopener">link</a>)</td>
      </tr>
      <tr>
          <td><code>RandomGrayscale</code></td>
          <td>Randomly convert image or videos to grayscale with a probability of p (default 0.1). (
<a href="https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.RandomGrayscale.html#torchvision.transforms.v2.RandomGrayscale" target="_blank" rel="noopener">link</a>)</td>
      </tr>
      <tr>
          <td><code>RandomEqualize</code></td>
          <td>Equalize the histogram of the given image or video with a given probability. (
<a href="https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.RandomEqualize.html#torchvision.transforms.v2.RandomEqualize" target="_blank" rel="noopener">link</a>)</td>
      </tr>
      <tr>
          <td><code>RandomPosterize</code></td>
          <td>Randomly posterize an image by reducing the number of bits for each color channel. (
<a href="https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.RandomPosterize.html#torchvision.transforms.v2.RandomPosterize" target="_blank" rel="noopener">link</a>)</td>
      </tr>
  </tbody>
</table>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#57606a"># Compose transforms for data augmentation</span>
</span></span><span style="display:flex;"><span>data_aug_tfms <span style="color:#0550ae">=</span> transforms<span style="color:#0550ae">.</span>Compose<span style="color:#1f2328">(</span>
</span></span><span style="display:flex;"><span>    transforms<span style="color:#0550ae">=</span><span style="color:#1f2328">[</span>
</span></span><span style="display:flex;"><span>        iou_crop<span style="color:#1f2328">,</span>
</span></span><span style="display:flex;"><span>        transforms<span style="color:#0550ae">.</span>ColorJitter<span style="color:#1f2328">(</span>
</span></span><span style="display:flex;"><span>                brightness <span style="color:#0550ae">=</span> <span style="color:#1f2328">(</span><span style="color:#0550ae">0.875</span><span style="color:#1f2328">,</span> <span style="color:#0550ae">1.125</span><span style="color:#1f2328">),</span>
</span></span><span style="display:flex;"><span>                contrast <span style="color:#0550ae">=</span> <span style="color:#1f2328">(</span><span style="color:#0550ae">0.5</span><span style="color:#1f2328">,</span> <span style="color:#0550ae">1.5</span><span style="color:#1f2328">),</span>
</span></span><span style="display:flex;"><span>                saturation <span style="color:#0550ae">=</span> <span style="color:#1f2328">(</span><span style="color:#0550ae">0.5</span><span style="color:#1f2328">,</span> <span style="color:#0550ae">1.5</span><span style="color:#1f2328">),</span>
</span></span><span style="display:flex;"><span>                hue <span style="color:#0550ae">=</span> <span style="color:#1f2328">(</span><span style="color:#0550ae">-</span><span style="color:#0550ae">0.05</span><span style="color:#1f2328">,</span> <span style="color:#0550ae">0.05</span><span style="color:#1f2328">),</span>
</span></span><span style="display:flex;"><span>        <span style="color:#1f2328">),</span>
</span></span><span style="display:flex;"><span>        transforms<span style="color:#0550ae">.</span>RandomGrayscale<span style="color:#1f2328">(),</span>
</span></span><span style="display:flex;"><span>        transforms<span style="color:#0550ae">.</span>RandomEqualize<span style="color:#1f2328">(),</span>
</span></span><span style="display:flex;"><span>        transforms<span style="color:#0550ae">.</span>RandomPosterize<span style="color:#1f2328">(</span>bits<span style="color:#0550ae">=</span><span style="color:#0550ae">3</span><span style="color:#1f2328">,</span> p<span style="color:#0550ae">=</span><span style="color:#0550ae">0.5</span><span style="color:#1f2328">),</span>
</span></span><span style="display:flex;"><span>        transforms<span style="color:#0550ae">.</span>RandomHorizontalFlip<span style="color:#1f2328">(</span>p<span style="color:#0550ae">=</span><span style="color:#0550ae">0.5</span><span style="color:#1f2328">),</span>
</span></span><span style="display:flex;"><span>    <span style="color:#1f2328">],</span>
</span></span><span style="display:flex;"><span><span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Compose transforms to resize and pad input images</span>
</span></span><span style="display:flex;"><span>resize_pad_tfm <span style="color:#0550ae">=</span> transforms<span style="color:#0550ae">.</span>Compose<span style="color:#1f2328">([</span>
</span></span><span style="display:flex;"><span>    resize_max<span style="color:#1f2328">,</span> 
</span></span><span style="display:flex;"><span>    pad_square<span style="color:#1f2328">,</span>
</span></span><span style="display:flex;"><span>    transforms<span style="color:#0550ae">.</span>Resize<span style="color:#1f2328">([</span>train_sz<span style="color:#1f2328">]</span> <span style="color:#0550ae">*</span> <span style="color:#0550ae">2</span><span style="color:#1f2328">,</span> antialias<span style="color:#0550ae">=</span><span style="color:#cf222e">True</span><span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span><span style="color:#1f2328">])</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Compose transforms to sanitize bounding boxes and normalize input data</span>
</span></span><span style="display:flex;"><span>final_tfms <span style="color:#0550ae">=</span> transforms<span style="color:#0550ae">.</span>Compose<span style="color:#1f2328">([</span>
</span></span><span style="display:flex;"><span>    transforms<span style="color:#0550ae">.</span>ToImage<span style="color:#1f2328">(),</span> 
</span></span><span style="display:flex;"><span>    transforms<span style="color:#0550ae">.</span>ToDtype<span style="color:#1f2328">(</span>torch<span style="color:#0550ae">.</span>float32<span style="color:#1f2328">,</span> scale<span style="color:#0550ae">=</span><span style="color:#cf222e">True</span><span style="color:#1f2328">),</span>
</span></span><span style="display:flex;"><span>    transforms<span style="color:#0550ae">.</span>SanitizeBoundingBoxes<span style="color:#1f2328">(),</span>
</span></span><span style="display:flex;"><span><span style="color:#1f2328">])</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Define the transformations for training and validation datasets</span>
</span></span><span style="display:flex;"><span>train_tfms <span style="color:#0550ae">=</span> transforms<span style="color:#0550ae">.</span>Compose<span style="color:#1f2328">([</span>
</span></span><span style="display:flex;"><span>    data_aug_tfms<span style="color:#1f2328">,</span> 
</span></span><span style="display:flex;"><span>    resize_pad_tfm<span style="color:#1f2328">,</span> 
</span></span><span style="display:flex;"><span>    final_tfms
</span></span><span style="display:flex;"><span><span style="color:#1f2328">])</span>
</span></span><span style="display:flex;"><span>valid_tfms <span style="color:#0550ae">=</span> transforms<span style="color:#0550ae">.</span>Compose<span style="color:#1f2328">([</span>resize_pad_tfm<span style="color:#1f2328">,</span> final_tfms<span style="color:#1f2328">])</span>
</span></span></code></pre></div><p>We do not need to include a <code>Normalize</code> transform as the model internally normalizes input.</p>




<h3 id="initialize-datasets">Initialize Datasets
  <a href="#initialize-datasets"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h3>
<p>Now, we can create our training and validation dataset objects using the dataset splits and transforms.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#57606a"># Create a mapping from class names to class indices</span>
</span></span><span style="display:flex;"><span>class_to_idx <span style="color:#0550ae">=</span> <span style="color:#1f2328">{</span>c<span style="color:#1f2328">:</span> i <span style="color:#cf222e">for</span> i<span style="color:#1f2328">,</span> c <span style="color:#0550ae">in</span> <span style="color:#6639ba">enumerate</span><span style="color:#1f2328">(</span>class_names<span style="color:#1f2328">)}</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Instantiate the datasets using the defined transformations</span>
</span></span><span style="display:flex;"><span>train_dataset <span style="color:#0550ae">=</span> StudentIDDataset<span style="color:#1f2328">(</span>train_keys<span style="color:#1f2328">,</span> annotation_df<span style="color:#1f2328">,</span> img_dict<span style="color:#1f2328">,</span> class_to_idx<span style="color:#1f2328">,</span> train_tfms<span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>valid_dataset <span style="color:#0550ae">=</span> StudentIDDataset<span style="color:#1f2328">(</span>val_keys<span style="color:#1f2328">,</span> annotation_df<span style="color:#1f2328">,</span> img_dict<span style="color:#1f2328">,</span> class_to_idx<span style="color:#1f2328">,</span> valid_tfms<span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Print the number of samples in the training and validation datasets</span>
</span></span><span style="display:flex;"><span>pd<span style="color:#0550ae">.</span>Series<span style="color:#1f2328">({</span>
</span></span><span style="display:flex;"><span>    <span style="color:#0a3069">&#39;Training dataset size:&#39;</span><span style="color:#1f2328">:</span> <span style="color:#6639ba">len</span><span style="color:#1f2328">(</span>train_dataset<span style="color:#1f2328">),</span>
</span></span><span style="display:flex;"><span>    <span style="color:#0a3069">&#39;Validation dataset size:&#39;</span><span style="color:#1f2328">:</span> <span style="color:#6639ba">len</span><span style="color:#1f2328">(</span>valid_dataset<span style="color:#1f2328">)}</span>
</span></span><span style="display:flex;"><span><span style="color:#1f2328">)</span><span style="color:#0550ae">.</span>to_frame<span style="color:#1f2328">()</span><span style="color:#0550ae">.</span>style<span style="color:#0550ae">.</span>hide<span style="color:#1f2328">(</span>axis<span style="color:#0550ae">=</span><span style="color:#0a3069">&#39;columns&#39;</span><span style="color:#1f2328">)</span>
</span></span></code></pre></div><div style="overflow-x:auto; max-height:500px">
<table id="T_71879">
  <thead>
  </thead>
  <tbody>
    <tr>
      <th id="T_71879_level0_row0" class="row_heading level0 row0" >Training dataset size:</th>
      <td id="T_71879_row0_col0" class="data row0 col0" >120</td>
    </tr>
    <tr>
      <th id="T_71879_level0_row1" class="row_heading level0 row1" >Validation dataset size:</th>
      <td id="T_71879_row1_col0" class="data row1 col0" >30</td>
    </tr>
  </tbody>
</table>
</div>




<h3 id="inspect-samples">Inspect Samples
  <a href="#inspect-samples"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h3>
<p>Letâ€™s verify the dataset objects work correctly by inspecting the first samples from the training and validation sets.</p>




<h4 id="inspect-training-set-sample">Inspect training set sample
  <a href="#inspect-training-set-sample"></a>
</h4>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>dataset_sample <span style="color:#0550ae">=</span> train_dataset<span style="color:#1f2328">[</span><span style="color:#0550ae">0</span><span style="color:#1f2328">]</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Get colors for dataset sample</span>
</span></span><span style="display:flex;"><span>sample_colors <span style="color:#0550ae">=</span> <span style="color:#1f2328">[</span>int_colors<span style="color:#1f2328">[</span><span style="color:#6639ba">int</span><span style="color:#1f2328">(</span>i<span style="color:#0550ae">.</span>item<span style="color:#1f2328">())]</span> <span style="color:#cf222e">for</span> i <span style="color:#0550ae">in</span> dataset_sample<span style="color:#1f2328">[</span><span style="color:#0550ae">1</span><span style="color:#1f2328">][</span><span style="color:#0a3069">&#39;labels&#39;</span><span style="color:#1f2328">]]</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Annotate the sample image with segmentation masks</span>
</span></span><span style="display:flex;"><span>annotated_tensor <span style="color:#0550ae">=</span> draw_segmentation_masks<span style="color:#1f2328">(</span> 
</span></span><span style="display:flex;"><span>    image<span style="color:#0550ae">=</span><span style="color:#1f2328">(</span>dataset_sample<span style="color:#1f2328">[</span><span style="color:#0550ae">0</span><span style="color:#1f2328">]</span><span style="color:#0550ae">*</span><span style="color:#0550ae">255</span><span style="color:#1f2328">)</span><span style="color:#0550ae">.</span>to<span style="color:#1f2328">(</span>dtype<span style="color:#0550ae">=</span>torch<span style="color:#0550ae">.</span>uint8<span style="color:#1f2328">),</span> 
</span></span><span style="display:flex;"><span>    masks<span style="color:#0550ae">=</span>dataset_sample<span style="color:#1f2328">[</span><span style="color:#0550ae">1</span><span style="color:#1f2328">][</span><span style="color:#0a3069">&#39;masks&#39;</span><span style="color:#1f2328">],</span> 
</span></span><span style="display:flex;"><span>    alpha<span style="color:#0550ae">=</span><span style="color:#0550ae">0.3</span><span style="color:#1f2328">,</span> 
</span></span><span style="display:flex;"><span>    colors<span style="color:#0550ae">=</span>sample_colors
</span></span><span style="display:flex;"><span><span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Annotate the sample image with bounding boxes</span>
</span></span><span style="display:flex;"><span>annotated_tensor <span style="color:#0550ae">=</span> draw_bboxes<span style="color:#1f2328">(</span>
</span></span><span style="display:flex;"><span>    image<span style="color:#0550ae">=</span>annotated_tensor<span style="color:#1f2328">,</span> 
</span></span><span style="display:flex;"><span>    boxes<span style="color:#0550ae">=</span>dataset_sample<span style="color:#1f2328">[</span><span style="color:#0550ae">1</span><span style="color:#1f2328">][</span><span style="color:#0a3069">&#39;boxes&#39;</span><span style="color:#1f2328">],</span> 
</span></span><span style="display:flex;"><span>    labels<span style="color:#0550ae">=</span><span style="color:#1f2328">[</span>class_names<span style="color:#1f2328">[</span><span style="color:#6639ba">int</span><span style="color:#1f2328">(</span>i<span style="color:#0550ae">.</span>item<span style="color:#1f2328">())]</span> <span style="color:#cf222e">for</span> i <span style="color:#0550ae">in</span> dataset_sample<span style="color:#1f2328">[</span><span style="color:#0550ae">1</span><span style="color:#1f2328">][</span><span style="color:#0a3069">&#39;labels&#39;</span><span style="color:#1f2328">]],</span> 
</span></span><span style="display:flex;"><span>    colors<span style="color:#0550ae">=</span>sample_colors
</span></span><span style="display:flex;"><span><span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>tensor_to_pil<span style="color:#1f2328">(</span>annotated_tensor<span style="color:#1f2328">)</span>
</span></span></code></pre></div><p><img src="./images/output_69_0.png" alt="">{fig-align=&ldquo;center&rdquo;}</p>




<h4 id="inspect-validation-set-sample">Inspect validation set sample
  <a href="#inspect-validation-set-sample"></a>
</h4>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>dataset_sample <span style="color:#0550ae">=</span> valid_dataset<span style="color:#1f2328">[</span><span style="color:#0550ae">0</span><span style="color:#1f2328">]</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Get colors for dataset sample</span>
</span></span><span style="display:flex;"><span>sample_colors <span style="color:#0550ae">=</span> <span style="color:#1f2328">[</span>int_colors<span style="color:#1f2328">[</span><span style="color:#6639ba">int</span><span style="color:#1f2328">(</span>i<span style="color:#0550ae">.</span>item<span style="color:#1f2328">())]</span> <span style="color:#cf222e">for</span> i <span style="color:#0550ae">in</span> dataset_sample<span style="color:#1f2328">[</span><span style="color:#0550ae">1</span><span style="color:#1f2328">][</span><span style="color:#0a3069">&#39;labels&#39;</span><span style="color:#1f2328">]]</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Annotate the sample image with segmentation masks</span>
</span></span><span style="display:flex;"><span>annotated_tensor <span style="color:#0550ae">=</span> draw_segmentation_masks<span style="color:#1f2328">(</span>
</span></span><span style="display:flex;"><span>    image<span style="color:#0550ae">=</span><span style="color:#1f2328">(</span>dataset_sample<span style="color:#1f2328">[</span><span style="color:#0550ae">0</span><span style="color:#1f2328">]</span><span style="color:#0550ae">*</span><span style="color:#0550ae">255</span><span style="color:#1f2328">)</span><span style="color:#0550ae">.</span>to<span style="color:#1f2328">(</span>dtype<span style="color:#0550ae">=</span>torch<span style="color:#0550ae">.</span>uint8<span style="color:#1f2328">),</span>
</span></span><span style="display:flex;"><span>    masks<span style="color:#0550ae">=</span>dataset_sample<span style="color:#1f2328">[</span><span style="color:#0550ae">1</span><span style="color:#1f2328">][</span><span style="color:#0a3069">&#39;masks&#39;</span><span style="color:#1f2328">],</span> 
</span></span><span style="display:flex;"><span>    alpha<span style="color:#0550ae">=</span><span style="color:#0550ae">0.3</span><span style="color:#1f2328">,</span> 
</span></span><span style="display:flex;"><span>    colors<span style="color:#0550ae">=</span>sample_colors
</span></span><span style="display:flex;"><span><span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Annotate the sample image with bounding boxes</span>
</span></span><span style="display:flex;"><span>annotated_tensor <span style="color:#0550ae">=</span> draw_bboxes<span style="color:#1f2328">(</span>
</span></span><span style="display:flex;"><span>    image<span style="color:#0550ae">=</span>annotated_tensor<span style="color:#1f2328">,</span> 
</span></span><span style="display:flex;"><span>    boxes<span style="color:#0550ae">=</span>dataset_sample<span style="color:#1f2328">[</span><span style="color:#0550ae">1</span><span style="color:#1f2328">][</span><span style="color:#0a3069">&#39;boxes&#39;</span><span style="color:#1f2328">],</span> 
</span></span><span style="display:flex;"><span>    labels<span style="color:#0550ae">=</span><span style="color:#1f2328">[</span>class_names<span style="color:#1f2328">[</span><span style="color:#6639ba">int</span><span style="color:#1f2328">(</span>i<span style="color:#0550ae">.</span>item<span style="color:#1f2328">())]</span> <span style="color:#cf222e">for</span> i <span style="color:#0550ae">in</span> dataset_sample<span style="color:#1f2328">[</span><span style="color:#0550ae">1</span><span style="color:#1f2328">][</span><span style="color:#0a3069">&#39;labels&#39;</span><span style="color:#1f2328">]],</span> 
</span></span><span style="display:flex;"><span>    colors<span style="color:#0550ae">=</span>sample_colors
</span></span><span style="display:flex;"><span><span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>tensor_to_pil<span style="color:#1f2328">(</span>annotated_tensor<span style="color:#1f2328">)</span>
</span></span></code></pre></div><p><img src="./images/output_71_0.png" alt="">{fig-align=&ldquo;center&rdquo;}</p>




<h3 id="initialize-dataloaders">Initialize DataLoaders
  <a href="#initialize-dataloaders"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h3>
<p>The last step before training is to instantiate the DataLoaders for the training and validation sets. Try decreasing the batch size if you encounter memory limitations.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#57606a"># Set the training batch size</span>
</span></span><span style="display:flex;"><span>bs <span style="color:#0550ae">=</span> <span style="color:#0550ae">4</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Set the number of worker processes for loading data.</span>
</span></span><span style="display:flex;"><span>num_workers <span style="color:#0550ae">=</span> multiprocessing<span style="color:#0550ae">.</span>cpu_count<span style="color:#1f2328">()</span><span style="color:#0550ae">//</span><span style="color:#0550ae">2</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Define parameters for DataLoader</span>
</span></span><span style="display:flex;"><span>data_loader_params <span style="color:#0550ae">=</span> <span style="color:#1f2328">{</span>
</span></span><span style="display:flex;"><span>    <span style="color:#0a3069">&#39;batch_size&#39;</span><span style="color:#1f2328">:</span> bs<span style="color:#1f2328">,</span>  <span style="color:#57606a"># Batch size for data loading</span>
</span></span><span style="display:flex;"><span>    <span style="color:#0a3069">&#39;num_workers&#39;</span><span style="color:#1f2328">:</span> num_workers<span style="color:#1f2328">,</span>  <span style="color:#57606a"># Number of subprocesses to use for data loading</span>
</span></span><span style="display:flex;"><span>    <span style="color:#0a3069">&#39;persistent_workers&#39;</span><span style="color:#1f2328">:</span> <span style="color:#cf222e">True</span><span style="color:#1f2328">,</span>  <span style="color:#57606a"># If True, the data loader will not shutdown the worker processes after a dataset has been consumed once. This allows to maintain the worker dataset instances alive.</span>
</span></span><span style="display:flex;"><span>    <span style="color:#0a3069">&#39;pin_memory&#39;</span><span style="color:#1f2328">:</span> <span style="color:#0a3069">&#39;cuda&#39;</span> <span style="color:#0550ae">in</span> device<span style="color:#1f2328">,</span>  <span style="color:#57606a"># If True, the data loader will copy Tensors into CUDA pinned memory before returning them. Useful when using GPU.</span>
</span></span><span style="display:flex;"><span>    <span style="color:#0a3069">&#39;pin_memory_device&#39;</span><span style="color:#1f2328">:</span> device <span style="color:#cf222e">if</span> <span style="color:#0a3069">&#39;cuda&#39;</span> <span style="color:#0550ae">in</span> device <span style="color:#cf222e">else</span> <span style="color:#0a3069">&#39;&#39;</span><span style="color:#1f2328">,</span>  <span style="color:#57606a"># Specifies the device where the data should be loaded. Commonly set to use the GPU.</span>
</span></span><span style="display:flex;"><span>    <span style="color:#0a3069">&#39;collate_fn&#39;</span><span style="color:#1f2328">:</span> <span style="color:#cf222e">lambda</span> batch<span style="color:#1f2328">:</span> <span style="color:#6639ba">tuple</span><span style="color:#1f2328">(</span><span style="color:#6639ba">zip</span><span style="color:#1f2328">(</span><span style="color:#0550ae">*</span>batch<span style="color:#1f2328">)),</span>
</span></span><span style="display:flex;"><span><span style="color:#1f2328">}</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Create DataLoader for training data. Data is shuffled for every epoch.</span>
</span></span><span style="display:flex;"><span>train_dataloader <span style="color:#0550ae">=</span> DataLoader<span style="color:#1f2328">(</span>train_dataset<span style="color:#1f2328">,</span> <span style="color:#0550ae">**</span>data_loader_params<span style="color:#1f2328">,</span> shuffle<span style="color:#0550ae">=</span><span style="color:#cf222e">True</span><span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Create DataLoader for validation data. Shuffling is not necessary for validation data.</span>
</span></span><span style="display:flex;"><span>valid_dataloader <span style="color:#0550ae">=</span> DataLoader<span style="color:#1f2328">(</span>valid_dataset<span style="color:#1f2328">,</span> <span style="color:#0550ae">**</span>data_loader_params<span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Print the number of batches in the training and validation DataLoaders</span>
</span></span><span style="display:flex;"><span>pd<span style="color:#0550ae">.</span>Series<span style="color:#1f2328">({</span>
</span></span><span style="display:flex;"><span>    <span style="color:#0a3069">&#39;Number of batches in train DataLoader:&#39;</span><span style="color:#1f2328">:</span> <span style="color:#6639ba">len</span><span style="color:#1f2328">(</span>train_dataloader<span style="color:#1f2328">),</span>
</span></span><span style="display:flex;"><span>    <span style="color:#0a3069">&#39;Number of batches in validation DataLoader:&#39;</span><span style="color:#1f2328">:</span> <span style="color:#6639ba">len</span><span style="color:#1f2328">(</span>valid_dataloader<span style="color:#1f2328">)}</span>
</span></span><span style="display:flex;"><span><span style="color:#1f2328">)</span><span style="color:#0550ae">.</span>to_frame<span style="color:#1f2328">()</span><span style="color:#0550ae">.</span>style<span style="color:#0550ae">.</span>hide<span style="color:#1f2328">(</span>axis<span style="color:#0550ae">=</span><span style="color:#0a3069">&#39;columns&#39;</span><span style="color:#1f2328">)</span>
</span></span></code></pre></div><div style="overflow-x:auto; max-height:500px">
<table id="T_15fe8">
  <thead>
  </thead>
  <tbody>
    <tr>
      <th id="T_15fe8_level0_row0" class="row_heading level0 row0" >Number of batches in train DataLoader:</th>
      <td id="T_15fe8_row0_col0" class="data row0 col0" >30</td>
    </tr>
    <tr>
      <th id="T_15fe8_level0_row1" class="row_heading level0 row1" >Number of batches in validation DataLoader:</th>
      <td id="T_15fe8_row1_col0" class="data row1 col0" >8</td>
    </tr>
  </tbody>
</table>
</div>
That completes the data preparation. Now, we can finally train our Mask R-CNN model.




<h2 id="fine-tuning-the-model">Fine-tuning the Model
  <a href="#fine-tuning-the-model"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h2>
<p>In this section, we will implement the training code and fine-tune our model.</p>




<h3 id="define-the-training-loop">Define the Training Loop
  <a href="#define-the-training-loop"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h3>
<p>The following function performs a single pass through the training or validation set.</p>
<p>The model has different behavior when in <code>training</code> mode versus <code>evaluation</code> mode. In training mode, it calculates the loss internally for the object detection and segmentation tasks and returns a dictionary with the individual loss values. We can sum up these separate values to get the total loss.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#cf222e">def</span> <span style="color:#6639ba">run_epoch</span><span style="color:#1f2328">(</span>model<span style="color:#1f2328">,</span> dataloader<span style="color:#1f2328">,</span> optimizer<span style="color:#1f2328">,</span> lr_scheduler<span style="color:#1f2328">,</span> device<span style="color:#1f2328">,</span> scaler<span style="color:#1f2328">,</span> epoch_id<span style="color:#1f2328">,</span> is_training<span style="color:#1f2328">):</span>
</span></span><span style="display:flex;"><span>    <span style="color:#0a3069">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">    Function to run a single training or evaluation epoch.
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">    
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">    Args:
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">        model: A PyTorch model to train or evaluate.
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">        dataloader: A PyTorch DataLoader providing the data.
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">        optimizer: The optimizer to use for training the model.
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">        loss_func: The loss function used for training.
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">        device: The device (CPU or GPU) to run the model on.
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">        scaler: Gradient scaler for mixed-precision training.
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">        is_training: Boolean flag indicating whether the model is in training or evaluation mode.
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">    
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">    Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">        The average loss for the epoch.
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#57606a"># Set the model to training mode</span>
</span></span><span style="display:flex;"><span>    model<span style="color:#0550ae">.</span>train<span style="color:#1f2328">()</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    epoch_loss <span style="color:#0550ae">=</span> <span style="color:#0550ae">0</span>  <span style="color:#57606a"># Initialize the total loss for this epoch</span>
</span></span><span style="display:flex;"><span>    progress_bar <span style="color:#0550ae">=</span> tqdm<span style="color:#1f2328">(</span>total<span style="color:#0550ae">=</span><span style="color:#6639ba">len</span><span style="color:#1f2328">(</span>dataloader<span style="color:#1f2328">),</span> desc<span style="color:#0550ae">=</span><span style="color:#0a3069">&#34;Train&#34;</span> <span style="color:#cf222e">if</span> is_training <span style="color:#cf222e">else</span> <span style="color:#0a3069">&#34;Eval&#34;</span><span style="color:#1f2328">)</span>  <span style="color:#57606a"># Initialize a progress bar</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#57606a"># Loop over the data</span>
</span></span><span style="display:flex;"><span>    <span style="color:#cf222e">for</span> batch_id<span style="color:#1f2328">,</span> <span style="color:#1f2328">(</span>inputs<span style="color:#1f2328">,</span> targets<span style="color:#1f2328">)</span> <span style="color:#0550ae">in</span> <span style="color:#6639ba">enumerate</span><span style="color:#1f2328">(</span>dataloader<span style="color:#1f2328">):</span>
</span></span><span style="display:flex;"><span>        <span style="color:#57606a"># Move inputs and targets to the specified device</span>
</span></span><span style="display:flex;"><span>        inputs <span style="color:#0550ae">=</span> torch<span style="color:#0550ae">.</span>stack<span style="color:#1f2328">(</span>inputs<span style="color:#1f2328">)</span><span style="color:#0550ae">.</span>to<span style="color:#1f2328">(</span>device<span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#57606a"># Forward pass with Automatic Mixed Precision (AMP) context manager</span>
</span></span><span style="display:flex;"><span>        <span style="color:#cf222e">with</span> autocast<span style="color:#1f2328">(</span>torch<span style="color:#0550ae">.</span>device<span style="color:#1f2328">(</span>device<span style="color:#1f2328">)</span><span style="color:#0550ae">.</span>type<span style="color:#1f2328">):</span>
</span></span><span style="display:flex;"><span>            <span style="color:#cf222e">if</span> is_training<span style="color:#1f2328">:</span>
</span></span><span style="display:flex;"><span>                losses <span style="color:#0550ae">=</span> model<span style="color:#1f2328">(</span>inputs<span style="color:#0550ae">.</span>to<span style="color:#1f2328">(</span>device<span style="color:#1f2328">),</span> move_data_to_device<span style="color:#1f2328">(</span>targets<span style="color:#1f2328">,</span> device<span style="color:#1f2328">))</span>
</span></span><span style="display:flex;"><span>            <span style="color:#cf222e">else</span><span style="color:#1f2328">:</span>
</span></span><span style="display:flex;"><span>                <span style="color:#cf222e">with</span> torch<span style="color:#0550ae">.</span>no_grad<span style="color:#1f2328">():</span>
</span></span><span style="display:flex;"><span>                    losses <span style="color:#0550ae">=</span> model<span style="color:#1f2328">(</span>inputs<span style="color:#0550ae">.</span>to<span style="color:#1f2328">(</span>device<span style="color:#1f2328">),</span> move_data_to_device<span style="color:#1f2328">(</span>targets<span style="color:#1f2328">,</span> device<span style="color:#1f2328">))</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>            <span style="color:#57606a"># Compute the loss</span>
</span></span><span style="display:flex;"><span>            loss <span style="color:#0550ae">=</span> <span style="color:#6639ba">sum</span><span style="color:#1f2328">([</span>loss <span style="color:#cf222e">for</span> loss <span style="color:#0550ae">in</span> losses<span style="color:#0550ae">.</span>values<span style="color:#1f2328">()])</span>  <span style="color:#57606a"># Sum up the losses</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#57606a"># If in training mode, backpropagate the error and update the weights</span>
</span></span><span style="display:flex;"><span>        <span style="color:#cf222e">if</span> is_training<span style="color:#1f2328">:</span>
</span></span><span style="display:flex;"><span>            <span style="color:#cf222e">if</span> scaler<span style="color:#1f2328">:</span>
</span></span><span style="display:flex;"><span>                scaler<span style="color:#0550ae">.</span>scale<span style="color:#1f2328">(</span>loss<span style="color:#1f2328">)</span><span style="color:#0550ae">.</span>backward<span style="color:#1f2328">()</span>
</span></span><span style="display:flex;"><span>                scaler<span style="color:#0550ae">.</span>step<span style="color:#1f2328">(</span>optimizer<span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>                old_scaler <span style="color:#0550ae">=</span> scaler<span style="color:#0550ae">.</span>get_scale<span style="color:#1f2328">()</span>
</span></span><span style="display:flex;"><span>                scaler<span style="color:#0550ae">.</span>update<span style="color:#1f2328">()</span>
</span></span><span style="display:flex;"><span>                new_scaler <span style="color:#0550ae">=</span> scaler<span style="color:#0550ae">.</span>get_scale<span style="color:#1f2328">()</span>
</span></span><span style="display:flex;"><span>                <span style="color:#cf222e">if</span> new_scaler <span style="color:#0550ae">&gt;=</span> old_scaler<span style="color:#1f2328">:</span>
</span></span><span style="display:flex;"><span>                    lr_scheduler<span style="color:#0550ae">.</span>step<span style="color:#1f2328">()</span>
</span></span><span style="display:flex;"><span>            <span style="color:#cf222e">else</span><span style="color:#1f2328">:</span>
</span></span><span style="display:flex;"><span>                loss<span style="color:#0550ae">.</span>backward<span style="color:#1f2328">()</span>
</span></span><span style="display:flex;"><span>                optimizer<span style="color:#0550ae">.</span>step<span style="color:#1f2328">()</span>
</span></span><span style="display:flex;"><span>                lr_scheduler<span style="color:#0550ae">.</span>step<span style="color:#1f2328">()</span>
</span></span><span style="display:flex;"><span>                
</span></span><span style="display:flex;"><span>            optimizer<span style="color:#0550ae">.</span>zero_grad<span style="color:#1f2328">()</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#57606a"># Update the total loss</span>
</span></span><span style="display:flex;"><span>        loss_item <span style="color:#0550ae">=</span> loss<span style="color:#0550ae">.</span>item<span style="color:#1f2328">()</span>
</span></span><span style="display:flex;"><span>        epoch_loss <span style="color:#0550ae">+=</span> loss_item
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#57606a"># Update the progress bar</span>
</span></span><span style="display:flex;"><span>        progress_bar_dict <span style="color:#0550ae">=</span> <span style="color:#6639ba">dict</span><span style="color:#1f2328">(</span>loss<span style="color:#0550ae">=</span>loss_item<span style="color:#1f2328">,</span> avg_loss<span style="color:#0550ae">=</span>epoch_loss<span style="color:#0550ae">/</span><span style="color:#1f2328">(</span>batch_id<span style="color:#0550ae">+</span><span style="color:#0550ae">1</span><span style="color:#1f2328">))</span>
</span></span><span style="display:flex;"><span>        <span style="color:#cf222e">if</span> is_training<span style="color:#1f2328">:</span>
</span></span><span style="display:flex;"><span>            progress_bar_dict<span style="color:#0550ae">.</span>update<span style="color:#1f2328">(</span>lr<span style="color:#0550ae">=</span>lr_scheduler<span style="color:#0550ae">.</span>get_last_lr<span style="color:#1f2328">()[</span><span style="color:#0550ae">0</span><span style="color:#1f2328">])</span>
</span></span><span style="display:flex;"><span>        progress_bar<span style="color:#0550ae">.</span>set_postfix<span style="color:#1f2328">(</span>progress_bar_dict<span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>        progress_bar<span style="color:#0550ae">.</span>update<span style="color:#1f2328">()</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#57606a"># If loss is NaN or infinity, stop training</span>
</span></span><span style="display:flex;"><span>        <span style="color:#cf222e">if</span> is_training<span style="color:#1f2328">:</span>
</span></span><span style="display:flex;"><span>            stop_training_message <span style="color:#0550ae">=</span> <span style="color:#0a3069">f</span><span style="color:#0a3069">&#34;Loss is NaN or infinite at epoch </span><span style="color:#0a3069">{</span>epoch_id<span style="color:#0a3069">}</span><span style="color:#0a3069">, batch </span><span style="color:#0a3069">{</span>batch_id<span style="color:#0a3069">}</span><span style="color:#0a3069">. Stopping training.&#34;</span>
</span></span><span style="display:flex;"><span>            <span style="color:#cf222e">assert</span> <span style="color:#0550ae">not</span> math<span style="color:#0550ae">.</span>isnan<span style="color:#1f2328">(</span>loss_item<span style="color:#1f2328">)</span> <span style="color:#0550ae">and</span> math<span style="color:#0550ae">.</span>isfinite<span style="color:#1f2328">(</span>loss_item<span style="color:#1f2328">),</span> stop_training_message
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#57606a"># Cleanup and close the progress bar </span>
</span></span><span style="display:flex;"><span>    progress_bar<span style="color:#0550ae">.</span>close<span style="color:#1f2328">()</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#57606a"># Return the average loss for this epoch</span>
</span></span><span style="display:flex;"><span>    <span style="color:#cf222e">return</span> epoch_loss <span style="color:#0550ae">/</span> <span style="color:#1f2328">(</span>batch_id <span style="color:#0550ae">+</span> <span style="color:#0550ae">1</span><span style="color:#1f2328">)</span>
</span></span></code></pre></div><p>Next, we define the <code>train_loop</code> function, which executes the main training loop. It iterates over each epoch, runs through the training and validation sets, and saves the best model based on the validation loss.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#cf222e">def</span> <span style="color:#6639ba">train_loop</span><span style="color:#1f2328">(</span>model<span style="color:#1f2328">,</span> 
</span></span><span style="display:flex;"><span>               train_dataloader<span style="color:#1f2328">,</span> 
</span></span><span style="display:flex;"><span>               valid_dataloader<span style="color:#1f2328">,</span> 
</span></span><span style="display:flex;"><span>               optimizer<span style="color:#1f2328">,</span>  
</span></span><span style="display:flex;"><span>               lr_scheduler<span style="color:#1f2328">,</span> 
</span></span><span style="display:flex;"><span>               device<span style="color:#1f2328">,</span> 
</span></span><span style="display:flex;"><span>               epochs<span style="color:#1f2328">,</span> 
</span></span><span style="display:flex;"><span>               checkpoint_path<span style="color:#1f2328">,</span> 
</span></span><span style="display:flex;"><span>               use_scaler<span style="color:#0550ae">=</span><span style="color:#cf222e">False</span><span style="color:#1f2328">):</span>
</span></span><span style="display:flex;"><span>    <span style="color:#0a3069">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">    Main training loop.
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">    
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">    Args:
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">        model: A PyTorch model to train.
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">        train_dataloader: A PyTorch DataLoader providing the training data.
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">        valid_dataloader: A PyTorch DataLoader providing the validation data.
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">        optimizer: The optimizer to use for training the model.
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">        lr_scheduler: The learning rate scheduler.
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">        device: The device (CPU or GPU) to run the model on.
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">        epochs: The number of epochs to train for.
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">        checkpoint_path: The path where to save the best model checkpoint.
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">        use_scaler: Whether to scale graidents when using a CUDA device
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">    
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">    Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">        None
</span></span></span><span style="display:flex;"><span><span style="color:#0a3069">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#57606a"># Initialize a gradient scaler for mixed-precision training if the device is a CUDA GPU</span>
</span></span><span style="display:flex;"><span>    scaler <span style="color:#0550ae">=</span> torch<span style="color:#0550ae">.</span>cuda<span style="color:#0550ae">.</span>amp<span style="color:#0550ae">.</span>GradScaler<span style="color:#1f2328">()</span> <span style="color:#cf222e">if</span> device<span style="color:#0550ae">.</span>type <span style="color:#0550ae">==</span> <span style="color:#0a3069">&#39;cuda&#39;</span> <span style="color:#0550ae">and</span> use_scaler <span style="color:#cf222e">else</span> <span style="color:#cf222e">None</span>
</span></span><span style="display:flex;"><span>    best_loss <span style="color:#0550ae">=</span> <span style="color:#6639ba">float</span><span style="color:#1f2328">(</span><span style="color:#0a3069">&#39;inf&#39;</span><span style="color:#1f2328">)</span>  <span style="color:#57606a"># Initialize the best validation loss</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#57606a"># Loop over the epochs</span>
</span></span><span style="display:flex;"><span>    <span style="color:#cf222e">for</span> epoch <span style="color:#0550ae">in</span> tqdm<span style="color:#1f2328">(</span><span style="color:#6639ba">range</span><span style="color:#1f2328">(</span>epochs<span style="color:#1f2328">),</span> desc<span style="color:#0550ae">=</span><span style="color:#0a3069">&#34;Epochs&#34;</span><span style="color:#1f2328">):</span>
</span></span><span style="display:flex;"><span>        <span style="color:#57606a"># Run a training epoch and get the training loss</span>
</span></span><span style="display:flex;"><span>        train_loss <span style="color:#0550ae">=</span> run_epoch<span style="color:#1f2328">(</span>model<span style="color:#1f2328">,</span> train_dataloader<span style="color:#1f2328">,</span> optimizer<span style="color:#1f2328">,</span> lr_scheduler<span style="color:#1f2328">,</span> device<span style="color:#1f2328">,</span> scaler<span style="color:#1f2328">,</span> epoch<span style="color:#1f2328">,</span> is_training<span style="color:#0550ae">=</span><span style="color:#cf222e">True</span><span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#57606a"># Run an evaluation epoch and get the validation loss</span>
</span></span><span style="display:flex;"><span>        <span style="color:#cf222e">with</span> torch<span style="color:#0550ae">.</span>no_grad<span style="color:#1f2328">():</span>
</span></span><span style="display:flex;"><span>            valid_loss <span style="color:#0550ae">=</span> run_epoch<span style="color:#1f2328">(</span>model<span style="color:#1f2328">,</span> valid_dataloader<span style="color:#1f2328">,</span> <span style="color:#cf222e">None</span><span style="color:#1f2328">,</span> <span style="color:#cf222e">None</span><span style="color:#1f2328">,</span> device<span style="color:#1f2328">,</span> scaler<span style="color:#1f2328">,</span> epoch<span style="color:#1f2328">,</span> is_training<span style="color:#0550ae">=</span><span style="color:#cf222e">False</span><span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#57606a"># If the validation loss is lower than the best validation loss seen so far, save the model checkpoint</span>
</span></span><span style="display:flex;"><span>        <span style="color:#cf222e">if</span> valid_loss <span style="color:#0550ae">&lt;</span> best_loss<span style="color:#1f2328">:</span>
</span></span><span style="display:flex;"><span>            best_loss <span style="color:#0550ae">=</span> valid_loss
</span></span><span style="display:flex;"><span>            torch<span style="color:#0550ae">.</span>save<span style="color:#1f2328">(</span>model<span style="color:#0550ae">.</span>state_dict<span style="color:#1f2328">(),</span> checkpoint_path<span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#57606a"># Save metadata about the training process</span>
</span></span><span style="display:flex;"><span>            training_metadata <span style="color:#0550ae">=</span> <span style="color:#1f2328">{</span>
</span></span><span style="display:flex;"><span>                <span style="color:#0a3069">&#39;epoch&#39;</span><span style="color:#1f2328">:</span> epoch<span style="color:#1f2328">,</span>
</span></span><span style="display:flex;"><span>                <span style="color:#0a3069">&#39;train_loss&#39;</span><span style="color:#1f2328">:</span> train_loss<span style="color:#1f2328">,</span>
</span></span><span style="display:flex;"><span>                <span style="color:#0a3069">&#39;valid_loss&#39;</span><span style="color:#1f2328">:</span> valid_loss<span style="color:#1f2328">,</span> 
</span></span><span style="display:flex;"><span>                <span style="color:#0a3069">&#39;learning_rate&#39;</span><span style="color:#1f2328">:</span> lr_scheduler<span style="color:#0550ae">.</span>get_last_lr<span style="color:#1f2328">()[</span><span style="color:#0550ae">0</span><span style="color:#1f2328">],</span>
</span></span><span style="display:flex;"><span>                <span style="color:#0a3069">&#39;model_architecture&#39;</span><span style="color:#1f2328">:</span> model<span style="color:#0550ae">.</span>name
</span></span><span style="display:flex;"><span>            <span style="color:#1f2328">}</span>
</span></span><span style="display:flex;"><span>            <span style="color:#cf222e">with</span> <span style="color:#6639ba">open</span><span style="color:#1f2328">(</span>Path<span style="color:#1f2328">(</span>checkpoint_path<span style="color:#0550ae">.</span>parent<span style="color:#0550ae">/</span><span style="color:#0a3069">&#39;training_metadata.json&#39;</span><span style="color:#1f2328">),</span> <span style="color:#0a3069">&#39;w&#39;</span><span style="color:#1f2328">)</span> <span style="color:#cf222e">as</span> f<span style="color:#1f2328">:</span>
</span></span><span style="display:flex;"><span>                json<span style="color:#0550ae">.</span>dump<span style="color:#1f2328">(</span>training_metadata<span style="color:#1f2328">,</span> f<span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#57606a"># If the device is a GPU, empty the cache</span>
</span></span><span style="display:flex;"><span>    <span style="color:#cf222e">if</span> device<span style="color:#0550ae">.</span>type <span style="color:#0550ae">!=</span> <span style="color:#0a3069">&#39;cpu&#39;</span><span style="color:#1f2328">:</span>
</span></span><span style="display:flex;"><span>        <span style="color:#6639ba">getattr</span><span style="color:#1f2328">(</span>torch<span style="color:#1f2328">,</span> device<span style="color:#0550ae">.</span>type<span style="color:#1f2328">)</span><span style="color:#0550ae">.</span>empty_cache<span style="color:#1f2328">()</span>
</span></span></code></pre></div>



<h3 id="set-the-model-checkpoint-path">Set the Model Checkpoint Path
  <a href="#set-the-model-checkpoint-path"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h3>
<p>Before we proceed with training, letâ€™s generate a timestamp for the training session and create a directory to save the checkpoints during training.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#57606a"># Generate timestamp for the training session (Year-Month-Day_Hour_Minute_Second)</span>
</span></span><span style="display:flex;"><span>timestamp <span style="color:#0550ae">=</span> datetime<span style="color:#0550ae">.</span>datetime<span style="color:#0550ae">.</span>now<span style="color:#1f2328">()</span><span style="color:#0550ae">.</span>strftime<span style="color:#1f2328">(</span><span style="color:#0a3069">&#34;%Y-%m-</span><span style="color:#0a3069">%d</span><span style="color:#0a3069">_%H-%M-%S&#34;</span><span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Create a directory to store the checkpoints if it does not already exist</span>
</span></span><span style="display:flex;"><span>checkpoint_dir <span style="color:#0550ae">=</span> Path<span style="color:#1f2328">(</span>project_dir<span style="color:#0550ae">/</span><span style="color:#0a3069">f</span><span style="color:#0a3069">&#34;</span><span style="color:#0a3069">{</span>timestamp<span style="color:#0a3069">}</span><span style="color:#0a3069">&#34;</span><span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Create the checkpoint directory if it does not already exist</span>
</span></span><span style="display:flex;"><span>checkpoint_dir<span style="color:#0550ae">.</span>mkdir<span style="color:#1f2328">(</span>parents<span style="color:#0550ae">=</span><span style="color:#cf222e">True</span><span style="color:#1f2328">,</span> exist_ok<span style="color:#0550ae">=</span><span style="color:#cf222e">True</span><span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># The model checkpoint path</span>
</span></span><span style="display:flex;"><span>checkpoint_path <span style="color:#0550ae">=</span> checkpoint_dir<span style="color:#0550ae">/</span><span style="color:#0a3069">f</span><span style="color:#0a3069">&#34;</span><span style="color:#0a3069">{</span>model<span style="color:#0550ae">.</span>name<span style="color:#0a3069">}</span><span style="color:#0a3069">.pth&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6639ba">print</span><span style="color:#1f2328">(</span>checkpoint_path<span style="color:#1f2328">)</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>pytorch-mask-r-cnn-instance-segmentation/2023-09-19_15-17-57/maskrcnn_resnet50_fpn_v2.pth
</span></span></code></pre></div><p>Letâ€™s also save a copy of the colormap for the current dataset in the training folder for future use.</p>




<h3 id="save-the-color-map">Save the Color Map
  <a href="#save-the-color-map"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h3>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#57606a"># Create a color map and write it to a JSON file</span>
</span></span><span style="display:flex;"><span>color_map <span style="color:#0550ae">=</span> <span style="color:#1f2328">{</span><span style="color:#0a3069">&#39;items&#39;</span><span style="color:#1f2328">:</span> <span style="color:#1f2328">[{</span><span style="color:#0a3069">&#39;label&#39;</span><span style="color:#1f2328">:</span> label<span style="color:#1f2328">,</span> <span style="color:#0a3069">&#39;color&#39;</span><span style="color:#1f2328">:</span> color<span style="color:#1f2328">}</span> <span style="color:#cf222e">for</span> label<span style="color:#1f2328">,</span> color <span style="color:#0550ae">in</span> <span style="color:#6639ba">zip</span><span style="color:#1f2328">(</span>class_names<span style="color:#1f2328">,</span> colors<span style="color:#1f2328">)]}</span>
</span></span><span style="display:flex;"><span><span style="color:#cf222e">with</span> <span style="color:#6639ba">open</span><span style="color:#1f2328">(</span><span style="color:#0a3069">f</span><span style="color:#0a3069">&#34;</span><span style="color:#0a3069">{</span>checkpoint_dir<span style="color:#0a3069">}</span><span style="color:#0a3069">/</span><span style="color:#0a3069">{</span>dataset_path<span style="color:#0550ae">.</span>name<span style="color:#0a3069">}</span><span style="color:#0a3069">-colormap.json&#34;</span><span style="color:#1f2328">,</span> <span style="color:#0a3069">&#34;w&#34;</span><span style="color:#1f2328">)</span> <span style="color:#cf222e">as</span> file<span style="color:#1f2328">:</span>
</span></span><span style="display:flex;"><span>    json<span style="color:#0550ae">.</span>dump<span style="color:#1f2328">(</span>color_map<span style="color:#1f2328">,</span> file<span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Print the name of the file that the color map was written to</span>
</span></span><span style="display:flex;"><span><span style="color:#6639ba">print</span><span style="color:#1f2328">(</span><span style="color:#0a3069">f</span><span style="color:#0a3069">&#34;</span><span style="color:#0a3069">{</span>checkpoint_dir<span style="color:#0a3069">}</span><span style="color:#0a3069">/</span><span style="color:#0a3069">{</span>dataset_path<span style="color:#0550ae">.</span>name<span style="color:#0a3069">}</span><span style="color:#0a3069">-colormap.json&#34;</span><span style="color:#1f2328">)</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>pytorch-mask-r-cnn-instance-segmentation/2023-09-19_15-17-57/student-id-colormap.json
</span></span></code></pre></div>



<h3 id="configure-the-training-parameters">Configure the Training Parameters
  <a href="#configure-the-training-parameters"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h3>
<p>Now, we can configure the parameters for training. We must specify the learning rate and number of training epochs. We will also instantiate the optimizer and learning rate scheduler.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#57606a"># Learning rate for the model</span>
</span></span><span style="display:flex;"><span>lr <span style="color:#0550ae">=</span> <span style="color:#0550ae">5e-4</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Number of training epochs</span>
</span></span><span style="display:flex;"><span>epochs <span style="color:#0550ae">=</span> <span style="color:#0550ae">40</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># AdamW optimizer; includes weight decay for regularization</span>
</span></span><span style="display:flex;"><span>optimizer <span style="color:#0550ae">=</span> torch<span style="color:#0550ae">.</span>optim<span style="color:#0550ae">.</span>AdamW<span style="color:#1f2328">(</span>model<span style="color:#0550ae">.</span>parameters<span style="color:#1f2328">(),</span> lr<span style="color:#0550ae">=</span>lr<span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Learning rate scheduler; adjusts the learning rate during training</span>
</span></span><span style="display:flex;"><span>lr_scheduler <span style="color:#0550ae">=</span> torch<span style="color:#0550ae">.</span>optim<span style="color:#0550ae">.</span>lr_scheduler<span style="color:#0550ae">.</span>OneCycleLR<span style="color:#1f2328">(</span>optimizer<span style="color:#1f2328">,</span> 
</span></span><span style="display:flex;"><span>                                                   max_lr<span style="color:#0550ae">=</span>lr<span style="color:#1f2328">,</span> 
</span></span><span style="display:flex;"><span>                                                   total_steps<span style="color:#0550ae">=</span>epochs<span style="color:#0550ae">*</span><span style="color:#6639ba">len</span><span style="color:#1f2328">(</span>train_dataloader<span style="color:#1f2328">))</span>
</span></span></code></pre></div>



<h3 id="train-the-model">Train the Model
  <a href="#train-the-model"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h3>
<p>Finally, we can train the model using the <code>train_loop</code> function. Training time will depend on the available hardware.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>train_loop<span style="color:#1f2328">(</span>model<span style="color:#0550ae">=</span>model<span style="color:#1f2328">,</span> 
</span></span><span style="display:flex;"><span>           train_dataloader<span style="color:#0550ae">=</span>train_dataloader<span style="color:#1f2328">,</span>
</span></span><span style="display:flex;"><span>           valid_dataloader<span style="color:#0550ae">=</span>valid_dataloader<span style="color:#1f2328">,</span>
</span></span><span style="display:flex;"><span>           optimizer<span style="color:#0550ae">=</span>optimizer<span style="color:#1f2328">,</span> 
</span></span><span style="display:flex;"><span>           lr_scheduler<span style="color:#0550ae">=</span>lr_scheduler<span style="color:#1f2328">,</span> 
</span></span><span style="display:flex;"><span>           device<span style="color:#0550ae">=</span>torch<span style="color:#0550ae">.</span>device<span style="color:#1f2328">(</span>device<span style="color:#1f2328">),</span> 
</span></span><span style="display:flex;"><span>           epochs<span style="color:#0550ae">=</span>epochs<span style="color:#1f2328">,</span> 
</span></span><span style="display:flex;"><span>           checkpoint_path<span style="color:#0550ae">=</span>checkpoint_path<span style="color:#1f2328">,</span>
</span></span><span style="display:flex;"><span>           use_scaler<span style="color:#0550ae">=</span><span style="color:#cf222e">True</span><span style="color:#1f2328">)</span>
</span></span></code></pre></div><div style="height: 500px; overflow-y: auto;">
<pre class="text">
<code>Epochs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [03:22<00:00, 5.04s/it]
Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00, 8.51it/s, loss=0.472, avg_loss=0.917, lr=2.82e-5]
Eval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 12.23it/s, loss=0.346, avg_loss=0.421]
Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00, 9.10it/s, loss=0.134, avg_loss=0.35, lr=5.23e-5]
Eval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 12.53it/s, loss=0.138, avg_loss=0.209]
Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00, 8.67it/s, loss=0.371, avg_loss=0.207, lr=9.07e-5]
Eval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 10.86it/s, loss=0.0978, avg_loss=0.147]
Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00, 9.08it/s, loss=0.125, avg_loss=0.191, lr=0.000141]
Eval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 10.68it/s, loss=0.0925, avg_loss=0.182]
Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00, 9.17it/s, loss=0.17, avg_loss=0.227, lr=0.000199]
Eval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 11.88it/s, loss=0.0917, avg_loss=0.205]
Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00, 8.87it/s, loss=0.116, avg_loss=0.193, lr=0.000261]
Eval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 12.42it/s, loss=0.075, avg_loss=0.13]
Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00, 8.81it/s, loss=0.154, avg_loss=0.2, lr=0.000323]
Eval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 10.71it/s, loss=0.111, avg_loss=0.144]
Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00, 8.56it/s, loss=0.17, avg_loss=0.206, lr=0.000381]
Eval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 14.12it/s, loss=0.136, avg_loss=0.177]
Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00, 9.12it/s, loss=0.668, avg_loss=0.252, lr=0.000431]
Eval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 11.77it/s, loss=0.15, avg_loss=0.357]
Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00, 9.13it/s, loss=0.297, avg_loss=0.3, lr=0.000469]
Eval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 11.99it/s, loss=0.139, avg_loss=0.22]
Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00, 9.16it/s, loss=0.357, avg_loss=0.254, lr=0.000492]
Eval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 11.99it/s, loss=0.135, avg_loss=0.193]
Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00, 8.85it/s, loss=0.471, avg_loss=0.253, lr=0.0005]
Eval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 12.07it/s, loss=0.0909, avg_loss=0.165]
Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00, 9.19it/s, loss=0.454, avg_loss=0.216, lr=0.000498]
Eval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 11.56it/s, loss=0.104, avg_loss=0.172]
Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00, 9.03it/s, loss=0.165, avg_loss=0.225, lr=0.000494]
Eval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 11.85it/s, loss=0.0873, avg_loss=0.14]
Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00, 8.82it/s, loss=0.0918, avg_loss=0.215, lr=0.000486]
Eval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 11.76it/s, loss=0.0951, avg_loss=0.137]
Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00, 9.16it/s, loss=0.1, avg_loss=0.179, lr=0.000475]
Eval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 11.66it/s, loss=0.0898, avg_loss=0.131]
Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00, 9.03it/s, loss=0.205, avg_loss=0.164, lr=0.000461]
Eval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 11.70it/s, loss=0.103, avg_loss=0.131]
Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00, 9.20it/s, loss=0.118, avg_loss=0.201, lr=0.000445]
Eval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 10.57it/s, loss=0.0923, avg_loss=0.168]
Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00, 9.15it/s, loss=0.0848, avg_loss=0.197, lr=0.000426]
Eval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 12.11it/s, loss=0.0793, avg_loss=0.135]
Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00, 8.94it/s, loss=0.138, avg_loss=0.169, lr=0.000405]
Eval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 12.12it/s, loss=0.0741, avg_loss=0.134]
Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00, 9.00it/s, loss=0.3, avg_loss=0.195, lr=0.000382]
Eval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 11.81it/s, loss=0.133, avg_loss=0.163]
Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00, 9.09it/s, loss=0.29, avg_loss=0.126, lr=0.000358]
Eval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 11.96it/s, loss=0.0742, avg_loss=0.12]
Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00, 8.97it/s, loss=0.113, avg_loss=0.136, lr=0.000332]
Eval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 14.36it/s, loss=0.0749, avg_loss=0.116]
Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00, 9.12it/s, loss=0.0852, avg_loss=0.137, lr=0.000305]
Eval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 12.19it/s, loss=0.0689, avg_loss=0.114]
Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00, 9.16it/s, loss=0.118, avg_loss=0.142, lr=0.000277]
Eval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 11.83it/s, loss=0.0643, avg_loss=0.117]
Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00, 9.01it/s, loss=0.0898, avg_loss=0.134, lr=0.000249]
Eval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 12.22it/s, loss=0.0726, avg_loss=0.105]
Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00, 9.02it/s, loss=0.0792, avg_loss=0.122, lr=0.000221]
Eval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 11.67it/s, loss=0.0679, avg_loss=0.1]
Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00, 9.13it/s, loss=0.0842, avg_loss=0.127, lr=0.000193]
Eval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 11.75it/s, loss=0.0724, avg_loss=0.101]
Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00, 9.11it/s, loss=0.0794, avg_loss=0.126, lr=0.000167]
Eval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 11.94it/s, loss=0.0656, avg_loss=0.0925]
Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00, 9.02it/s, loss=0.0992, avg_loss=0.113, lr=0.000141]
Eval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 11.72it/s, loss=0.0586, avg_loss=0.0914]
Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00, 9.05it/s, loss=0.15, avg_loss=0.117, lr=0.000116]
Eval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 12.15it/s, loss=0.0593, avg_loss=0.089]
Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00, 9.06it/s, loss=0.23, avg_loss=0.107, lr=9.34e-5]
Eval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 11.97it/s, loss=0.0559, avg_loss=0.0899]
Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00, 8.93it/s, loss=0.0678, avg_loss=0.0973, lr=7.26e-5]
Eval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 11.98it/s, loss=0.0631, avg_loss=0.0831]
Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00, 9.11it/s, loss=0.0892, avg_loss=0.0847, lr=5.4e-5]
Eval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 12.03it/s, loss=0.0587, avg_loss=0.0813]
Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00, 9.08it/s, loss=0.0662, avg_loss=0.0854, lr=3.78e-5]
Eval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 11.68it/s, loss=0.06, avg_loss=0.0842]
Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00, 8.96it/s, loss=0.065, avg_loss=0.0936, lr=2.44e-5]
Eval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 12.23it/s, loss=0.0532, avg_loss=0.0795]
Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00, 8.70it/s, loss=0.226, avg_loss=0.0877, lr=1.37e-5]
Eval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 12.32it/s, loss=0.0544, avg_loss=0.0792]
Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00, 9.10it/s, loss=0.0718, avg_loss=0.0849, lr=6.06e-6]
Eval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 11.96it/s, loss=0.0556, avg_loss=0.0769]
Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00, 9.03it/s, loss=0.0538, avg_loss=0.0903, lr=1.47e-6]
Eval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 10.89it/s, loss=0.0526, avg_loss=0.0778]
Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00, 9.13it/s, loss=0.0511, avg_loss=0.0853, lr=3.75e-9]
Eval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 11.66it/s, loss=0.0526, avg_loss=0.0772]</code>
</pre>
</div>
---
<p>At last, we have our fine-tuned Mask R-CNN model. To wrap up the tutorial, we can test our model by performing inference on individual images.</p>




<h2 id="making-predictions-with-the-model">Making Predictions with the Model
  <a href="#making-predictions-with-the-model"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h2>
<p>In this final part of the tutorial, we will cover how to perform inference on individual images with our Mask R-CNN model and filter the predictions.</p>




<h3 id="preparing-input-data">Preparing Input Data
  <a href="#preparing-input-data"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h3>
<p>Letâ€™s use a random image from the validation set. That way, we have some ground truth annotation data to compare against. Unlike during training, we wonâ€™t stick to square input dimensions for inference.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#57606a"># Choose a random item from the validation set</span>
</span></span><span style="display:flex;"><span>file_id <span style="color:#0550ae">=</span> random<span style="color:#0550ae">.</span>choice<span style="color:#1f2328">(</span>val_keys<span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Retrieve the image file path associated with the file ID</span>
</span></span><span style="display:flex;"><span>test_file <span style="color:#0550ae">=</span> img_dict<span style="color:#1f2328">[</span>file_id<span style="color:#1f2328">]</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Open the test file</span>
</span></span><span style="display:flex;"><span>test_img <span style="color:#0550ae">=</span> Image<span style="color:#0550ae">.</span>open<span style="color:#1f2328">(</span>test_file<span style="color:#1f2328">)</span><span style="color:#0550ae">.</span>convert<span style="color:#1f2328">(</span><span style="color:#0a3069">&#39;RGB&#39;</span><span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Resize the test image</span>
</span></span><span style="display:flex;"><span>input_img <span style="color:#0550ae">=</span> resize_img<span style="color:#1f2328">(</span>test_img<span style="color:#1f2328">,</span> target_sz<span style="color:#0550ae">=</span>train_sz<span style="color:#1f2328">,</span> divisor<span style="color:#0550ae">=</span><span style="color:#0550ae">1</span><span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Calculate the scale between the source image and the resized image</span>
</span></span><span style="display:flex;"><span>min_img_scale <span style="color:#0550ae">=</span> <span style="color:#6639ba">min</span><span style="color:#1f2328">(</span>test_img<span style="color:#0550ae">.</span>size<span style="color:#1f2328">)</span> <span style="color:#0550ae">/</span> <span style="color:#6639ba">min</span><span style="color:#1f2328">(</span>input_img<span style="color:#0550ae">.</span>size<span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>display<span style="color:#1f2328">(</span>test_img<span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Print the prediction data as a Pandas DataFrame for easy formatting</span>
</span></span><span style="display:flex;"><span>pd<span style="color:#0550ae">.</span>Series<span style="color:#1f2328">({</span>
</span></span><span style="display:flex;"><span>    <span style="color:#0a3069">&#34;Source Image Size:&#34;</span><span style="color:#1f2328">:</span> test_img<span style="color:#0550ae">.</span>size<span style="color:#1f2328">,</span>
</span></span><span style="display:flex;"><span>    <span style="color:#0a3069">&#34;Input Dims:&#34;</span><span style="color:#1f2328">:</span> input_img<span style="color:#0550ae">.</span>size<span style="color:#1f2328">,</span>
</span></span><span style="display:flex;"><span>    <span style="color:#0a3069">&#34;Min Image Scale:&#34;</span><span style="color:#1f2328">:</span> min_img_scale<span style="color:#1f2328">,</span>
</span></span><span style="display:flex;"><span>    <span style="color:#0a3069">&#34;Input Image Size:&#34;</span><span style="color:#1f2328">:</span> input_img<span style="color:#0550ae">.</span>size
</span></span><span style="display:flex;"><span><span style="color:#1f2328">})</span><span style="color:#0550ae">.</span>to_frame<span style="color:#1f2328">()</span><span style="color:#0550ae">.</span>style<span style="color:#0550ae">.</span>hide<span style="color:#1f2328">(</span>axis<span style="color:#0550ae">=</span><span style="color:#0a3069">&#39;columns&#39;</span><span style="color:#1f2328">)</span>
</span></span></code></pre></div><p><img src="./images/output_89_0.png" alt="">{fig-align=&ldquo;center&rdquo;}</p>
<div style="overflow-x:auto; max-height:500px">
<table id="T_73deb">
  <thead>
  </thead>
  <tbody>
    <tr>
      <th id="T_73deb_level0_row0" class="row_heading level0 row0" >Source Image Size:</th>
      <td id="T_73deb_row0_col0" class="data row0 col0" >(480, 640)</td>
    </tr>
    <tr>
      <th id="T_73deb_level0_row1" class="row_heading level0 row1" >Input Dims:</th>
      <td id="T_73deb_row1_col0" class="data row1 col0" >(512, 682)</td>
    </tr>
    <tr>
      <th id="T_73deb_level0_row2" class="row_heading level0 row2" >Min Image Scale:</th>
      <td id="T_73deb_row2_col0" class="data row2 col0" >0.937500</td>
    </tr>
    <tr>
      <th id="T_73deb_level0_row3" class="row_heading level0 row3" >Input Image Size:</th>
      <td id="T_73deb_row3_col0" class="data row3 col0" >(512, 682)</td>
    </tr>
  </tbody>
</table>
</div>




<h4 id="get-the-target-annotation-data">Get the target annotation data
  <a href="#get-the-target-annotation-data"></a>
</h4>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#57606a"># Extract the polygon points for segmentation mask</span>
</span></span><span style="display:flex;"><span>target_shape_points <span style="color:#0550ae">=</span> <span style="color:#1f2328">[</span>shape<span style="color:#1f2328">[</span><span style="color:#0a3069">&#39;points&#39;</span><span style="color:#1f2328">]</span> <span style="color:#cf222e">for</span> shape <span style="color:#0550ae">in</span> annotation_df<span style="color:#0550ae">.</span>loc<span style="color:#1f2328">[</span>file_id<span style="color:#1f2328">][</span><span style="color:#0a3069">&#39;shapes&#39;</span><span style="color:#1f2328">]]</span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Format polygon points for PIL</span>
</span></span><span style="display:flex;"><span>target_xy_coords <span style="color:#0550ae">=</span> <span style="color:#1f2328">[[</span><span style="color:#6639ba">tuple</span><span style="color:#1f2328">(</span>p<span style="color:#1f2328">)</span> <span style="color:#cf222e">for</span> p <span style="color:#0550ae">in</span> points<span style="color:#1f2328">]</span> <span style="color:#cf222e">for</span> points <span style="color:#0550ae">in</span> target_shape_points<span style="color:#1f2328">]</span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Generate mask images from polygons</span>
</span></span><span style="display:flex;"><span>target_mask_imgs <span style="color:#0550ae">=</span> <span style="color:#1f2328">[</span>create_polygon_mask<span style="color:#1f2328">(</span>test_img<span style="color:#0550ae">.</span>size<span style="color:#1f2328">,</span> xy<span style="color:#1f2328">)</span> <span style="color:#cf222e">for</span> xy <span style="color:#0550ae">in</span> target_xy_coords<span style="color:#1f2328">]</span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Convert mask images to tensors</span>
</span></span><span style="display:flex;"><span>target_masks <span style="color:#0550ae">=</span> Mask<span style="color:#1f2328">(</span>torch<span style="color:#0550ae">.</span>concat<span style="color:#1f2328">([</span>Mask<span style="color:#1f2328">(</span>transforms<span style="color:#0550ae">.</span>PILToTensor<span style="color:#1f2328">()(</span>mask_img<span style="color:#1f2328">),</span> dtype<span style="color:#0550ae">=</span>torch<span style="color:#0550ae">.</span>bool<span style="color:#1f2328">)</span> <span style="color:#cf222e">for</span> mask_img <span style="color:#0550ae">in</span> target_mask_imgs<span style="color:#1f2328">]))</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Get the target labels and bounding boxes</span>
</span></span><span style="display:flex;"><span>target_labels <span style="color:#0550ae">=</span> <span style="color:#1f2328">[</span>shape<span style="color:#1f2328">[</span><span style="color:#0a3069">&#39;label&#39;</span><span style="color:#1f2328">]</span> <span style="color:#cf222e">for</span> shape <span style="color:#0550ae">in</span> annotation_df<span style="color:#0550ae">.</span>loc<span style="color:#1f2328">[</span>file_id<span style="color:#1f2328">][</span><span style="color:#0a3069">&#39;shapes&#39;</span><span style="color:#1f2328">]]</span>
</span></span><span style="display:flex;"><span>target_bboxes <span style="color:#0550ae">=</span> BoundingBoxes<span style="color:#1f2328">(</span>data<span style="color:#0550ae">=</span>torchvision<span style="color:#0550ae">.</span>ops<span style="color:#0550ae">.</span>masks_to_boxes<span style="color:#1f2328">(</span>target_masks<span style="color:#1f2328">),</span> <span style="color:#6639ba">format</span><span style="color:#0550ae">=</span><span style="color:#0a3069">&#39;xyxy&#39;</span><span style="color:#1f2328">,</span> canvas_size<span style="color:#0550ae">=</span>test_img<span style="color:#0550ae">.</span>size<span style="color:#1f2328">[::</span><span style="color:#0550ae">-</span><span style="color:#0550ae">1</span><span style="color:#1f2328">])</span>
</span></span></code></pre></div>



<h4 id="pass-the-input-data-to-the-model">Pass the input data to the model
  <a href="#pass-the-input-data-to-the-model"></a>
</h4>
<p>Now, we can convert the test image to a tensor and pass it to the model. Ensure the model is set to evaluation mode to get predictions instead of loss values.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#57606a"># Set the model to evaluation mode</span>
</span></span><span style="display:flex;"><span>model<span style="color:#0550ae">.</span>eval<span style="color:#1f2328">();</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Ensure the model and input data are on the same device</span>
</span></span><span style="display:flex;"><span>model<span style="color:#0550ae">.</span>to<span style="color:#1f2328">(</span>device<span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>input_tensor <span style="color:#0550ae">=</span> transforms<span style="color:#0550ae">.</span>Compose<span style="color:#1f2328">([</span>transforms<span style="color:#0550ae">.</span>ToImage<span style="color:#1f2328">(),</span> transforms<span style="color:#0550ae">.</span>ToDtype<span style="color:#1f2328">(</span>torch<span style="color:#0550ae">.</span>float32<span style="color:#1f2328">,</span> scale<span style="color:#0550ae">=</span><span style="color:#cf222e">True</span><span style="color:#1f2328">)])(</span>input_img<span style="color:#1f2328">)[</span><span style="color:#cf222e">None</span><span style="color:#1f2328">]</span><span style="color:#0550ae">.</span>to<span style="color:#1f2328">(</span>device<span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Make a prediction with the model</span>
</span></span><span style="display:flex;"><span><span style="color:#cf222e">with</span> torch<span style="color:#0550ae">.</span>no_grad<span style="color:#1f2328">():</span>
</span></span><span style="display:flex;"><span>    model_output <span style="color:#0550ae">=</span> model<span style="color:#1f2328">(</span>input_tensor<span style="color:#1f2328">)</span>
</span></span></code></pre></div>



<h4 id="filter-the-model-output">Filter the model output
  <a href="#filter-the-model-output"></a>
</h4>
<p>The model performs most post-processing steps internally, so we only need to filter the output based on the desired confidence threshold. The model returns predictions as a list of dictionaries. Each dictionary stores bounding boxes, label indices, confidence scores, and segmentation masks for a single sample in the input batch.</p>
<p>Since we resized the test image, we must scale the bounding boxes and segmentation masks to the source resolution.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#57606a"># Set the confidence threshold</span>
</span></span><span style="display:flex;"><span>threshold <span style="color:#0550ae">=</span> <span style="color:#0550ae">0.5</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Move model output to the CPU</span>
</span></span><span style="display:flex;"><span>model_output <span style="color:#0550ae">=</span> move_data_to_device<span style="color:#1f2328">(</span>model_output<span style="color:#1f2328">,</span> <span style="color:#0a3069">&#39;cpu&#39;</span><span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Filter the output based on the confidence threshold</span>
</span></span><span style="display:flex;"><span>scores_mask <span style="color:#0550ae">=</span> model_output<span style="color:#1f2328">[</span><span style="color:#0550ae">0</span><span style="color:#1f2328">][</span><span style="color:#0a3069">&#39;scores&#39;</span><span style="color:#1f2328">]</span> <span style="color:#0550ae">&gt;</span> threshold
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Scale the predicted bounding boxes</span>
</span></span><span style="display:flex;"><span>pred_bboxes <span style="color:#0550ae">=</span> BoundingBoxes<span style="color:#1f2328">(</span>model_output<span style="color:#1f2328">[</span><span style="color:#0550ae">0</span><span style="color:#1f2328">][</span><span style="color:#0a3069">&#39;boxes&#39;</span><span style="color:#1f2328">][</span>scores_mask<span style="color:#1f2328">]</span><span style="color:#0550ae">*</span>min_img_scale<span style="color:#1f2328">,</span> <span style="color:#6639ba">format</span><span style="color:#0550ae">=</span><span style="color:#0a3069">&#39;xyxy&#39;</span><span style="color:#1f2328">,</span> canvas_size<span style="color:#0550ae">=</span>input_img<span style="color:#0550ae">.</span>size<span style="color:#1f2328">[::</span><span style="color:#0550ae">-</span><span style="color:#0550ae">1</span><span style="color:#1f2328">])</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Get the class names for the predicted label indices</span>
</span></span><span style="display:flex;"><span>pred_labels <span style="color:#0550ae">=</span> <span style="color:#1f2328">[</span>class_names<span style="color:#1f2328">[</span><span style="color:#6639ba">int</span><span style="color:#1f2328">(</span>label<span style="color:#1f2328">)]</span> <span style="color:#cf222e">for</span> label <span style="color:#0550ae">in</span> model_output<span style="color:#1f2328">[</span><span style="color:#0550ae">0</span><span style="color:#1f2328">][</span><span style="color:#0a3069">&#39;labels&#39;</span><span style="color:#1f2328">][</span>scores_mask<span style="color:#1f2328">]]</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Extract the confidence scores</span>
</span></span><span style="display:flex;"><span>pred_scores <span style="color:#0550ae">=</span> model_output<span style="color:#1f2328">[</span><span style="color:#0550ae">0</span><span style="color:#1f2328">][</span><span style="color:#0a3069">&#39;scores&#39;</span><span style="color:#1f2328">]</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Scale and stack the predicted segmentation masks</span>
</span></span><span style="display:flex;"><span>pred_masks <span style="color:#0550ae">=</span> F<span style="color:#0550ae">.</span>interpolate<span style="color:#1f2328">(</span>model_output<span style="color:#1f2328">[</span><span style="color:#0550ae">0</span><span style="color:#1f2328">][</span><span style="color:#0a3069">&#39;masks&#39;</span><span style="color:#1f2328">][</span>scores_mask<span style="color:#1f2328">],</span> size<span style="color:#0550ae">=</span>test_img<span style="color:#0550ae">.</span>size<span style="color:#1f2328">[::</span><span style="color:#0550ae">-</span><span style="color:#0550ae">1</span><span style="color:#1f2328">])</span>
</span></span><span style="display:flex;"><span>pred_masks <span style="color:#0550ae">=</span> torch<span style="color:#0550ae">.</span>concat<span style="color:#1f2328">([</span>Mask<span style="color:#1f2328">(</span>torch<span style="color:#0550ae">.</span>where<span style="color:#1f2328">(</span>mask <span style="color:#0550ae">&gt;=</span> threshold<span style="color:#1f2328">,</span> <span style="color:#0550ae">1</span><span style="color:#1f2328">,</span> <span style="color:#0550ae">0</span><span style="color:#1f2328">),</span> dtype<span style="color:#0550ae">=</span>torch<span style="color:#0550ae">.</span>bool<span style="color:#1f2328">)</span> <span style="color:#cf222e">for</span> mask <span style="color:#0550ae">in</span> pred_masks<span style="color:#1f2328">])</span>
</span></span></code></pre></div>



<h4 id="annotate-the-image-using-the-model-predictions">Annotate the image using the model predictions
  <a href="#annotate-the-image-using-the-model-predictions"></a>
</h4>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#57606a"># Get the annotation colors for the targets and predictions</span>
</span></span><span style="display:flex;"><span>target_colors<span style="color:#0550ae">=</span><span style="color:#1f2328">[</span>int_colors<span style="color:#1f2328">[</span>i<span style="color:#1f2328">]</span> <span style="color:#cf222e">for</span> i <span style="color:#0550ae">in</span> <span style="color:#1f2328">[</span>class_names<span style="color:#0550ae">.</span>index<span style="color:#1f2328">(</span>label<span style="color:#1f2328">)</span> <span style="color:#cf222e">for</span> label <span style="color:#0550ae">in</span> target_labels<span style="color:#1f2328">]]</span>
</span></span><span style="display:flex;"><span>pred_colors<span style="color:#0550ae">=</span><span style="color:#1f2328">[</span>int_colors<span style="color:#1f2328">[</span>i<span style="color:#1f2328">]</span> <span style="color:#cf222e">for</span> i <span style="color:#0550ae">in</span> <span style="color:#1f2328">[</span>class_names<span style="color:#0550ae">.</span>index<span style="color:#1f2328">(</span>label<span style="color:#1f2328">)</span> <span style="color:#cf222e">for</span> label <span style="color:#0550ae">in</span> pred_labels<span style="color:#1f2328">]]</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Convert the test images to a tensor</span>
</span></span><span style="display:flex;"><span>img_tensor <span style="color:#0550ae">=</span> transforms<span style="color:#0550ae">.</span>PILToTensor<span style="color:#1f2328">()(</span>test_img<span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Annotate the test image with the target segmentation masks</span>
</span></span><span style="display:flex;"><span>annotated_tensor <span style="color:#0550ae">=</span> draw_segmentation_masks<span style="color:#1f2328">(</span>image<span style="color:#0550ae">=</span>img_tensor<span style="color:#1f2328">,</span> masks<span style="color:#0550ae">=</span>target_masks<span style="color:#1f2328">,</span> alpha<span style="color:#0550ae">=</span><span style="color:#0550ae">0.3</span><span style="color:#1f2328">,</span> colors<span style="color:#0550ae">=</span>target_colors<span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Annotate the test image with the target bounding boxes</span>
</span></span><span style="display:flex;"><span>annotated_tensor <span style="color:#0550ae">=</span> draw_bboxes<span style="color:#1f2328">(</span>image<span style="color:#0550ae">=</span>annotated_tensor<span style="color:#1f2328">,</span> boxes<span style="color:#0550ae">=</span>target_bboxes<span style="color:#1f2328">,</span> labels<span style="color:#0550ae">=</span>target_labels<span style="color:#1f2328">,</span> colors<span style="color:#0550ae">=</span>target_colors<span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Display the annotated test image</span>
</span></span><span style="display:flex;"><span>annotated_test_img <span style="color:#0550ae">=</span> tensor_to_pil<span style="color:#1f2328">(</span>annotated_tensor<span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Annotate the test image with the predicted segmentation masks</span>
</span></span><span style="display:flex;"><span>annotated_tensor <span style="color:#0550ae">=</span> draw_segmentation_masks<span style="color:#1f2328">(</span>image<span style="color:#0550ae">=</span>img_tensor<span style="color:#1f2328">,</span> masks<span style="color:#0550ae">=</span>pred_masks<span style="color:#1f2328">,</span> alpha<span style="color:#0550ae">=</span><span style="color:#0550ae">0.3</span><span style="color:#1f2328">,</span> colors<span style="color:#0550ae">=</span>pred_colors<span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Annotate the test image with the predicted labels and bounding boxes</span>
</span></span><span style="display:flex;"><span>annotated_tensor <span style="color:#0550ae">=</span> draw_bboxes<span style="color:#1f2328">(</span>
</span></span><span style="display:flex;"><span>    image<span style="color:#0550ae">=</span>annotated_tensor<span style="color:#1f2328">,</span> 
</span></span><span style="display:flex;"><span>    boxes<span style="color:#0550ae">=</span>pred_bboxes<span style="color:#1f2328">,</span> 
</span></span><span style="display:flex;"><span>    labels<span style="color:#0550ae">=</span><span style="color:#1f2328">[</span><span style="color:#0a3069">f</span><span style="color:#0a3069">&#34;</span><span style="color:#0a3069">{</span>label<span style="color:#0a3069">}</span><span style="color:#0a3069">\n</span><span style="color:#0a3069">{</span>prob<span style="color:#0550ae">*</span><span style="color:#0550ae">100</span><span style="color:#0a3069">:</span><span style="color:#0a3069">.2f</span><span style="color:#0a3069">}</span><span style="color:#0a3069">%&#34;</span> <span style="color:#cf222e">for</span> label<span style="color:#1f2328">,</span> prob <span style="color:#0550ae">in</span> <span style="color:#6639ba">zip</span><span style="color:#1f2328">(</span>pred_labels<span style="color:#1f2328">,</span> pred_scores<span style="color:#1f2328">)],</span>
</span></span><span style="display:flex;"><span>    colors<span style="color:#0550ae">=</span>pred_colors
</span></span><span style="display:flex;"><span><span style="color:#1f2328">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Display the annotated test image with the predicted bounding boxes</span>
</span></span><span style="display:flex;"><span>display<span style="color:#1f2328">(</span>stack_imgs<span style="color:#1f2328">([</span>annotated_test_img<span style="color:#1f2328">,</span> tensor_to_pil<span style="color:#1f2328">(</span>annotated_tensor<span style="color:#1f2328">)]))</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#57606a"># Print the prediction data as a Pandas DataFrame for easy formatting</span>
</span></span><span style="display:flex;"><span>pd<span style="color:#0550ae">.</span>Series<span style="color:#1f2328">({</span>
</span></span><span style="display:flex;"><span>    <span style="color:#0a3069">&#34;Target BBoxes:&#34;</span><span style="color:#1f2328">:</span> <span style="color:#1f2328">[</span><span style="color:#0a3069">f</span><span style="color:#0a3069">&#34;</span><span style="color:#0a3069">{</span>label<span style="color:#0a3069">}</span><span style="color:#0a3069">:</span><span style="color:#0a3069">{</span>bbox<span style="color:#0a3069">}</span><span style="color:#0a3069">&#34;</span> <span style="color:#cf222e">for</span> label<span style="color:#1f2328">,</span> bbox <span style="color:#0550ae">in</span> <span style="color:#6639ba">zip</span><span style="color:#1f2328">(</span>target_labels<span style="color:#1f2328">,</span> np<span style="color:#0550ae">.</span>round<span style="color:#1f2328">(</span>target_bboxes<span style="color:#0550ae">.</span>numpy<span style="color:#1f2328">(),</span> decimals<span style="color:#0550ae">=</span><span style="color:#0550ae">3</span><span style="color:#1f2328">))],</span>
</span></span><span style="display:flex;"><span>    <span style="color:#0a3069">&#34;Predicted BBoxes:&#34;</span><span style="color:#1f2328">:</span> <span style="color:#1f2328">[</span><span style="color:#0a3069">f</span><span style="color:#0a3069">&#34;</span><span style="color:#0a3069">{</span>label<span style="color:#0a3069">}</span><span style="color:#0a3069">:</span><span style="color:#0a3069">{</span>bbox<span style="color:#0a3069">}</span><span style="color:#0a3069">&#34;</span> <span style="color:#cf222e">for</span> label<span style="color:#1f2328">,</span> bbox <span style="color:#0550ae">in</span> <span style="color:#6639ba">zip</span><span style="color:#1f2328">(</span>pred_labels<span style="color:#1f2328">,</span> pred_bboxes<span style="color:#0550ae">.</span>round<span style="color:#1f2328">(</span>decimals<span style="color:#0550ae">=</span><span style="color:#0550ae">3</span><span style="color:#1f2328">)</span><span style="color:#0550ae">.</span>numpy<span style="color:#1f2328">())],</span>
</span></span><span style="display:flex;"><span>    <span style="color:#0a3069">&#34;Confidence Scores:&#34;</span><span style="color:#1f2328">:</span> <span style="color:#1f2328">[</span><span style="color:#0a3069">f</span><span style="color:#0a3069">&#34;</span><span style="color:#0a3069">{</span>label<span style="color:#0a3069">}</span><span style="color:#0a3069">: </span><span style="color:#0a3069">{</span>prob<span style="color:#0550ae">*</span><span style="color:#0550ae">100</span><span style="color:#0a3069">:</span><span style="color:#0a3069">.2f</span><span style="color:#0a3069">}</span><span style="color:#0a3069">%&#34;</span> <span style="color:#cf222e">for</span> label<span style="color:#1f2328">,</span> prob <span style="color:#0550ae">in</span> <span style="color:#6639ba">zip</span><span style="color:#1f2328">(</span>pred_labels<span style="color:#1f2328">,</span> pred_scores<span style="color:#1f2328">)]</span>
</span></span><span style="display:flex;"><span><span style="color:#1f2328">})</span><span style="color:#0550ae">.</span>to_frame<span style="color:#1f2328">()</span><span style="color:#0550ae">.</span>style<span style="color:#0550ae">.</span>hide<span style="color:#1f2328">(</span>axis<span style="color:#0550ae">=</span><span style="color:#0a3069">&#39;columns&#39;</span><span style="color:#1f2328">)</span>
</span></span></code></pre></div><p><img src="./images/output_97_0.png" alt="">{fig-align=&ldquo;center&rdquo;}</p>
<div style="overflow-x:auto; max-height:500px">
<table id="T_da99b">
  <thead>
  </thead>
  <tbody>
    <tr>
      <th id="T_da99b_level0_row0" class="row_heading level0 row0" >Target BBoxes:</th>
      <td id="T_da99b_row0_col0" class="data row0 col0" >['student_id:[ 11. 164. 451. 455.]']</td>
    </tr>
    <tr>
      <th id="T_da99b_level0_row1" class="row_heading level0 row1" >Predicted BBoxes:</th>
      <td id="T_da99b_row1_col0" class="data row1 col0" >['student_id:[ 10.621 162.709 452.722 453.537]']</td>
    </tr>
    <tr>
      <th id="T_da99b_level0_row2" class="row_heading level0 row2" >Confidence Scores:</th>
      <td id="T_da99b_row2_col0" class="data row2 col0" >['student_id: 99.94%']</td>
    </tr>
  </tbody>
</table>
</div>
The segmentation mask has a few rough spots, but the model appears to have learned to detect and segment ID cards as desired.




<h2 id="conclusion">Conclusion
  <a href="#conclusion"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h2>
<p>Congratulations on completing this tutorial for training Mask R-CNN models in PyTorch! The skills and knowledge youâ€™ve acquired here serve as a solid foundation for future projects.</p>




<h2 id="recommended-tutorials">Recommended Tutorials
  <a href="#recommended-tutorials"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h2>
<ul>
<li>
<a href="./onnx-export/"><strong>Exporting Mask R-CNN Models from PyTorch to ONNX</strong></a><strong>:</strong> Learn how to export Mask R-CNN models from PyTorch to ONNX and perform inference using ONNX Runtime.</li>
<li>
<a href="/posts/torchvision-labelme-annotation-tutorials/segmentation-polygons/"><strong>Working with LabelMe Segmentation Annotations in Torchvision</strong></a><strong>:</strong> Learn how to work with LabelMe segmentation annotations in torchvision for instance segmentation tasks.</li>
</ul>

        
        <details closed class="f6 fw7 input-reset">
  <dl class="f6 lh-copy">
    <dt class="fw7">Posted on:</dt>
    <dd class="fw5 ml0">February 23, 2025</dd>
  </dl>
  <dl class="f6 lh-copy">
    <dt class="fw7">Length:</dt>
    <dd class="fw5 ml0">36 minute read, 7570 words</dd>
  </dl>
  
  <dl class="f6 lh-copy">
    <dt class="fw7">Categories:</dt>
    <dd class="fw5 ml0"> <a href="http://localhost:1313/categories/pytorch">pytorch</a>  <a href="http://localhost:1313/categories/mask-rcnn">mask-rcnn</a>  <a href="http://localhost:1313/categories/object-detection">object-detection</a>  <a href="http://localhost:1313/categories/instance-segmentation">instance-segmentation</a>  <a href="http://localhost:1313/categories/tutorial">tutorial</a> </dd>
  </dl>
  
  
  
  <dl class="f6 lh-copy">
    <dt class="fw7">See Also:</dt>
    
  </dl>
</details>

      </section>
      <footer class="post-footer">
        <div class="post-pagination dt w-100 mt4 mb2">
  
  
  
  
    <a class="next dtc pl2 tr v-top fw6"
    href="http://localhost:1313/blog/social/">Set up your social &rarr;</a>
  
</div>

      </footer>
    </article>
    
      <div class="post-comments pa0 pa4-l mt4">
  
    
      <script src="https://utteranc.es/client.js"
              repo="apreshill/apero"
              issue-term="pathname"
              theme="boxy-light"
              label="comments :crystal_ball:"
              crossorigin="anonymous"
              async
              type="text/javascript">
      </script>
    
  
</div>

    
  </section>
</main>
<footer class="site-footer pv4 bt b--transparent ph5" role="contentinfo">
  <nav class="db dt-l w-100">
    <p class="site-copyright f7 db dtc-l v-mid w-100 w-33-l tc tl-l pv2 pv0-l mv0 lh-copy">
      &copy; 2025 RStudio, Anywhere
      <span class="middot-divider"></span>
      Made with <span xmlns:dct="http://purl.org/dc/terms/" property="dct:title"><a xmlns:dct="http://purl.org/dc/terms/" href="https://github.com/hugo-apero/" rel="dct:source">Hugo ApÃ©ro</a></span>.
      <br />
      
Based on <span xmlns:dct="http://purl.org/dc/terms/" property="dct:title"><a xmlns:dct="http://purl.org/dc/terms/" href="https://github.com/formspree/blogophonic-hugo" rel="dct:source">Blogophonic</a></span> by <a xmlns:cc="http://creativecommons.org/ns#" href="https://formspree.io" property="cc:attributionName" rel="cc:attributionURL">Formspree</a>.
    </p>
    
    <div class="site-social-links db dtc-l v-mid w-100 w-33-l tc pv2 pv0-l mv0">
      <div class="social-icon-links" aria-hidden="true">
  
  
    
    
    
      
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://drive.google.com/drive/u/0/my-drive" title="file-text" target="_blank" rel="me noopener">
      <i class="far fa-file-text fa-lg fa-fw"></i>
    </a>
  
    
    
    
      
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://github.com/shivamchhetry20" title="github" target="_blank" rel="me noopener">
      <i class="fab fa-github fa-lg fa-fw"></i>
    </a>
  
    
    
    
      
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://linkedin.in/shivamkc01" title="linkedin-in" target="_blank" rel="me noopener">
      <i class="fab fa-linkedin-in fa-lg fa-fw"></i>
    </a>
  
    
    
    
      
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://www.instagram.com/explore/tags/apero/" title="instagram" target="_blank" rel="me noopener">
      <i class="fab fa-instagram fa-lg fa-fw"></i>
    </a>
  
</div>

    </div>
    
    <div class="site-links f6 db dtc-l v-mid w-100 w-67-l tc tr-l pv2 pv0-l mv0">
      
      <a class="dib pv1 ph2 link" href="/license/" title="License">License</a>
      
      <a class="dib pv1 ph2 link" href="/contact/" title="Contact form">Contact</a>
      
      <a class="dib pv1 ph2 link" href="/contributors/" title="Contributors">Contributors</a>
      
    </div>
  </nav>
  
    <script>

    var i, text, code, codes = document.getElementsByTagName('code');
    for (let i = 0; i < codes.length;) {
      code = codes[i];
      if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
        text = code.textContent;
        if (/^\$[^$]/.test(text) && /[^$]\$$/.test(text)) {
          text = text.replace(/^\$/, '\\(').replace(/\$$/, '\\)');
          code.textContent = text;
        }
        if (/^\\\((.|\s)+\\\)$/.test(text) ||
            /^\\\[(.|\s)+\\\]$/.test(text) ||
            /^\$(.|\s)+\$$/.test(text) ||
            /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
          code.outerHTML = code.innerHTML;  
          continue;
        }
      }
      i++;
    }
</script>

  
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.css" integrity="sha384-RZU/ijkSsFbcmivfdRBQDtwuwVqK7GMOw6IMvKyeWL2K5UAlyp6WonmB8m7Jd0Hn" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.js" integrity="sha384-pK1WpvzWVBQiP0/GjnvRxV4mOb0oxFuyRxJlk6vVw146n3egcN5C925NCP7a7BY8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/contrib/auto-render.min.js" integrity="sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>



    
  
  
</footer>

      </div>
    </body>
</html>
